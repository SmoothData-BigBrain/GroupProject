














from pyspark import SparkContext

# Row used to create row objects for statistics table
from pyspark.sql import SparkSession, Row


from pyspark.sql import functions as F
from pyspark.sql.functions import col, isnan, when, count, isnull, abs, lit


import seaborn as sns
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


spark = SparkSession.builder \
    .appName("Flight Data Analysis") \
    .getOrCreate()

spark.conf.set("spark.sql.debug.maxToStringFields", 1000)
spark.sparkContext.setLogLevel("ERROR")








import glob
import os
os.getcwd()


import glob
import os
#os.getcwd()
folder_path = 'raw'
csv_files = glob.glob(os.path.join(folder_path, '*.csv'))
df = spark.read.csv(csv_files,
                       sep = ',',
                       inferSchema = True,
                       header = True)





#df = spark.read.csv('combined_file.csv', sep = ',', inferSchema = True, header = True)





col_des = spark.read.csv('flights_column_des.csv', sep = ',', inferSchema = True, header = True)








# get df shape
num_rows = df.count()
num_cols = len(df.columns)
print(f"Shape of the DataFrame: ({num_rows}, {num_cols})")


#num_entries = df.count() #29193782
#num_entries = 29193782
num_entries = num_rows








#null_counts = df.select([count(col(c)).alias(c) for c in df.columns]).collect()[0].asDict()
#print(null_counts)


# compute count of non-null vals for each col in df
from pyspark.sql.functions import col, sum

null_counts = df.select([sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in df.columns]) \
                .collect()[0] \
                .asDict()

print(null_counts)


# Dictionary indicates that the last several cols have all nulls, print last 10 cols of df to manually inspect if NULLS are present
df.select(df.columns[-10:]).show(5)


# Dictionary indicates that the first several cols have no nulls, print first 10 cols of df to manually inspect if NULLS are present
df.select(df.columns[:10]).show(5)


# get data type for each column
for name, dtype in df.dtypes:
    print(f"{name}: {dtype}")





#columns_with_few_nulls = [col_name for col_name, count_val in null_counts.items() if count_val > 0.9*num_entries]
#columns_with_no_nulls = [col_name for col_name, count_val in null_counts.items() if count_val != 0]
#print(columns_with_few_nulls)
#print(columns_with_all_nulls)
#df.select(*columns_with_nulls).show()





#newdf = df.select(columns_with_few_nulls)
#newdf.select(newdf.columns[0:10]).show()


#newdf.select(newdf.columns[10:19]).show()


#newdf.select(newdf.columns[19:33]).show()


#newdf.select(newdf.columns[33:50]).show()


#newdf.select(newdf.columns[50:62]).show()


## Count non-nulls in a list of specified columns

#repeat_cols = ["Marketing_Airline_Network", "Operated_or_Branded_Code_Share_Partners", "IATA_Code_Marketing_Airline", "Operating_Airline ", "IATA_Code_Operating_Airline"]
#print(newdf.select([count(col(c)).alias(c) for c in repeat_cols]).collect()[0].asDict())


#print(newdf.select(repeat_cols).distinct().count())


## filter dataset with cols to keep

#cols_to_keep = ["Year", "Month", "DayofMonth", "Origin", "OriginCityName", "DestCityName", "DepDelay", "ArrDelay", "Cancelled", "CRSElapsedTime", "ActualElapsedTime"]
#my_df = newdf.select(cols_to_keep)
#my_df.show()





non_null_counts = df.select([count(col(c)).alias(c) for c in df.columns]).collect()[0].asDict()

# Calculate non-null percentages
non_null_percentages = {
    col_name: (count_val / num_entries) * 100
    for col_name, count_val in non_null_counts.items()
}

sorted_columns = sorted(non_null_percentages.items(), key=lambda x: x[1], reverse=True)

for col_name, pct in sorted_columns:
    print(f"{col_name}: {pct:.2f}% non-null")








columns_above_90 = [col_name for col_name, pct in non_null_percentages.items() if pct >= 90]
filtered_df = df.select(columns_above_90)
filtered_df.select(filtered_df.columns[:8]).show(5)


# get filtered df shape
num_rows = filtered_df.count()
num_cols = len(filtered_df.columns)
print(f"Shape of the Filtered DataFrame removing cols w/ <90% null values: ({num_rows}, {num_cols})")


#filtered_df.write.mode('overwrite').option("header", "true").csv('filtered_df_temp')


filtered_df.write.mode("overwrite").parquet("filtered_df_parquet")


# save filtered df to not have to redo code later
#filtered_df.coalesce(1).write.mode("overwrite").option("header", True).csv("filtered_df_temp")

# read in already filtered_df saved previously csv
#filtered_df = spark.read.option("header", "true").csv("filtered_df_temp")

# read in already filtered_df saved previously parquet
filtered_df = spark.read.parquet("filtered_df_parquet")





col_des.count() # count original col number


# get all cols in filtered_df
filtered_cols = filtered_df.columns 

# remove any white space
filtered_cols = [str(c).strip() for c in filtered_cols]

# subset column description dataframe for only columns in filtered dataset
filtered_col_des = col_des.filter(col('column').isin(filtered_cols))


# check df was filtered correctly, length & row count should match
print(len(filtered_cols))
print(filtered_col_des.count())


# View all column descriptions in filtered dataframe

filtered_col_des.show(n=filtered_col_des.count(), truncate=False)





# get data type for each column
for name, dtype in filtered_df.dtypes:
    print(f"{name}: {dtype}")


non_string_cols = [col_name for col_name, dtype in filtered_df.dtypes if dtype != 'string']


# subset column description dataframe for only non-string
non_string_col_des = filtered_col_des.filter(col('column').isin(non_string_cols))
non_string_col_des.show(n=non_string_col_des.count(), truncate=False)





cont_col_des = non_string_col_des.filter(
    ~non_string_col_des['column'].rlike('Dest|Origin|ID|Number|FlightDate')
)
cont_col_des.show(n=cont_col_des.count(), truncate=False)


# get statistics for all continuous variables
cont_cols = [row['column'] for row in cont_col_des.select('column').collect()]

describe_df = filtered_df.select(cont_cols).describe()

# compute Q1, Median, Q3 for each column
stats = {
    "25%": {},
    "50%": {},
    "75%": {}
}

for col_name in cont_cols:
    q1, median, q3 = filtered_df.approxQuantile(col_name, [0.25, 0.5, 0.75], 0.01)
    stats["25%"][col_name] = str(q1)
    stats["50%"][col_name] = str(median)
    stats["75%"][col_name] = str(q3)

# convert new rows to df rows
new_rows = [Row(summary=stat_name, **cols) for stat_name, cols in stats.items()]
quartile_df = spark.createDataFrame(new_rows)

# append the new rows to describe_df
full_summary_df = describe_df.unionByName(quartile_df)


# save df to not have to recalc results later
full_summary_df.write.mode("overwrite").parquet("summary_output")


#full_summary_df = spark.read.csv('full_summary_df.csv', sep = ',', inferSchema = True, header = True)
full_summary_df = spark.read.parquet("summary_output")


full_summary_df.select(full_summary_df.columns[:6]).show(truncate=False)


# view df columns
full_summary_df.select(full_summary_df.columns[11:17]).show(truncate=False)








# get mean and median rows as dicts
mean_row = full_summary_df.filter(col("summary") == "mean").collect()[0].asDict()
median_row = full_summary_df.filter(col("summary") == "50%").collect()[0].asDict()

# skip the 'summary' key
cols = [col for col in mean_row.keys() if col != "summary"]

# build rows of (column, absolute_diff, skew direction)
result_rows = []
for c in cols: # for each col
    mean_val = float(mean_row[c]) # get mean
    median_val = float(median_row[c]) # get median
    diff = __builtins__.abs(mean_val - median_val) # get abs difference
    skew = "right" if mean_val > median_val else "left" if mean_val < median_val else "none" # get skew direction
    result_rows.append(Row(column=c, absolute_diff=diff, skew=skew)) # aggregate

# create df
diff_df = spark.createDataFrame(result_rows)

# get top 20
top_skewed = diff_df.orderBy(col("absolute_diff").desc()).limit(20)

top_skewed.show(truncate=False)



# get all cols in filtered_df
skewed_cols = [row['column'] for row in top_skewed.select('column').collect()]

# remove any white space
skewed_cols = [str(c).strip() for c in skewed_cols]

# subset column description dataframe for only columns in filtered dataset
skewed_col_des = col_des.filter(col('column').isin(skewed_cols))


skewed_col_des.show(n=skewed_col_des.count(), truncate=False)


# list of columns from 'top_skewed'
columns_to_plot = [row['column'] for row in top_skewed.collect()]

# filter the columns that exist in filtered_df
valid_columns = [col for col in columns_to_plot if col in filtered_df.columns]

# plot histograms for each column
n_cols = 4  # 4 histograms per row
n_rows = (len(valid_columns) + n_cols - 1) // n_cols  # calculate num rows needed

fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 5))

# flatten axes for easier indexing
axes = axes.flatten()

# loop through cols and plot
for i, column in enumerate(valid_columns):
    hist = filtered_df.select(column).rdd.flatMap(lambda x: x).histogram(20)  # 20 bins
    
    bin_edges, bin_counts = hist

    # plot the histogram using the bin edges and counts
    axes[i].bar(bin_edges[:-1], bin_counts, width=(bin_edges[1] - bin_edges[0]), edgecolor='black')

    # set axes & title
    axes[i].set_title(f"Histogram of {column}")
    axes[i].set_xlabel('Value')
    axes[i].set_ylabel('Frequency')

# turn off any unused subplots
for i in range(len(valid_columns), len(axes)):
    axes[i].axis('off')

plt.tight_layout()
plt.show()








count_delay = filtered_df.select(["Origin", "DepDelay"]).groupBy("Origin")\
        .agg(count(F.when(col("DepDelay") > 0, 1)).alias("DelayCount"), 
             count(F.when(col("DepDelay") < 0, 1)).alias("EarlyCount"),
            count("*").alias("TotalCount")).orderBy(col("TotalCount").desc())
pandas_delay = count_delay.toPandas()# Assuming pdf has these columns: OriginCity, DelayedFlights, EarlyFlights, OnTimeFlights

# Bar positions
cities = top_20["Origin"]
x = np.arange(len(cities))

# Heights
early = top_20["EarlyCount"]
on_time = top_20["OnTimeCount"]
delayed = top_20["DelayCount"]

# Plot
plt.figure(figsize=(12, 6))
plt.bar(x, early, label="Early", color="green")
plt.bar(x, on_time, bottom=early, label="On Time", color="gray")
plt.bar(x, delayed, bottom=early + on_time, label="Delayed", color="red")

# Labels and formatting
plt.xticks(x, cities, rotation=45)
plt.ylabel("Number of Flights")
plt.title("Flight Status by Origin City (Top 20)")
plt.legend(title="Flight Status")
plt.tight_layout()
plt.show()





year_delay = filtered_df.select(["Year", "DepDelay"]).groupBy("Year")\
        .agg(count(F.when(col("DepDelay") > 0, 1)).alias("DelayCount"), 
             count(F.when(col("DepDelay") < 0, 1)).alias("EarlyCount"),
            count("*").alias("TotalCount")).orderBy(col("Year"))
pandas_year_delay = year_delay.toPandas()

pydf = pandas_year_delay.copy()
pydf["OnTimeCount"] = pydf["TotalCount"] - pydf["DelayCount"] - pydf["EarlyCount"]
year_counts = pydf

# Bar positions
years = year_counts["Year"]
x = np.arange(len(years))

# Heights
early = year_counts["EarlyCount"]
on_time = year_counts["OnTimeCount"]
delayed = year_counts["DelayCount"]

# Plot
plt.figure(figsize=(12, 6))
plt.bar(x, early, label="Early", color="green")
plt.bar(x, on_time, bottom=early, label="On Time", color="gray")
plt.bar(x, delayed, bottom=early + on_time, label="Delayed", color="red")

# Labels and formatting
plt.xticks(x, years)
plt.ylabel("Number of Flights")
plt.title("Flight Status by Year")
plt.legend(title="Flight Status")
plt.tight_layout()
plt.show()





month_delay = filtered_df.select(["Month", "DepDelay"]).groupBy("Month")\
        .agg(count(F.when(col("DepDelay") > 0, 1)).alias("DelayCount"), 
             count(F.when(col("DepDelay") < 0, 1)).alias("EarlyCount"),
            count("*").alias("TotalCount")).orderBy(col("Month"))
pandas_month_delay = month_delay.toPandas()

pmdf = pandas_month_delay.copy()
pmdf["OnTimeCount"] = pmdf["TotalCount"] - pmdf["DelayCount"] - pmdf["EarlyCount"]
month_counts = pmdf

# Bar positions
months = month_counts["Month"]
x = np.arange(len(months))

# Heights
early = month_counts["EarlyCount"]
on_time = month_counts["OnTimeCount"]
delayed = month_counts["DelayCount"]

# Plot
plt.figure(figsize=(12, 6))
plt.bar(x, early, label="Early", color="green")
plt.bar(x, on_time, bottom=early, label="On Time", color="gray")
plt.bar(x, delayed, bottom=early + on_time, label="Delayed", color="red")

# Labels and formatting
plt.xticks(x, months)
plt.ylabel("Number of Flights")
plt.title("Flight Status by Month")
plt.legend(title="Flight Status")
plt.tight_layout()
plt.show()





cols_to_keep_2 = ["Operating_Airline ", "Origin", "Dest", "ArrDelayMinutes", "DepDelayMinutes", "Distance", "OriginCityName", "DestCityName"]
df2 = df.select(cols_to_keep_2)


# group by origin and city, then calculating the total average delay between the cities
route_delays = filtered_df.groupBy("OriginCityName", "DestCityName") \
    .agg(
        (F.avg("DepDelayMinutes") + F.avg("ArrDelayMinutes")).alias("AvgTotalDelay")
    ) \
    .orderBy(F.col("AvgTotalDelay").desc())


# convert to pandas
route_delays_pd = route_delays.limit(10).toPandas()


# combining origin and dest for visual purposes 
route_delays_pd['Route'] = route_delays_pd['OriginCityName'] + ' to ' + route_delays_pd['DestCityName']


# plot
plt.figure(figsize=(14, 8))
sns.barplot(x='AvgTotalDelay', y='Route', data=route_delays_pd)
plt.title('Top 10 Most Delayed Flight Routes')
plt.xlabel('Average Total Delay in Minutes')
plt.ylabel('Route (Origin to Destination)')
plt.xticks(rotation=0)
plt.show()





# combining departure delay and arrival delay to one column
df2 = filtered_df.withColumn('TotalDelay', F.col('DepDelayMinutes') + F.col('ArrDelayMinutes'))


# selecting only delays that are over 0
delayed_flights = df2.filter(df2['TotalDelay'] > 0)


# group by airline, then calculating the total average delay
total_delay = delayed_flights.groupBy("Operating_Airline ").agg(
    F.avg("TotalDelay").alias("TotalDelayMinutes")
).orderBy(F.col("TotalDelayMinutes").desc())


# convert to pandas
total_delay_pd = total_delay.limit(10).toPandas()


total_delay_pd.head(10)


# plot
plt.figure(figsize=(12, 8))
sns.barplot(x="TotalDelayMinutes", y="Operating_Airline ", data=total_delay_pd)
plt.title('Top 10 Airlines with the Most Delay in Minutes')
plt.xlabel('Average Total Delay in Minutes')
plt.ylabel('Airline')
plt.show()





# convert to pandas
distance_delays_pd = df2.select("Distance", "DepDelayMinutes").toPandas()


# convert to numeric to prevent error
distance_delays_pd['Distance'] = pd.to_numeric(distance_delays_pd['Distance'], errors='coerce')
distance_delays_pd['DepDelayMinutes'] = pd.to_numeric(distance_delays_pd['DepDelayMinutes'], errors='coerce')


# removing any NaNs and 0 values
distance_delays_pd = distance_delays_pd.dropna(subset=['Distance', 'DepDelayMinutes'])
distance_delays_pd = distance_delays_pd[(distance_delays_pd['Distance'] > 0) & (distance_delays_pd['DepDelayMinutes'] > 0)]


# plot
plt.figure(figsize=(12, 8))
sns.scatterplot(x='Distance', y='DepDelayMinutes', data= distance_delays_pd, alpha=0.6)
plt.title('Flight Distance vs Departure Delay')
plt.xlabel('Flight Distance in Miles')
plt.ylabel('Departure Delay in Minutes')
plt.show()
