{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "263d72e9-c4b6-4e1e-9c8f-ee3c0e44b16e",
   "metadata": {},
   "source": [
    "# Group Project Milestone 2: Data Exploration & Initial PreProcessing\n",
    "\n",
    "In this assignment you will need to:\n",
    "\n",
    "1. Create a GitHub ID\n",
    "2. Create a GitHub Repository (Public or Private it is up to you. In the end it will have to be Public) and add your group members as collaborators\n",
    "3. Perform the data exploration step (i.e. evaluate your data, # of observations, details about your data distributions, scales, missing data, column descriptions) Note: For image data you can still describe your data by the number of classes, # of images, plot example classes of the image, size of images, are sizes uniform? Do they need to be cropped? normalized? etc.\n",
    "4. Plot your data. For tabular data, you will need to run scatters, for image data, you will need to plot your example classes.\n",
    "5. How will you preprocess your data? You should explain this in your README.md file and link your Jupyter notebook to it. All code and  Jupyter notebooks have be uploaded to your repo.\n",
    "6. You must also include in your Jupyter Notebook, a link for data download and environment setup requirements: \n",
    "\n",
    "\n",
    "!wget !unzip like functions as well as !pip install functions for non standard libraries not available in colab are required to be in the top section of your jupyter lab notebook. Or having the data on GitHub (you will need the academic license for GitHub to do this, larger datasets will require a link to external storage)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43e3dd4-fca0-4421-9ce5-d68e001a4cd8",
   "metadata": {},
   "source": [
    "## GitHub ID\n",
    "\n",
    "https://github.com/SmoothData-BigBrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88642e35-73bb-4077-acc6-184563785bc6",
   "metadata": {},
   "source": [
    "## Dataset link\n",
    "\n",
    "https://www.kaggle.com/datasets/robikscube/flight-delay-dataset-20182022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d757d7-5d93-4e03-b350-a04fc12a5d31",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Perform the data exploration step (i.e. evaluate your data, # of observations, details about your data distributions, scales, missing data, column descriptions) Note: For image data you can still describe your data by the number of classes, # of images, plot example classes of the image, size of images, are sizes uniform? Do they need to be cropped? normalized? etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a03d6c-1a93-4b54-8ecf-596818ee30e4",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8cc60b-8fd9-4a6a-b979-c114ba13dd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import util\n",
    "\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col, isnan, when, count, isnull, sum\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e41cd7-f31e-46cd-bcf1-138b597e7944",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Flight Data Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863f162f-b363-4c17-b226-356b1582e8e0",
   "metadata": {},
   "source": [
    "### Read in dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feb895f-738d-486f-8d67-4e7a1ff8251f",
   "metadata": {},
   "source": [
    "#### read in data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c2de02-6753-4c6d-a5cf-d724eba5828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# home_dir = os.path.expanduser('~')\n",
    "# path_for_Nam = 'C:/GitGroupProject/GroupProject' # comment this later\n",
    "# download_path = os.path.join(path_for_Nam,'/data/') # comment this later\n",
    "\n",
    "download_path = os.path.join('/workspaces/GroupProject/data/') # Uncomment this later\n",
    "\n",
    "# home_dir = os.path.expanduser('~')\n",
    "# download_path = os.path.join(home_dir, 'GroupProject/data/')\n",
    "file_id = '1tch7xbFIgBtXKXa16E4QCpVKedUExfO3'  # My File ID for airlines.zip on GDrive \n",
    "util.check_and_fetch_data(file_id, download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf04e58-8767-4dd4-ae08-f790b0920c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_path = '~/Desktop/GroupProject/data/archive/raw'\n",
    "# path_for_Nam = 'C:/GitGroupProject/GroupProject'\n",
    "# home_dir = os.path.expanduser('~')\n",
    "# download_path = os.path.join(home_dir, 'Desktop/GroupProject/data/')\n",
    "\n",
    "# download_path = os.path.join(path_for_Nam, '/data/')\n",
    "# Nam_local = 'C:/lecture-notebooks/GroupProject/data/archive/raw' # comment this later\n",
    "\n",
    "# csv_files = glob.glob(f\"{Nam_local}/*.csv\") # comment this later\n",
    "csv_files = glob.glob(f\"{download_path}archive/raw/*.csv\") #Uncomment this later\n",
    "df = spark.read.csv(csv_files,\n",
    "                       sep = ',',\n",
    "                       inferSchema = True,\n",
    "                       header = True)\n",
    "\n",
    "df.coalesce(1).write.csv(\"combined_file_csv\", header=True) # Uncomment this later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54311585-7681-4a72-a1a5-c7e1516e6f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").parquet(\"combined_file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3328ed84-368d-4e38-8730-066ecb00c303",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"combined_file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35740a54-1abf-4d8b-9840-ebd4b670f67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read.csv('combined_file.csv', sep = ',', inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbe9348-4801-4a17-b8a2-a6b3d27b2c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_des = spark.read.csv('flights_column_des.csv', sep = ',', inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52b9cd5-ec4f-4234-ab63-62fcb7d2475f",
   "metadata": {},
   "source": [
    "## Explore Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1dbdd3-0278-4743-b6eb-fd8f912fe390",
   "metadata": {},
   "source": [
    "### Get dataset shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc98c34-1dc1-45ee-921e-cb7c2534331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get df shape\n",
    "num_entries = df.count()\n",
    "num_cols = len(df.columns)\n",
    "print(f\"Shape of the DataFrame: ({num_entries}, {num_cols})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ead44d9-dd3c-44f0-b34b-bd7eee5f9400",
   "metadata": {},
   "source": [
    "### Explore null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10788ca0-d5aa-49e1-9d1a-6c2a0d024700",
   "metadata": {},
   "source": [
    "#### Column:Null Value Counts stored as a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fa680a-affe-4b04-840e-a52fb01db98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute count of non-null vals for each col in df\n",
    "\n",
    "null_counts = df.select([sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in df.columns]) \\\n",
    "                .collect()[0] \\\n",
    "                .asDict()\n",
    "\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007c5300-296e-490e-93f3-165d27f63e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary indicates that the last several cols have all nulls, print last 10 cols of df to manually inspect if NULLS are present\n",
    "df.select(df.columns[-10:]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739efbd1-1ebd-4549-a7a6-9246cdf4ed3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary indicates that the first several cols have no nulls, print first 10 cols of df to manually inspect if NULLS are present\n",
    "df.select(df.columns[:10]).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892ad7f6-6fca-49b6-a39c-fda420eaf2c7",
   "metadata": {},
   "source": [
    "#### Computing non-null counts as percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60addd02-eed0-42b9-8aea-20ef522d7115",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_null_counts = df.select([count(col(c)).alias(c) for c in df.columns]).collect()[0].asDict()\n",
    "\n",
    "# Calculate non-null percentages\n",
    "non_null_percentages = {\n",
    "    col_name: (count_val / num_entries) * 100\n",
    "    for col_name, count_val in non_null_counts.items()\n",
    "}\n",
    "\n",
    "sorted_columns = sorted(non_null_percentages.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for col_name, pct in sorted_columns:\n",
    "    print(f\"{col_name}: {pct:.2f}% non-null\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2587bb8f-7c94-4206-abbf-6892a1946f81",
   "metadata": {},
   "source": [
    "### Subset dataset - removing columns with <90% null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6026b7-cf26-4ba0-aad3-ed786cca2cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_above_90 = [col_name for col_name, pct in non_null_percentages.items() if pct >= 90]\n",
    "filtered_df = df.select(columns_above_90)\n",
    "filtered_df.select(filtered_df.columns[:8]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c44dab-7784-4664-a50a-39a53641cda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip the extra spaces from col names \n",
    "for c in filtered_df.columns:\n",
    "    filtered_df = filtered_df.withColumnRenamed(c, c.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a120f4-61e3-4e38-ad52-0b21fb368318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filtered df shape\n",
    "filtered_num_rows = filtered_df.count()\n",
    "filtered_num_cols = len(filtered_df.columns)\n",
    "print(f\"Shape of the Filtered DataFrame removing cols w/ <90% non-null values: ({filtered_num_rows}, {filtered_num_cols})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fc3021-969e-4854-8e82-5b93325dcbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc94a931-7d70-460f-845f-ebf903d055a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the master df since we won't need it anymore at all\n",
    "del df\n",
    "# df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e0fafa-5968-4614-8d88-875119f399ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filtered df to not have to redo code later\n",
    "#filtered_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(\"filtered_df_temp\")\n",
    "\n",
    "# # read in already filtered_df saved previously\n",
    "# filtered_df = spark.read.csv('part-00000-b248588c-b561-414a-ba2c-bc77825e455a-c000.csv', sep = ',', inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ef213e",
   "metadata": {},
   "source": [
    "#### **Discussion**\n",
    "\n",
    "Dataset consists of columns with >90% non-null values and then it drops down to 0-17% non-null. Dataset to be used for further exploration will only include columns with >90% non-null values for more robust analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a60fe8-5968-46dc-8b89-6d30f7f97ef6",
   "metadata": {},
   "source": [
    "### Remaining Column Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc302c6-1c8d-474a-8b3a-eedbb2cff3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all cols in filtered_df\n",
    "filtered_cols = filtered_df.columns \n",
    "\n",
    "# remove any white space\n",
    "filtered_cols = [str(c).strip() for c in filtered_cols]\n",
    "\n",
    "# subset column description dataframe for only columns in filtered dataset\n",
    "filtered_col_des = col_des.filter(col('column').isin(filtered_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e0eb34-70f1-4c87-a7d1-575fab89b0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check df was filtered correctly, length & row count should match\n",
    "print(len(filtered_cols))\n",
    "print(filtered_col_des.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9b65cc-65c0-458f-8669-f33cc3f960c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View all column descriptions in filtered dataframe\n",
    "# Full data col description is in \"../data/README.md\"\n",
    "filtered_col_des.show(n=filtered_col_des.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc67605-0fba-46ec-b337-358d30e82339",
   "metadata": {},
   "source": [
    "### Explore Dataset Statistics & Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8010c2b-e810-4c3d-bd64-6653b9f5d63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data type for each column\n",
    "for name, dtype in filtered_df.dtypes:\n",
    "    print(f\"{name}: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139371c8-8fcb-4638-a3ed-56cbf475727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_string_cols = [col_name for col_name, dtype in filtered_df.dtypes if dtype != 'string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10722ce-b5c3-4735-a49d-86032ece9115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset column description dataframe for only non-string\n",
    "non_string_col_des = filtered_col_des.filter(col('column').isin(non_string_cols))\n",
    "non_string_col_des.show(n=non_string_col_des.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac44c139-4bdb-4771-9a9b-1e0be2ea4635",
   "metadata": {},
   "source": [
    "### Discussion on skewed data distributions\n",
    "\n",
    "When taking a look at the columns with the most amount of skew in the data distribution, columns that are ID inidicators or Flight numbers do not make sense to further investigations of data distributions. Although these are numerical values, they represent categorical variables as opposed to continuous. \n",
    "\n",
    "Columns with 'ID','Number', 'Origin', 'Dest' in the column name will be removed from statistical analysis to remove these categorical variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094db735-e065-47b8-b634-2a7b570891af",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_col_des = non_string_col_des.filter(\n",
    "    ~non_string_col_des['column'].rlike('Dest|Origin|ID|Number|FlightDate')\n",
    ")\n",
    "cont_col_des.show(n=cont_col_des.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f447eb1-f02c-4630-b549-e17ba5c22874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get statistics for all continuous variables\n",
    "cont_cols = [row['column'] for row in cont_col_des.select('column').collect()]\n",
    "\n",
    "describe_df = filtered_df.select(cont_cols).describe()\n",
    "\n",
    "# compute Q1, Median, Q3 for each column\n",
    "stats = {\n",
    "    \"25%\": {},\n",
    "    \"50%\": {},\n",
    "    \"75%\": {}\n",
    "}\n",
    "\n",
    "for col_name in cont_cols:\n",
    "    q1, median, q3 = filtered_df.approxQuantile(col_name, [0.25, 0.5, 0.75], 0.01)\n",
    "    stats[\"25%\"][col_name] = str(q1)\n",
    "    stats[\"50%\"][col_name] = str(median)\n",
    "    stats[\"75%\"][col_name] = str(q3)\n",
    "\n",
    "# convert new rows to df rows\n",
    "new_rows = [Row(summary=stat_name, **cols) for stat_name, cols in stats.items()]\n",
    "quartile_df = spark.createDataFrame(new_rows)\n",
    "\n",
    "# append the new rows to describe_df\n",
    "full_summary_df = describe_df.unionByName(quartile_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa81d22-37f7-4b68-80d6-d04e4bac3cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the new rows to describe_df\n",
    "full_summary_df = describe_df.unionByName(quartile_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407891b4-f3e8-40a5-a9fa-570a30abc424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert and save as parquet\n",
    "# full_summary_df.to_parquet(\"full_summary_df\", index=False)\n",
    "full_summary_df.write.mode(\"overwrite\").parquet(\"full_summary_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70530df-be00-4d7f-82c4-f3ae8ef1ddfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_summary_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(\"full_summary_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910780ae-036c-4734-8d7c-9b09bef8c032",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_summary_df = spark.read.parquet(\"full_summary_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d044b40-76f0-4546-92f9-d5d923193c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_summary_df.select(full_summary_df.columns[:6]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce22dad0-0fda-4167-a939-1e555d30da1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view df columns\n",
    "full_summary_df.select(full_summary_df.columns[11:17]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc35b0f-0d14-42fb-aca4-a1b71fa2c667",
   "metadata": {},
   "source": [
    "### Explore skewed data\n",
    "\n",
    "mean > median, data is right-skewed (longer tail on the right)\n",
    "median < mean, data is left-skewed (longer tail on the left)\n",
    "\n",
    "This code is to find top 20 features with largest skews. These features will then be plotted in histograms\n",
    "\n",
    "The purpose of doing this is to understand if there are any outliers in the dataset that may be worth removing from the dataset prior to applying ML methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b3acce-c99b-40b9-95a0-bde8890a30d0",
   "metadata": {},
   "source": [
    "### Explore data distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56419c4f-626b-4dc2-893f-9040acd940a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mean and median rows as dicts\n",
    "mean_row = full_summary_df.filter(col(\"summary\") == \"mean\").collect()[0].asDict()\n",
    "median_row = full_summary_df.filter(col(\"summary\") == \"50%\").collect()[0].asDict()\n",
    "\n",
    "# skip the 'summary' key\n",
    "cols = [col for col in mean_row.keys() if col != \"summary\"]\n",
    "\n",
    "# build rows of (column, absolute_diff, skew direction)\n",
    "result_rows = []\n",
    "for c in cols: # for each col\n",
    "    mean_val = float(mean_row[c]) # get mean\n",
    "    median_val = float(median_row[c]) # get median\n",
    "    diff = __builtins__.abs(mean_val - median_val) # get abs difference\n",
    "    skew = \"right\" if mean_val > median_val else \"left\" if mean_val < median_val else \"none\" # get skew direction\n",
    "    result_rows.append(Row(column=c, absolute_diff=diff, skew=skew)) # aggregate\n",
    "\n",
    "# create df\n",
    "diff_df = spark.createDataFrame(result_rows)\n",
    "\n",
    "# get top 20\n",
    "top_skewed = diff_df.orderBy(col(\"absolute_diff\").desc()).limit(20)\n",
    "\n",
    "top_skewed.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086edcdb-8a88-49f5-aedc-694077773bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all cols in filtered_df\n",
    "skewed_cols = [row['column'] for row in top_skewed.select('column').collect()]\n",
    "\n",
    "# remove any white space\n",
    "skewed_cols = [str(c).strip() for c in skewed_cols]\n",
    "\n",
    "# subset column description dataframe for only columns in filtered dataset\n",
    "skewed_col_des = col_des.filter(col('column').isin(skewed_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7774476a-fa3d-4d39-b880-c42e17a85f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed_col_des.show(n=skewed_col_des.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cd615b-04cb-4267-bd3f-f12200da0d43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list of columns from 'top_skewed'\n",
    "columns_to_plot = [row['column'] for row in top_skewed.collect()]\n",
    "\n",
    "# filter the columns that exist in filtered_df\n",
    "valid_columns = [col for col in columns_to_plot if col in filtered_df.columns]\n",
    "\n",
    "# plot histograms for each column\n",
    "n_cols = 4  # 4 histograms per row\n",
    "n_rows = (len(valid_columns) + n_cols - 1) // n_cols  # calculate num rows needed\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 5))\n",
    "\n",
    "# flatten axes for easier indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# loop through cols and plot\n",
    "for i, column in enumerate(valid_columns):\n",
    "    hist = filtered_df.select(column).rdd.flatMap(lambda x: x).histogram(20)  # 20 bins\n",
    "\n",
    "    bin_edges, bin_counts = hist\n",
    "\n",
    "    # plot the histogram using the bin edges and counts\n",
    "    axes[i].bar(bin_edges[:-1], bin_counts, width=(bin_edges[1] - bin_edges[0]), edgecolor='black')\n",
    "\n",
    "    # set axes & title\n",
    "    axes[i].set_title(f\"Histogram of {column}\")\n",
    "    axes[i].set_xlabel('Value')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "# turn off any unused subplots\n",
    "for i in range(len(valid_columns), len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "# need to update to add labels for axis \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5265a98b-d34a-4bf0-be25-150481e74fd4",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "The distance column, majority of flights in this dataset have a distance <1000 miles. With a few outliers ranging from 3000-5000 miles. \n",
    "\n",
    "Wheels On & Wheels Off time and CRSDepTime & DepTime columns have a few outliers at 0:00-4:00am, majority of times are listed between 5:00 & 23:59\n",
    "\n",
    "The majority of TaxiOut and TaxiIn times are around 0 (or <50minutes). However, there are outliers sitting at ~1300 & 300 minutes respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09a6c97-5cb6-4897-9f92-865985101252",
   "metadata": {},
   "source": [
    "## Which Origin Cities had the most delayed flights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1cd7b9-0510-49f4-895e-88d5c482c4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = [\"Year\", \"Month\", \"DayofMonth\", \"Origin\", \"OriginCityName\", \"DestCityName\", \"DepDelay\", \"ArrDelay\", \"Cancelled\", \"CRSElapsedTime\", \"ActualElapsedTime\"]\n",
    "flight_delayed_df = filtered_df.select(cols_to_keep)\n",
    "flight_delayed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a482027a-fcad-464f-8677-d13cfd74cff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_delay = flight_delayed_df.select([\"Origin\", \"DepDelay\"]).groupBy(\"Origin\")\\\n",
    "        .agg(count(F.when(col(\"DepDelay\") > 0, 1)).alias(\"DelayCount\"), \n",
    "             count(F.when(col(\"DepDelay\") < 0, 1)).alias(\"EarlyCount\"),\n",
    "            count(\"*\").alias(\"TotalCount\")).orderBy(col(\"TotalCount\").desc())\n",
    "pandas_delay = count_delay.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e785195-1ef5-4d5c-ab9a-1b8895f4722e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa229bb-0f91-4f37-9603-e76daf90ed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pandas_delay.copy()\n",
    "pdf[\"OnTimeCount\"] = pdf[\"TotalCount\"] - pdf[\"DelayCount\"] - pdf[\"EarlyCount\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37c32e2-7d91-48c6-a9c3-e01f5310b9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20 = pdf.head(20)\n",
    "top_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4affc1df-26b5-4def-a19b-d8740631b79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_delay = flight_delayed_df.select([\"Origin\", \"DepDelay\"]).groupBy(\"Origin\")\\\n",
    "        .agg(count(F.when(col(\"DepDelay\") > 0, 1)).alias(\"DelayCount\"), \n",
    "             count(F.when(col(\"DepDelay\") < 0, 1)).alias(\"EarlyCount\"),\n",
    "            count(\"*\").alias(\"TotalCount\")).orderBy(col(\"TotalCount\").desc())\n",
    "pandas_delay = count_delay.toPandas()# Assuming pdf has these columns: OriginCity, DelayedFlights, EarlyFlights, OnTimeFlights\n",
    "\n",
    "# Bar positions\n",
    "cities = top_20[\"Origin\"]\n",
    "x = np.arange(len(cities))\n",
    "\n",
    "# Heights\n",
    "early = top_20[\"EarlyCount\"]\n",
    "on_time = top_20[\"OnTimeCount\"]\n",
    "delayed = top_20[\"DelayCount\"]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x, early, label=\"Early\", color=\"green\")\n",
    "plt.bar(x, on_time, bottom=early, label=\"On Time\", color=\"gray\")\n",
    "plt.bar(x, delayed, bottom=early + on_time, label=\"Delayed\", color=\"red\")\n",
    "\n",
    "# Labels and formatting\n",
    "plt.xticks(x, cities, rotation=45)\n",
    "plt.ylabel(\"Number of Flights\")\n",
    "plt.title(\"Flight Status by Origin City (Top 20)\")\n",
    "plt.legend(title=\"Flight Status\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0cb107-82c8-4ef5-a286-b71f118439fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_delay = filtered_df.select([\"Year\", \"DepDelay\"]).groupBy(\"Year\")\\\n",
    "        .agg(count(F.when(col(\"DepDelay\") > 0, 1)).alias(\"DelayCount\"), \n",
    "             count(F.when(col(\"DepDelay\") < 0, 1)).alias(\"EarlyCount\"),\n",
    "            count(\"*\").alias(\"TotalCount\")).orderBy(col(\"TotalCount\"))\n",
    "pandas_year_delay = year_delay.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a11f36a-d220-4de2-bffd-3eb1e4d8d252",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep_2 = [\"Operating_Airline\", \"Origin\", \"Dest\", \"ArrDelayMinutes\", \"DepDelayMinutes\", \"Distance\", \"OriginCityName\", \"DestCityName\"]\n",
    "df2 = filtered_df.select(cols_to_keep_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4ea3b3",
   "metadata": {},
   "source": [
    "## Which routes had the most delays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed5ef3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by origin and city, then calculating the total average delay between the cities\n",
    "route_delays = df2.groupBy(\"OriginCityName\", \"DestCityName\") \\\n",
    "    .agg(\n",
    "        (F.avg(\"DepDelayMinutes\") + F.avg(\"ArrDelayMinutes\")).alias(\"AvgTotalDelay\")\n",
    "    ) \\\n",
    "    .orderBy(F.col(\"AvgTotalDelay\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac38afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pandas\n",
    "route_delays_pd = route_delays.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cd5e53-8fdb-48ce-8d14-6406a21ce58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "route_delays_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cd4d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining origin and dest for visual purposes \n",
    "route_delays_pd['Route'] = route_delays_pd['OriginCityName'] + ' to ' + route_delays_pd['DestCityName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bc7146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(x='AvgTotalDelay', y='Route', data=route_delays_pd)\n",
    "plt.title('Top 10 Most Delayed Flight Routes')\n",
    "plt.xlabel('Average Total Delay in Minutes')\n",
    "plt.ylabel('Route (Origin to Destination)')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425647c9",
   "metadata": {},
   "source": [
    "## Which airlines experience the most delays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bde894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining departure delay and arrival delay to one column\n",
    "df2 = df2.withColumn('TotalDelay', F.col('DepDelayMinutes') + F.col('ArrDelayMinutes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e21f13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting only delays that are over 0\n",
    "delayed_flights = df2.filter(df2['TotalDelay'] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0990ab41-5067-4225-975c-c903ddf19505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delayed_flights.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9704ab78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by airline, then calculating the total average delay\n",
    "total_delay = delayed_flights.groupBy(\"Operating_Airline\").agg(\n",
    "    F.avg(\"TotalDelay\").alias(\"TotalDelayMinutes\")\n",
    ").orderBy(F.col(\"TotalDelayMinutes\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b878004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pandas\n",
    "total_delay_pd = total_delay.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d634d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_delay_pd.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b00a6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=\"TotalDelayMinutes\", y=\"Operating_Airline\", data=total_delay_pd)\n",
    "plt.title('Top 10 Airlines with the Most Delay in Minutes')\n",
    "plt.xlabel('Average Total Delay in Minutes')\n",
    "plt.ylabel('Airline')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a26667",
   "metadata": {},
   "source": [
    "## Do flights with a longer distance have longer departure delays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf50580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pandas\n",
    "distance_delays_pd = df2.select(\"Distance\", \"DepDelayMinutes\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8484f124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numeric to prevent error\n",
    "distance_delays_pd['Distance'] = pd.to_numeric(distance_delays_pd['Distance'], errors='coerce')\n",
    "distance_delays_pd['DepDelayMinutes'] = pd.to_numeric(distance_delays_pd['DepDelayMinutes'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e3064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing any NaNs and 0 values\n",
    "distance_delays_pd = distance_delays_pd.dropna(subset=['Distance', 'DepDelayMinutes'])\n",
    "distance_delays_pd = distance_delays_pd[(distance_delays_pd['Distance'] > 0) & (distance_delays_pd['DepDelayMinutes'] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b30dd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x='Distance', y='DepDelayMinutes', data= distance_delays_pd, alpha=0.6)\n",
    "plt.title('Flight Distance vs Departure Delay')\n",
    "plt.xlabel('Flight Distance in Miles')\n",
    "plt.ylabel('Departure Delay in Minutes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9c5d09-ad6a-468f-851b-b45f143e261a",
   "metadata": {},
   "source": [
    "### Checking delayed and cancelled flights Group By by operating airlines and time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c992ae14-976c-44a0-bed8-1001fa18301c",
   "metadata": {},
   "source": [
    "The following columns were manually chosen after reading column description on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ecfd8d-6ef2-4f25-8d2d-8907059bc2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_check = ['Marketing_Airline_Network', 'Operating_Airline', 'Origin', 'Dest', 'DepDel15', 'DepDelay', 'ArrDel15', 'ArrDelay',\\\n",
    "                 'Cancelled', 'CarrierDelay', 'WeatherDelay', 'NASDelay', 'SecurityDelay', 'LateAircraftDelay',\\\n",
    "                 'Distance', 'Year', 'Month', 'DayofMonth', 'FlightDate', 'Flight_Number_Operating_Airline']\n",
    "print(f'col_to_check = {len(cols_to_check)}')\n",
    "# Check to see if interested columns is in columns_with_few_nulls\n",
    "removed_col = []\n",
    "my_cols = []\n",
    "for c in cols_to_check:\n",
    "    if c in filtered_df.columns:\n",
    "        my_cols.append(c)\n",
    "    else:\n",
    "        removed_col.append(c)\n",
    "\n",
    "print('Chosen columns:')\n",
    "print(my_cols)\n",
    "print()\n",
    "print('Rejected columns:')\n",
    "print(removed_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277c08e3-227b-485a-935f-83b889b504e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out a subset of columns to do data analysis\n",
    "Nam_df = filtered_df.select(my_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf8401c-ac80-43e5-a73a-f4c97b1e3890",
   "metadata": {},
   "source": [
    "Cancelled flights have nulls values in other columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76698145-d6d0-4ac0-b36d-ab7aa82e4752",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_check = ['DepDel15', 'DepDelay', 'ArrDel15', 'ArrDelay']\n",
    "print('There are entries where Cancelled column will have value of 1 (canceled) while other columns might be null')\n",
    "for c in cols_to_check:\n",
    "    num_to_display = Nam_df.where(isnull(col(c)) & (col('Cancelled') == 1)).count()\n",
    "    print(f'Number of entries where {c} column is null but Cancelled is 1: {num_to_display}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3ab495-d2f2-490d-aa05-ef7cf4f2885d",
   "metadata": {},
   "source": [
    "Since I intend to do data exploration with cancelled flights, it is not a good idea for me to do dropna() on the dataset as I will lose data for those flights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f074bb8f-64f3-43fe-9f5a-63e85e6868a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of flights that are delayed by more 15 minutes and cancelled flights\n",
    "num_delayed = Nam_df.where(col('DepDel15') == 1).count()\n",
    "num_cancelled = Nam_df.where(col('Cancelled') == 1).count()\n",
    "print(f'Number of flights that were delayed by more than 15 minutes: {num_delayed}')\n",
    "print(f'Number of flights that were cancelled: {num_cancelled}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bd7e5d-894a-4f18-a440-acefbb3d8785",
   "metadata": {},
   "source": [
    "#### Group by operating airline data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb0bf2-2622-4ed9-a0e3-7b7260c2c614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate by operating airline and turn into Pandas dataframe\n",
    "airline_flights = Nam_df.groupBy(col('Operating_Airline')).agg(F.sum(col('DepDel15')).alias('DelayedFlights'),\n",
    "                                                               F.sum(col('Cancelled')).alias('CancelledFlights'),\n",
    "                                                               F.count('*').alias('TotalFlights'))\n",
    "airline_flights_pd = airline_flights.toPandas()\n",
    "airline_flights_pd['DelayedPercentage'] = airline_flights_pd['DelayedFlights'] / airline_flights_pd['TotalFlights']\n",
    "airline_flights_pd['CancelledPercentage'] = airline_flights_pd['CancelledFlights'] / airline_flights_pd['TotalFlights']\n",
    "airline_flights_pd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d1961e-4871-4b2c-bc6c-45738307cd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph number of delayed flights by airlines\n",
    "airline_flights_pd = airline_flights_pd.sort_values(by = 'DelayedFlights', axis = 0, ascending = True)\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "percentage_barplot = sns.barplot(data = airline_flights_pd, x = 'Operating_Airline', y = 'DelayedFlights')\n",
    "plt.title('Number of delayed flights by Operating Airline')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a017b96b-7e89-4928-bd7d-cb5ab53eadd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph delay flight percentage by airline\n",
    "airline_flights_pd = airline_flights_pd.sort_values(by = 'DelayedPercentage', axis = 0, ascending = True)\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "percentage_barplot = sns.barplot(data = airline_flights_pd, x = 'Operating_Airline', y = 'DelayedPercentage')\n",
    "plt.title('Percentage of delayed flights by Operating Airline')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc87dcf-acea-4248-991f-4ac13f32ff8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph number of cancelled flights by airlines\n",
    "airline_flights_pd = airline_flights_pd.sort_values(by = 'CancelledFlights', axis = 0, ascending = True)\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.barplot(data = airline_flights_pd, x = 'Operating_Airline', y = 'CancelledFlights')\n",
    "plt.title('Number of cancelled flights by Operating Airline')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b9acda-ca0a-4a91-9157-a26481115591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph peracentage of cancelled flights by airlines\n",
    "airline_flights_pd = airline_flights_pd.sort_values(by = 'CancelledPercentage', axis = 0, ascending = True)\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.barplot(data = airline_flights_pd, x = 'Operating_Airline', y = 'CancelledPercentage')\n",
    "plt.title('Percentage of cancelled flights by Operating Airline')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21105ab1-d942-49fb-9a8c-fea16cd91449",
   "metadata": {},
   "source": [
    "#### Group by time data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713a8ee0-a6b3-4b6c-a223-e381b543721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Year and Month and turn into Pandas df\n",
    "time_agg = Nam_df.groupBy('Year', 'Month').agg(F.sum(col('DepDel15')).alias('DelayedFlights'),\n",
    "                                              F.sum(col('Cancelled')).alias('CancelledFlights'),\n",
    "                                              F.count('*').alias('TotalFlights'))\n",
    "time_agg_pd = time_agg.toPandas()\n",
    "time_agg_pd['DelayedPercentage'] = time_agg_pd['DelayedFlights'] / time_agg_pd['TotalFlights']\n",
    "time_agg_pd['CancelledPercentage'] = time_agg_pd['CancelledFlights'] / time_agg_pd['TotalFlights']\n",
    "time_agg_pd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793de5b8-9350-4663-b46e-87cc74471a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph number of flights overtime\n",
    "colors = ['red', 'blue', 'green', 'purple', 'gold']\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "barplot = sns.lineplot(data = time_agg_pd, x = 'Month', y = 'TotalFlights', hue = 'Year', palette = colors)\n",
    "plt.title('Number of monthly flights over the year')\n",
    "plt.ylabel('')\n",
    "plt.legend(loc = 'upper right', bbox_to_anchor = (1.15, 1), title = 'Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e991c337-a0a3-4453-86b9-ff0023d9138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph percentage of delayed flights over time\n",
    "colors = ['red', 'blue', 'green', 'purple', 'gold']\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "barplot = sns.lineplot(data = time_agg_pd, x = 'Month', y = 'DelayedPercentage', hue = 'Year', palette = colors)\n",
    "plt.title('Percentage of delayed monthly flights over the year')\n",
    "plt.ylabel('')\n",
    "plt.legend(loc = 'upper right', bbox_to_anchor = (1.15, 1), title = 'Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563e8562-992e-41eb-ad62-7dcda34cd125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph percentage of cancelled flights over time\n",
    "colors = ['red', 'blue', 'green', 'purple', 'gold']\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "barplot = sns.lineplot(data = time_agg_pd, x = 'Month', y = 'CancelledPercentage', hue = 'Year', palette = colors)\n",
    "plt.title('Percentage of cancelled monthly flights over the year')\n",
    "plt.ylabel('')\n",
    "plt.legend(loc = 'upper right', bbox_to_anchor = (1.15, 1), title = 'Year')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0a6eee-1b8a-4c02-adf2-24f6b931ef7c",
   "metadata": {},
   "source": [
    "## Group by airlines and time discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e85723b-87bb-4c69-b23b-fe5463d19723",
   "metadata": {},
   "source": [
    "##### Data cleaning\n",
    "\n",
    "Thanks to the team's work, we can learn that the majority of columns in our dataset contains a large number of NaN. We are able to quickly filter out those columns and focus on the others.\n",
    "\n",
    "My analysis focused on delayed and cancelled flights. I learned that entries of cancelled flights will have nulls in other columns making a simple dropna() not a viable data cleaning method.\n",
    "\n",
    "If we are to work with cancelled flights, we have to find away to fill in the nulls of other columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50186cea-70e9-497f-8c18-2f8f67213c5a",
   "metadata": {},
   "source": [
    "##### Aggregated by operating airlines\n",
    "\n",
    "Despite how miserable air travelling is, flights are seldomly late. About 10 - 25% of flights are at least 15 minutes late to depart. From the graph, I would say that an average of about 15% of flights operated by any airlines are late to depart.\n",
    "\n",
    "Airlines are keen to keep their flights operational cancelling less than 5% of their total scheduled flights. This makes sense as cancellation results in not only loss of revenue but also compensation of damages and potential loss of opportunities.\n",
    "\n",
    "Airline denoted by KS (Peninsula Airways) stood out to me. They do not serve a lot of flights. But their delayed and cancelled metrics are area of improvement to say the least."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0311efc1-0aaf-4e47-a987-127511ebc4dc",
   "metadata": {},
   "source": [
    "##### Aggregated by time\n",
    "\n",
    "Acknowledgement: I understand that the airline industry was heavy affected by COVID-19 and that the industry is recovering to pre-pandemic numbers.\n",
    "\n",
    "Overall, it seems that February tends to be a slow month for the airline industry. The summer months are busy. The holiday months of September - December are only slight less busy than the summer months. People are more likely to travel in the second half of the year.\n",
    "\n",
    "The data for 2018 flights stood out to me. The year started with fewer flights than 2021 and 2022 (post COVID-19 years) but then suddenly gained 300,000 flights for the summer months. This indicates to me that there is a potential socio/economical/geopolitical event happening and/or issue with data collection.\n",
    "\n",
    "Delayed flights are likely to happen during peak of the traveling seasons contributing to the misery of air travelling. February is an interesting month as customers are not travelling but flights pick up an increase in chance of being delayed. My guess is that flight crews and ground crews are burned out from the holiday season and their performance is decreased.\n",
    "\n",
    "As for cancelled flights, there is a gigantic mountain that sits in the middle of the graph. Almost half of scheduled flights for April of 2020 are cancelled. I wonder what happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca76416-7b00-413d-818f-274347f170f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
