{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "263d72e9-c4b6-4e1e-9c8f-ee3c0e44b16e",
   "metadata": {},
   "source": [
    "# Group Project Milestone 2: Data Exploration & Initial PreProcessing\n",
    "\n",
    "In this assignment you will need to:\n",
    "\n",
    "1. Create a GitHub ID\n",
    "2. Create a GitHub Repository (Public or Private it is up to you. In the end it will have to be Public) and add your group members as collaborators\n",
    "3. Perform the data exploration step (i.e. evaluate your data, # of observations, details about your data distributions, scales, missing data, column descriptions) Note: For image data you can still describe your data by the number of classes, # of images, plot example classes of the image, size of images, are sizes uniform? Do they need to be cropped? normalized? etc.\n",
    "4. Plot your data. For tabular data, you will need to run scatters, for image data, you will need to plot your example classes.\n",
    "5. How will you preprocess your data? You should explain this in your README.md file and link your Jupyter notebook to it. All code and  Jupyter notebooks have be uploaded to your repo.\n",
    "6. You must also include in your Jupyter Notebook, a link for data download and environment setup requirements: \n",
    "\n",
    "\n",
    "!wget !unzip like functions as well as !pip install functions for non standard libraries not available in colab are required to be in the top section of your jupyter lab notebook. Or having the data on GitHub (you will need the academic license for GitHub to do this, larger datasets will require a link to external storage)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43e3dd4-fca0-4421-9ce5-d68e001a4cd8",
   "metadata": {},
   "source": [
    "## GitHub ID\n",
    "\n",
    "https://github.com/SmoothData-BigBrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88642e35-73bb-4077-acc6-184563785bc6",
   "metadata": {},
   "source": [
    "## Dataset link\n",
    "\n",
    "https://www.kaggle.com/datasets/robikscube/flight-delay-dataset-20182022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d757d7-5d93-4e03-b350-a04fc12a5d31",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Perform the data exploration step (i.e. evaluate your data, # of observations, details about your data distributions, scales, missing data, column descriptions) Note: For image data you can still describe your data by the number of classes, # of images, plot example classes of the image, size of images, are sizes uniform? Do they need to be cropped? normalized? etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a03d6c-1a93-4b54-8ecf-596818ee30e4",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8cc60b-8fd9-4a6a-b979-c114ba13dd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# importing util.py \n",
    "import util\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col, isnan, when, count, isnull\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc31d96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# home_dir = os.path.expanduser('~')\n",
    "download_path = os.path.join('/workspaces/GroupProject/data/')\n",
    "# home_dir = os.path.expanduser('~')\n",
    "# download_path = os.path.join(home_dir, 'GroupProject/data/')\n",
    "file_id = '1tch7xbFIgBtXKXa16E4QCpVKedUExfO3'  # My File ID for airlines.zip on GDrive \n",
    "util.check_and_fetch_data(file_id, download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e41cd7-f31e-46cd-bcf1-138b597e7944",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Flight Data Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863f162f-b363-4c17-b226-356b1582e8e0",
   "metadata": {},
   "source": [
    "### Read in dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feb895f-738d-486f-8d67-4e7a1ff8251f",
   "metadata": {},
   "source": [
    "#### read in individual raw files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73aca89-c90d-41b6-b97c-8002c97862b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf04e58-8767-4dd4-ae08-f790b0920c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# folder_path = '~/Desktop/GroupProject/data/archive/raw'\n",
    "csv_files = glob.glob(f\"{download_path}/archive/raw/*.csv\")\n",
    "df = spark.read.csv(csv_files,\n",
    "                       sep = ',',\n",
    "                       inferSchema = True,\n",
    "                       header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b3bb4e-07c2-4819-9e23-593c1e8a5833",
   "metadata": {},
   "source": [
    "#### read in combined raw files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35740a54-1abf-4d8b-9840-ebd4b670f67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('combined_file.csv', sep = ',', inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33ffe78-9c7e-4592-9155-ae0bfb108bcd",
   "metadata": {},
   "source": [
    "#### read in dataset column description csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbe9348-4801-4a17-b8a2-a6b3d27b2c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_des = spark.read.csv('flights_column_des.csv', sep = ',', inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52b9cd5-ec4f-4234-ab63-62fcb7d2475f",
   "metadata": {},
   "source": [
    "## Explore Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1dbdd3-0278-4743-b6eb-fd8f912fe390",
   "metadata": {},
   "source": [
    "### Get dataset shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc98c34-1dc1-45ee-921e-cb7c2534331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get df shape\n",
    "num_rows = df.count()\n",
    "num_cols = len(df.columns)\n",
    "print(f\"Shape of the DataFrame: ({num_rows}, {num_cols})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35740a54-1abf-4d8b-9840-ebd4b670f67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_entries = df.count() #29193782\n",
    "#num_entries = 29193782"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ead44d9-dd3c-44f0-b34b-bd7eee5f9400",
   "metadata": {},
   "source": [
    "### Explore null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10788ca0-d5aa-49e1-9d1a-6c2a0d024700",
   "metadata": {},
   "source": [
    "#### Column:Null Value Counts stored as a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f421ce77-9ad0-4975-bbf0-cb74b20b0eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#null_counts = df.select([count(col(c)).alias(c) for c in df.columns]).collect()[0].asDict()\n",
    "#print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fa680a-affe-4b04-840e-a52fb01db98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute count of non-null vals for each col in df\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "null_counts = df.select([sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in df.columns]) \\\n",
    "                .collect()[0] \\\n",
    "                .asDict()\n",
    "\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007c5300-296e-490e-93f3-165d27f63e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary indicates that the last several cols have all nulls, print last 10 cols of df to manually inspect if NULLS are present\n",
    "df.select(df.columns[-10:]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739efbd1-1ebd-4549-a7a6-9246cdf4ed3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary indicates that the first several cols have no nulls, print first 10 cols of df to manually inspect if NULLS are present\n",
    "df.select(df.columns[:10]).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db9ed32-1c79-46fc-a2c5-7eb1f016e76c",
   "metadata": {},
   "source": [
    "#### Checking dataset for columns with few nulls or no nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b8484f-4aa3-417b-8803-0c69493ab115",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_few_nulls = [col_name for col_name, count_val in null_counts.items() if count_val > 0.9*num_entries]\n",
    "#columns_with_no_nulls = [col_name for col_name, count_val in null_counts.items() if count_val != 0]\n",
    "print(columns_with_few_nulls)\n",
    "#print(columns_with_all_nulls)\n",
    "#df.select(*columns_with_nulls).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c59392f-c9f0-482a-8445-9178cc5a149f",
   "metadata": {},
   "source": [
    "#### Filtering dataset for columns with few nulls & viewing output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a5cadb-9b84-4a3a-8402-29d76ed2ee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = df.select(columns_with_few_nulls)\n",
    "#newdf.select(newdf.columns[0:10]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b58c4a1-f57e-442b-8841-be2fb7177216",
   "metadata": {},
   "outputs": [],
   "source": [
    "#newdf.select(newdf.columns[10:19]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4594a49-1de3-40e6-bfb4-ad679842ec9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#newdf.select(newdf.columns[19:33]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b8a25d-a139-4519-b62d-b04f642ae061",
   "metadata": {},
   "outputs": [],
   "source": [
    "#newdf.select(newdf.columns[33:50]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6124e1a-25aa-4bcc-bbc2-5609361037c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#newdf.select(newdf.columns[50:62]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8908fcb-e07e-44d9-a7ea-08530c798ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Count non-nulls in a list of specified columns\n",
    "\n",
    "#repeat_cols = [\"Marketing_Airline_Network\", \"Operated_or_Branded_Code_Share_Partners\", \"IATA_Code_Marketing_Airline\", \"Operating_Airline \", \"IATA_Code_Operating_Airline\"]\n",
    "#print(newdf.select([count(col(c)).alias(c) for c in repeat_cols]).collect()[0].asDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076bc688-9867-4265-a0cd-f293fd5881ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(newdf.select(repeat_cols).distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92da359e-8942-48ab-a2c3-913dabb602f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter dataset with cols to keep\n",
    "\n",
    "cols_to_keep = [\"Year\", \"Month\", \"DayofMonth\", \"Origin\", \"OriginCityName\", \"DestCityName\", \"DepDelay\", \"ArrDelay\", \"Cancelled\", \"CRSElapsedTime\", \"ActualElapsedTime\"]\n",
    "my_df = newdf.select(cols_to_keep)\n",
    "my_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892ad7f6-6fca-49b6-a39c-fda420eaf2c7",
   "metadata": {},
   "source": [
    "#### Computing non-null counts as percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60addd02-eed0-42b9-8aea-20ef522d7115",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_null_counts = df.select([count(col(c)).alias(c) for c in df.columns]).collect()[0].asDict()\n",
    "\n",
    "# Calculate non-null percentages\n",
    "non_null_percentages = {\n",
    "    col_name: (count_val / num_entries) * 100\n",
    "    for col_name, count_val in non_null_counts.items()\n",
    "}\n",
    "\n",
    "sorted_columns = sorted(non_null_percentages.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for col_name, pct in sorted_columns:\n",
    "    print(f\"{col_name}: {pct:.2f}% non-null\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d4cd68-3406-49b5-b9d2-94535afb676d",
   "metadata": {},
   "source": [
    "#### **Discussion**\n",
    "\n",
    "Dataset consists of columns with >90% non-null values and then it drops down to 0-17% non-null. Dataset to be used for further exploration will only include columns with >90% non-null values for more robust analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2587bb8f-7c94-4206-abbf-6892a1946f81",
   "metadata": {},
   "source": [
    "### Subset dataset - removing columns with <90% null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6026b7-cf26-4ba0-aad3-ed786cca2cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_above_90 = [col_name for col_name, pct in non_null_percentages.items() if pct >= 90]\n",
    "filtered_df = df.select(columns_above_90)\n",
    "filtered_df.select(filtered_df.columns[:8]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a120f4-61e3-4e38-ad52-0b21fb368318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filtered df shape\n",
    "num_rows = filtered_df.count()\n",
    "num_cols = len(filtered_df.columns)\n",
    "print(f\"Shape of the Filtered DataFrame removing cols w/ <90% null values: ({num_rows}, {num_cols})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e0fafa-5968-4614-8d88-875119f399ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filtered df to not have to redo code later\n",
    "#filtered_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(\"filtered_df_temp\")\n",
    "\n",
    "# read in already filtered_df saved previously\n",
    "filtered_df = spark.read.csv('part-00000-b248588c-b561-414a-ba2c-bc77825e455a-c000.csv', sep = ',', inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a60fe8-5968-46dc-8b89-6d30f7f97ef6",
   "metadata": {},
   "source": [
    "### Remaining Column Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e0d14b-719c-4661-aeae-3fa729973835",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_des.count() # count original col number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc302c6-1c8d-474a-8b3a-eedbb2cff3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all cols in filtered_df\n",
    "filtered_cols = filtered_df.columns \n",
    "\n",
    "# remove any white space\n",
    "filtered_cols = [str(c).strip() for c in filtered_cols]\n",
    "\n",
    "# subset column description dataframe for only columns in filtered dataset\n",
    "filtered_col_des = col_des.filter(col('column').isin(filtered_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e0eb34-70f1-4c87-a7d1-575fab89b0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check df was filtered correctly, length & row count should match\n",
    "print(len(filtered_cols))\n",
    "print(filtered_col_des.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9b65cc-65c0-458f-8669-f33cc3f960c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View all column descriptions in filtered dataframe\n",
    "\n",
    "filtered_col_des.show(n=filtered_col_des.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc67605-0fba-46ec-b337-358d30e82339",
   "metadata": {},
   "source": [
    "### Explore Dataset Statistics & Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8010c2b-e810-4c3d-bd64-6653b9f5d63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data type for each column\n",
    "for name, dtype in filtered_df.dtypes:\n",
    "    print(f\"{name}: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139371c8-8fcb-4638-a3ed-56cbf475727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_string_cols = [col_name for col_name, dtype in filtered_df.dtypes if dtype != 'string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10722ce-b5c3-4735-a49d-86032ece9115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset column description dataframe for only non-string\n",
    "non_string_col_des = filtered_col_des.filter(col('column').isin(non_string_cols))\n",
    "non_string_col_des.show(n=non_string_col_des.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac44c139-4bdb-4771-9a9b-1e0be2ea4635",
   "metadata": {},
   "source": [
    "### Discussion on skewed data distributions\n",
    "\n",
    "When taking a look at the columns with the most amount of skew in the data distribution, columns that are ID inidicators or Flight numbers do not make sense to further investigations of data distributions. Although these are numerical values, they represent categorical variables as opposed to continuous. \n",
    "\n",
    "Columns with 'ID','Number', 'Origin', 'Dest' in the column name will be removed from statistical analysis to remove these categorical variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094db735-e065-47b8-b634-2a7b570891af",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_col_des = non_string_col_des.filter(\n",
    "    ~non_string_col_des['column'].rlike('Dest|Origin|ID|Number|FlightDate')\n",
    ")\n",
    "cont_col_des.show(n=cont_col_des.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f447eb1-f02c-4630-b549-e17ba5c22874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get statistics for all continuous variables\n",
    "cont_cols = [row['column'] for row in cont_col_des.select('column').collect()]\n",
    "\n",
    "describe_df = filtered_df.select(cont_cols).describe()\n",
    "\n",
    "# compute Q1, Median, Q3 for each column\n",
    "stats = {\n",
    "    \"25%\": {},\n",
    "    \"50%\": {},\n",
    "    \"75%\": {}\n",
    "}\n",
    "\n",
    "for col_name in cont_cols:\n",
    "    q1, median, q3 = filtered_df.approxQuantile(col_name, [0.25, 0.5, 0.75], 0.01)\n",
    "    stats[\"25%\"][col_name] = str(q1)\n",
    "    stats[\"50%\"][col_name] = str(median)\n",
    "    stats[\"75%\"][col_name] = str(q3)\n",
    "\n",
    "# convert new rows to df rows\n",
    "new_rows = [Row(summary=stat_name, **cols) for stat_name, cols in stats.items()]\n",
    "quartile_df = spark.createDataFrame(new_rows)\n",
    "\n",
    "# append the new rows to describe_df\n",
    "full_summary_df = describe_df.unionByName(quartile_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70530df-be00-4d7f-82c4-f3ae8ef1ddfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df to not have to recalc results later\n",
    "#print(os.getcwd())\n",
    "full_summary_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(\"summary_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac61f4ac-7045-4e93-93d4-0f1c89141920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_summary_df = spark.read.csv('full_summary_df.csv', sep = ',', inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d044b40-76f0-4546-92f9-d5d923193c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_summary_df.select(full_summary_df.columns[:6]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce22dad0-0fda-4167-a939-1e555d30da1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view df columns\n",
    "full_summary_df.select(full_summary_df.columns[11:17]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc35b0f-0d14-42fb-aca4-a1b71fa2c667",
   "metadata": {},
   "source": [
    "### Explore skewed data\n",
    "\n",
    "mean > median, data is right-skewed (longer tail on the right)\n",
    "median < mean, data is left-skewed (longer tail on the left)\n",
    "\n",
    "This code is to find top 20 features with largest skews. These features will then be plotted in histograms\n",
    "\n",
    "The purpose of doing this is to understand if there are any outliers in the dataset that may be worth removing from the dataset prior to applying ML methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b3acce-c99b-40b9-95a0-bde8890a30d0",
   "metadata": {},
   "source": [
    "### Explore data distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56419c4f-626b-4dc2-893f-9040acd940a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mean and median rows as dicts\n",
    "mean_row = full_summary_df.filter(col(\"summary\") == \"mean\").collect()[0].asDict()\n",
    "median_row = full_summary_df.filter(col(\"summary\") == \"50%\").collect()[0].asDict()\n",
    "\n",
    "# skip the 'summary' key\n",
    "cols = [col for col in mean_row.keys() if col != \"summary\"]\n",
    "\n",
    "# build rows of (column, absolute_diff, skew direction)\n",
    "result_rows = []\n",
    "for c in cols: # for each col\n",
    "    mean_val = float(mean_row[c]) # get mean\n",
    "    median_val = float(median_row[c]) # get median\n",
    "    diff = __builtins__.abs(mean_val - median_val) # get abs difference\n",
    "    skew = \"right\" if mean_val > median_val else \"left\" if mean_val < median_val else \"none\" # get skew direction\n",
    "    result_rows.append(Row(column=c, absolute_diff=diff, skew=skew)) # aggregate\n",
    "\n",
    "# create df\n",
    "diff_df = spark.createDataFrame(result_rows)\n",
    "\n",
    "# get top 20\n",
    "top_skewed = diff_df.orderBy(col(\"absolute_diff\").desc()).limit(20)\n",
    "\n",
    "top_skewed.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086edcdb-8a88-49f5-aedc-694077773bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all cols in filtered_df\n",
    "skewed_cols = [row['column'] for row in top_skewed.select('column').collect()]\n",
    "\n",
    "# remove any white space\n",
    "skewed_cols = [str(c).strip() for c in skewed_cols]\n",
    "\n",
    "# subset column description dataframe for only columns in filtered dataset\n",
    "skewed_col_des = col_des.filter(col('column').isin(skewed_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7774476a-fa3d-4d39-b880-c42e17a85f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed_col_des.show(n=skewed_col_des.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cd615b-04cb-4267-bd3f-f12200da0d43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list of columns from 'top_skewed'\n",
    "columns_to_plot = [row['column'] for row in top_skewed.collect()]\n",
    "\n",
    "# filter the columns that exist in filtered_df\n",
    "valid_columns = [col for col in columns_to_plot if col in filtered_df.columns]\n",
    "\n",
    "# plot histograms for each column\n",
    "n_cols = 4  # 4 histograms per row\n",
    "n_rows = (len(valid_columns) + n_cols - 1) // n_cols  # calculate num rows needed\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 5))\n",
    "\n",
    "# flatten axes for easier indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# loop through cols and plot\n",
    "for i, column in enumerate(valid_columns):\n",
    "    hist = filtered_df.select(column).rdd.flatMap(lambda x: x).histogram(20)  # 20 bins\n",
    "    \n",
    "    bin_edges, bin_counts = hist\n",
    "\n",
    "    # plot the histogram using the bin edges and counts\n",
    "    axes[i].bar(bin_edges[:-1], bin_counts, width=(bin_edges[1] - bin_edges[0]), edgecolor='black')\n",
    "\n",
    "    # set axes & title\n",
    "    axes[i].set_title(f\"Histogram of {column}\")\n",
    "    axes[i].set_xlabel('Value')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "# turn off any unused subplots\n",
    "for i in range(len(valid_columns), len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5265a98b-d34a-4bf0-be25-150481e74fd4",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "The distance column, majority of flights in this dataset have a distance <1000 miles. With a few outliers ranging from 3000-5000 miles. \n",
    "\n",
    "Wheels On & Wheels Off time and CRSDepTime & DepTime columns have a few outliers at 0:00-4:00am, majority of times are listed between 5:00 & 23:59\n",
    "\n",
    "The majority of TaxiOut and TaxiIn times are around 0 (or <50minutes). However, there are outliers sitting at ~1300 & 300 minutes respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09a6c97-5cb6-4897-9f92-865985101252",
   "metadata": {},
   "source": [
    "## Which Origin Cities had the most delayed flights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a482027a-fcad-464f-8677-d13cfd74cff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_delay = my_df.select([\"Origin\", \"DepDelay\"]).groupBy(\"Origin\")\\\n",
    "        .agg(count(F.when(col(\"DepDelay\") > 0, 1)).alias(\"DelayCount\"), \n",
    "             count(F.when(col(\"DepDelay\") < 0, 1)).alias(\"EarlyCount\"),\n",
    "            count(\"*\").alias(\"TotalCount\")).orderBy(col(\"TotalCount\").desc())\n",
    "pandas_delay = count_delay.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa229bb-0f91-4f37-9603-e76daf90ed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pandas_delay.copy()\n",
    "pdf[\"OnTimeCount\"] = pdf[\"TotalCount\"] - pdf[\"DelayCount\"] - pdf[\"EarlyCount\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37c32e2-7d91-48c6-a9c3-e01f5310b9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20 = pdf.head(20)\n",
    "top_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4affc1df-26b5-4def-a19b-d8740631b79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_delay = my_df.select([\"Origin\", \"DepDelay\"]).groupBy(\"Origin\")\\\n",
    "        .agg(count(F.when(col(\"DepDelay\") > 0, 1)).alias(\"DelayCount\"), \n",
    "             count(F.when(col(\"DepDelay\") < 0, 1)).alias(\"EarlyCount\"),\n",
    "            count(\"*\").alias(\"TotalCount\")).orderBy(col(\"TotalCount\").desc())\n",
    "pandas_delay = count_delay.toPandas()# Assuming pdf has these columns: OriginCity, DelayedFlights, EarlyFlights, OnTimeFlights\n",
    "\n",
    "# Bar positions\n",
    "cities = top_20[\"Origin\"]\n",
    "x = np.arange(len(cities))\n",
    "\n",
    "# Heights\n",
    "early = top_20[\"EarlyCount\"]\n",
    "on_time = top_20[\"OnTimeCount\"]\n",
    "delayed = top_20[\"DelayCount\"]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x, early, label=\"Early\", color=\"green\")\n",
    "plt.bar(x, on_time, bottom=early, label=\"On Time\", color=\"gray\")\n",
    "plt.bar(x, delayed, bottom=early + on_time, label=\"Delayed\", color=\"red\")\n",
    "\n",
    "# Labels and formatting\n",
    "plt.xticks(x, cities, rotation=45)\n",
    "plt.ylabel(\"Number of Flights\")\n",
    "plt.title(\"Flight Status by Origin City (Top 20)\")\n",
    "plt.legend(title=\"Flight Status\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0cb107-82c8-4ef5-a286-b71f118439fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_delay = my_df.select([\"Year\", \"DepDelay\"]).groupBy(\"Origin\")\\\n",
    "        .agg(count(F.when(col(\"DepDelay\") > 0, 1)).alias(\"DelayCount\"), \n",
    "             count(F.when(col(\"DepDelay\") < 0, 1)).alias(\"EarlyCount\"),\n",
    "            count(\"*\").alias(\"TotalCount\")).orderBy(col(\"TotalCount\").desc())\n",
    "pandas_year_delay = count_delay.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a11f36a-d220-4de2-bffd-3eb1e4d8d252",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep_2 = [\"Airline\", \"Origin\", \"Dest\", \"ArrDelayMinutes\", \"DepDelayMinutes\", \"Distance\", \"OriginCityName\", \"DestCityName\"]\n",
    "df2 = newdf.select(cols_to_keep_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4ea3b3",
   "metadata": {},
   "source": [
    "## Which routes had the most delays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed5ef3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by origin and city, then calculating the total average delay between the cities\n",
    "route_delays = df2.groupBy(\"OriginCityName\", \"DestCityName\") \\\n",
    "    .agg(\n",
    "        (f.avg(\"DepDelayMinutes\") + f.avg(\"ArrDelayMinutes\")).alias(\"AvgTotalDelay\")\n",
    "    ) \\\n",
    "    .orderBy(f.col(\"AvgTotalDelay\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac38afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pandas\n",
    "route_delays_pd = route_delays.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cd4d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining origin and dest for visual purposes \n",
    "route_delays_pd['Route'] = route_delays_pd['OriginCityName'] + ' to ' + route_delays_pd['DestCityName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bc7146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(x='AvgTotalDelay', y='Route', data=route_delays_pd)\n",
    "plt.title('Top 10 Most Delayed Flight Routes')\n",
    "plt.xlabel('Average Total Delay in Minutes')\n",
    "plt.ylabel('Route (Origin to Destination)')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425647c9",
   "metadata": {},
   "source": [
    "## Which airlines experience the most delays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bde894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining departure delay and arrival delay to one column\n",
    "df2 = df2.withColumn('TotalDelay', f.col('DepDelayMinutes') + f.col('ArrDelayMinutes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e21f13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting only delays that are over 0\n",
    "delayed_flights = df2.filter(df2['TotalDelay'] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9704ab78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by airline, then calculating the total average delay\n",
    "total_delay = delayed_flights.groupBy(\"Airline\").agg(\n",
    "    f.avg(\"TotalDelay\").alias(\"TotalDelayMinutes\")\n",
    ").orderBy(f.col(\"TotalDelayMinutes\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b878004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pandas\n",
    "total_delay_pd = total_delay.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d634d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_delay_pd.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b00a6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=\"TotalDelayMinutes\", y=\"Airline\", data=total_delay_pd)\n",
    "plt.title('Top 10 Airlines with the Most Delay in Minutes')\n",
    "plt.xlabel('Average Total Delay in Minutes')\n",
    "plt.ylabel('Airline')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a26667",
   "metadata": {},
   "source": [
    "## Do flights with a longer distance have longer departure delays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf50580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pandas\n",
    "distance_delays_pd = df2.select(\"Distance\", \"DepDelayMinutes\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8484f124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numeric to prevent error\n",
    "distance_delays_pd['Distance'] = pd.to_numeric(distance_delays_pd['Distance'], errors='coerce')\n",
    "distance_delays_pd['DepDelayMinutes'] = pd.to_numeric(distance_delays_pd['DepDelayMinutes'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e3064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing any NaNs and 0 values\n",
    "distance_delays_pd = distance_delays_pd.dropna(subset=['Distance', 'DepDelayMinutes'])\n",
    "distance_delays_pd = distance_delays_pd[(distance_delays_pd['Distance'] > 0) & (distance_delays_pd['DepDelayMinutes'] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b30dd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x='Distance', y='DepDelayMinutes', data= distance_delays_pd, alpha=0.6)\n",
    "plt.title('Flight Distance vs Departure Delay')\n",
    "plt.xlabel('Flight Distance in Miles')\n",
    "plt.ylabel('Departure Delay in Minutes')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
