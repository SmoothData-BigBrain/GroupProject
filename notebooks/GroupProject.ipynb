{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b43e3dd4-fca0-4421-9ce5-d68e001a4cd8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## GitHub ID\n",
    "\n",
    "https://github.com/SmoothData-BigBrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88642e35-73bb-4077-acc6-184563785bc6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Dataset link\n",
    "\n",
    "https://www.kaggle.com/datasets/robikscube/flight-delay-dataset-20182022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d757d7-5d93-4e03-b350-a04fc12a5d31",
   "metadata": {},
   "source": [
    "## Setup for spark and data\n",
    "\n",
    "Perform the data exploration step (i.e. evaluate your data, # of observations, details about your data distributions, scales, missing data, column descriptions) Note: For image data you can still describe your data by the number of classes, # of images, plot example classes of the image, size of images, are sizes uniform? Do they need to be cropped? normalized? etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a03d6c-1a93-4b54-8ecf-596818ee30e4",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271da376-11a6-4fa6-935b-f45b210b763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install everything inside the 'requirements.txt' file before running this notebook\n",
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8cc60b-8fd9-4a6a-b979-c114ba13dd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import util\n",
    "\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import col, isnan, when, count, isnull, sum, concat_ws, coalesce, lit, avg, rand, round\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863f162f-b363-4c17-b226-356b1582e8e0",
   "metadata": {},
   "source": [
    "### Creat Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e41cd7-f31e-46cd-bcf1-138b597e7944",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mihirs machine spark setting\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Flight Data Analysis\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"18g\") \\\n",
    "    .config(\"spark.executor.memory\", \"18g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", \"1000\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feb895f-738d-486f-8d67-4e7a1ff8251f",
   "metadata": {},
   "source": [
    "### Read in data files\n",
    "\n",
    "> **Note: Update the home_dir and download_path variables before running this cell block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c2de02-6753-4c6d-a5cf-d724eba5828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = os.path.expanduser('~')\n",
    "local_download_path = os.path.join(home_dir, 'Desktop/GroupProject/data/')\n",
    "file_id = '1tch7xbFIgBtXKXa16E4QCpVKedUExfO3'  # My File ID for airlines.zip on GDrive \n",
    "util.check_and_fetch_data(file_id, local_download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf04e58-8767-4dd4-ae08-f790b0920c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = glob.glob(f\"{local_download_path}archive/raw/*.csv\")\n",
    "df = spark.read.csv(csv_files,\n",
    "                       sep = ',',\n",
    "                       inferSchema = True,\n",
    "                       header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80859ac5-2131-4275-bd26-2f8bf88e1de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read.parquet(\"combined_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbe9348-4801-4a17-b8a2-a6b3d27b2c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_des = spark.read.csv('flights_column_des.csv', sep = ',', inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52b9cd5-ec4f-4234-ab63-62fcb7d2475f",
   "metadata": {},
   "source": [
    "## Explore Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1dbdd3-0278-4743-b6eb-fd8f912fe390",
   "metadata": {},
   "source": [
    "### Get dataset shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc98c34-1dc1-45ee-921e-cb7c2534331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get df shape\n",
    "num_entries = df.count()\n",
    "num_cols = len(df.columns)\n",
    "print(f\"Shape of the DataFrame: ({num_entries}, {num_cols})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ead44d9-dd3c-44f0-b34b-bd7eee5f9400",
   "metadata": {},
   "source": [
    "### Explore null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892ad7f6-6fca-49b6-a39c-fda420eaf2c7",
   "metadata": {},
   "source": [
    "#### Computing non-null counts as percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60addd02-eed0-42b9-8aea-20ef522d7115",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_null_counts = df.select([count(col(c)).alias(c) for c in df.columns]).collect()[0].asDict()\n",
    "\n",
    "# Calculate non-null percentages\n",
    "non_null_percentages = {\n",
    "    col_name: (count_val / num_entries) * 100\n",
    "    for col_name, count_val in non_null_counts.items()\n",
    "}\n",
    "\n",
    "sorted_columns = sorted(non_null_percentages.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for col_name, pct in sorted_columns:\n",
    "    print(f\"{col_name}: {pct:.2f}% non-null\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2587bb8f-7c94-4206-abbf-6892a1946f81",
   "metadata": {},
   "source": [
    "#### Subset dataset\n",
    "removing columns with <90% null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6026b7-cf26-4ba0-aad3-ed786cca2cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_above_90 = [col_name for col_name, pct in non_null_percentages.items() if pct >= 90]\n",
    "filtered_df = df.select(columns_above_90)\n",
    "filtered_df.select(filtered_df.columns[:8]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c44dab-7784-4664-a50a-39a53641cda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip the extra spaces from col names \n",
    "for c in filtered_df.columns:\n",
    "    filtered_df = filtered_df.withColumnRenamed(c, c.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a120f4-61e3-4e38-ad52-0b21fb368318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filtered df shape\n",
    "filtered_num_rows = filtered_df.count()\n",
    "filtered_num_cols = len(filtered_df.columns)\n",
    "print(f\"Shape of the Filtered DataFrame removing cols w/ <90% non-null values: ({filtered_num_rows}, {filtered_num_cols})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fc3021-969e-4854-8e82-5b93325dcbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc94a931-7d70-460f-845f-ebf903d055a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the master df since we won't need it anymore at all\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ef213e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Discussion on null values\n",
    "Dataset consists of columns with >90% non-null values and then it drops down to 0-17% non-null. Dataset to be used for further exploration will only include columns with >90% non-null values for more robust analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a60fe8-5968-46dc-8b89-6d30f7f97ef6",
   "metadata": {},
   "source": [
    "### Remaining Column Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc302c6-1c8d-474a-8b3a-eedbb2cff3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all cols in filtered_df\n",
    "filtered_cols = filtered_df.columns \n",
    "\n",
    "# remove any white space\n",
    "filtered_cols = [str(c).strip() for c in filtered_cols]\n",
    "\n",
    "# subset column description dataframe for only columns in filtered dataset\n",
    "filtered_col_des = col_des.filter(col('column').isin(filtered_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e0eb34-70f1-4c87-a7d1-575fab89b0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check df was filtered correctly, length & row count should match\n",
    "f_col_len = filtered_col_des.count()\n",
    "f_col_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9b65cc-65c0-458f-8669-f33cc3f960c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View all column descriptions in filtered dataframe\n",
    "# Full data col description is in \"../data/README.md\"\n",
    "filtered_col_des.show(n=f_col_len, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc67605-0fba-46ec-b337-358d30e82339",
   "metadata": {},
   "source": [
    "## Dataset Statistics & Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8010c2b-e810-4c3d-bd64-6653b9f5d63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data type for each column\n",
    "for name, dtype in filtered_df.dtypes:\n",
    "    print(f\"{name}: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139371c8-8fcb-4638-a3ed-56cbf475727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_string_cols = [col_name for col_name, dtype in filtered_df.dtypes if dtype != 'string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10722ce-b5c3-4735-a49d-86032ece9115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset column description dataframe for only non-string\n",
    "non_string_col_des = filtered_col_des.filter(col('column').isin(non_string_cols))\n",
    "non_string_col_des.show(n=non_string_col_des.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac44c139-4bdb-4771-9a9b-1e0be2ea4635",
   "metadata": {},
   "source": [
    "### Discussion on skewed data distributions\n",
    "\n",
    "When taking a look at the columns with the most amount of skew in the data distribution, columns that are ID inidicators or Flight numbers do not make sense to further investigations of data distributions. Although these are numerical values, they represent categorical variables as opposed to continuous. \n",
    "\n",
    "Columns with 'ID','Number', 'Origin', 'Dest' in the column name will be removed from statistical analysis to remove these categorical variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094db735-e065-47b8-b634-2a7b570891af",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_col_des = non_string_col_des.filter(\n",
    "    ~non_string_col_des['column'].rlike('Dest|Origin|ID|Number|FlightDate')\n",
    ")\n",
    "cont_col_des.show(n=cont_col_des.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f447eb1-f02c-4630-b549-e17ba5c22874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get statistics for all continuous variables\n",
    "cont_cols = [row['column'] for row in cont_col_des.select('column').collect()]\n",
    "\n",
    "describe_df = filtered_df.select(cont_cols).describe()\n",
    "\n",
    "# compute Q1, Median, Q3 for each column\n",
    "stats = {\n",
    "    \"25%\": {},\n",
    "    \"50%\": {},\n",
    "    \"75%\": {}\n",
    "}\n",
    "\n",
    "for col_name in cont_cols:\n",
    "    q1, median, q3 = filtered_df.approxQuantile(col_name, [0.25, 0.5, 0.75], 0.01)\n",
    "    stats[\"25%\"][col_name] = str(q1)\n",
    "    stats[\"50%\"][col_name] = str(median)\n",
    "    stats[\"75%\"][col_name] = str(q3)\n",
    "\n",
    "# convert new rows to df rows\n",
    "new_rows = [Row(summary=stat_name, **cols) for stat_name, cols in stats.items()]\n",
    "quartile_df = spark.createDataFrame(new_rows)\n",
    "\n",
    "# append the new rows to describe_df\n",
    "full_summary_df = describe_df.unionByName(quartile_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d044b40-76f0-4546-92f9-d5d923193c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_summary_df.select(full_summary_df.columns[:6]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce22dad0-0fda-4167-a939-1e555d30da1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view df columns\n",
    "full_summary_df.select(full_summary_df.columns[11:17]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc35b0f-0d14-42fb-aca4-a1b71fa2c667",
   "metadata": {},
   "source": [
    "### Explore skewed data\n",
    "\n",
    "mean > median, data is right-skewed (longer tail on the right)\n",
    "median < mean, data is left-skewed (longer tail on the left)\n",
    "\n",
    "This code is to find top 20 features with largest skews. These features will then be plotted in histograms\n",
    "\n",
    "The purpose of doing this is to understand if there are any outliers in the dataset that may be worth removing from the dataset prior to applying ML methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b3acce-c99b-40b9-95a0-bde8890a30d0",
   "metadata": {},
   "source": [
    "### Explore data distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56419c4f-626b-4dc2-893f-9040acd940a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mean and median rows as dicts\n",
    "mean_row = full_summary_df.filter(col(\"summary\") == \"mean\").collect()[0].asDict()\n",
    "median_row = full_summary_df.filter(col(\"summary\") == \"50%\").collect()[0].asDict()\n",
    "\n",
    "# skip the 'summary' key\n",
    "cols = [col for col in mean_row.keys() if col != \"summary\"]\n",
    "\n",
    "# build rows of (column, absolute_diff, skew direction)\n",
    "result_rows = []\n",
    "for c in cols: # for each col\n",
    "    mean_val = float(mean_row[c]) # get mean\n",
    "    median_val = float(median_row[c]) # get median\n",
    "    diff = __builtins__.abs(mean_val - median_val) # get abs difference\n",
    "    skew = \"right\" if mean_val > median_val else \"left\" if mean_val < median_val else \"none\" # get skew direction\n",
    "    result_rows.append(Row(column=c, absolute_diff=diff, skew=skew)) # aggregate\n",
    "\n",
    "# create df\n",
    "diff_df = spark.createDataFrame(result_rows)\n",
    "\n",
    "# get top 20\n",
    "top_skewed = diff_df.orderBy(col(\"absolute_diff\").desc()).limit(20)\n",
    "\n",
    "top_skewed.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086edcdb-8a88-49f5-aedc-694077773bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all cols in filtered_df\n",
    "skewed_cols = [row['column'] for row in top_skewed.select('column').collect()]\n",
    "\n",
    "# remove any white space\n",
    "skewed_cols = [str(c).strip() for c in skewed_cols]\n",
    "\n",
    "# subset column description dataframe for only columns in filtered dataset\n",
    "skewed_col_des = col_des.filter(col('column').isin(skewed_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7774476a-fa3d-4d39-b880-c42e17a85f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed_col_des.show(n=skewed_col_des.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cd615b-04cb-4267-bd3f-f12200da0d43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list of columns from 'top_skewed'\n",
    "columns_to_plot = [row['column'] for row in top_skewed.collect()]\n",
    "\n",
    "# filter the columns that exist in filtered_df\n",
    "valid_columns = [col for col in columns_to_plot if col in filtered_df.columns]\n",
    "\n",
    "# plot histograms for each column\n",
    "n_cols = 4  # 4 histograms per row\n",
    "n_rows = (len(valid_columns) + n_cols - 1) // n_cols  # calculate num rows needed\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 5))\n",
    "\n",
    "# flatten axes for easier indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# loop through cols and plot\n",
    "for i, column in enumerate(valid_columns):\n",
    "    hist = filtered_df.select(column).rdd.flatMap(lambda x: x).histogram(20)  # 20 bins\n",
    "\n",
    "    bin_edges, bin_counts = hist\n",
    "\n",
    "    # plot the histogram using the bin edges and counts\n",
    "    axes[i].bar(bin_edges[:-1], bin_counts, width=(bin_edges[1] - bin_edges[0]), edgecolor='black')\n",
    "\n",
    "    # set axes & title\n",
    "    axes[i].set_title(f\"Histogram of {column}\")\n",
    "    axes[i].set_xlabel('Value')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "# turn off any unused subplots\n",
    "for i in range(len(valid_columns), len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "# need to update to add labels for axis \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5265a98b-d34a-4bf0-be25-150481e74fd4",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "The distance column, majority of flights in this dataset have a distance <1000 miles. With a few outliers ranging from 3000-5000 miles. \n",
    "\n",
    "Wheels On & Wheels Off time and CRSDepTime & DepTime columns have a few outliers at 0:00-4:00am, majority of times are listed between 5:00 & 23:59\n",
    "\n",
    "The majority of TaxiOut and TaxiIn times are around 0 (or <50minutes). However, there are outliers sitting at ~1300 & 300 minutes respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043f7534-aaa3-4a11-b303-4a94c75bf79f",
   "metadata": {},
   "source": [
    "## Questions to analyze data with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09a6c97-5cb6-4897-9f92-865985101252",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Which Origin Cities had the most delayed flights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4affc1df-26b5-4def-a19b-d8740631b79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_delay = filtered_df.select([\"Origin\", \"DepDelay\"]).groupBy(\"Origin\")\\\n",
    "        .agg(count(F.when(col(\"DepDelay\") > 0, 1)).alias(\"DelayCount\"), \n",
    "             count(F.when(col(\"DepDelay\") < 0, 1)).alias(\"EarlyCount\"),\n",
    "            count(\"*\").alias(\"TotalCount\")).orderBy(col(\"TotalCount\").desc())\n",
    "pandas_delay = count_delay.toPandas()# Assuming pdf has these columns: OriginCity, DelayedFlights, EarlyFlights, OnTimeFlights\n",
    "\n",
    "pdf = pandas_delay.copy()\n",
    "pdf[\"OnTimeCount\"] = pdf[\"TotalCount\"] - pdf[\"DelayCount\"] - pdf[\"EarlyCount\"]\n",
    "top_20 = pdf.head(20)\n",
    "\n",
    "# Bar positions\n",
    "cities = top_20[\"Origin\"]\n",
    "x = np.arange(len(cities))\n",
    "\n",
    "# Heights\n",
    "early = top_20[\"EarlyCount\"]\n",
    "on_time = top_20[\"OnTimeCount\"]\n",
    "delayed = top_20[\"DelayCount\"]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x, early, label=\"Early\", color=\"green\")\n",
    "plt.bar(x, on_time, bottom=early, label=\"On Time\", color=\"gray\")\n",
    "plt.bar(x, delayed, bottom=early + on_time, label=\"Delayed\", color=\"red\")\n",
    "\n",
    "# Labels and formatting\n",
    "plt.xticks(x, cities, rotation=45)\n",
    "plt.ylabel(\"Number of Flights\")\n",
    "plt.title(\"Flight Status by Origin City (Top 20)\")\n",
    "plt.legend(title=\"Flight Status\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c41dc78-d0bc-4d48-ac2e-9629dac7ff1c",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "The plot above shows a stacked bar chart of the origin cities with the 20 highest total flight counts. The bars are stacked according to early departures (green), on time departures (grey), and delayed departures (red) and are organized in descending order starting at the left. From this plot, the overall trend suggests that the majority of flights are early and only a small proportion of flights are actually on time. The origin city with the seemingly largest proportion of delayed departures is Denver and, speaking as someone from Colorado, I can personally attest to this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f3f67a-f47e-48dd-90d9-b25cf3beebf6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Which years had the most delayed flights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0cb107-82c8-4ef5-a286-b71f118439fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_delay = filtered_df.select([\"Year\", \"DepDelay\"]).groupBy(\"Year\")\\\n",
    "        .agg(count(F.when(col(\"DepDelay\") > 0, 1)).alias(\"DelayCount\"), \n",
    "             count(F.when(col(\"DepDelay\") < 0, 1)).alias(\"EarlyCount\"),\n",
    "            count(\"*\").alias(\"TotalCount\")).orderBy(col(\"Year\"))\n",
    "pandas_year_delay = year_delay.toPandas()\n",
    "\n",
    "pydf = pandas_year_delay.copy()\n",
    "pydf[\"OnTimeCount\"] = pydf[\"TotalCount\"] - pydf[\"DelayCount\"] - pydf[\"EarlyCount\"]\n",
    "year_counts = pydf\n",
    "\n",
    "# Bar positions\n",
    "years = year_counts[\"Year\"]\n",
    "x = np.arange(len(years))\n",
    "\n",
    "# Heights\n",
    "early = year_counts[\"EarlyCount\"]\n",
    "on_time = year_counts[\"OnTimeCount\"]\n",
    "delayed = year_counts[\"DelayCount\"]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x, early, label=\"Early\", color=\"green\")\n",
    "plt.bar(x, on_time, bottom=early, label=\"On Time\", color=\"gray\")\n",
    "plt.bar(x, delayed, bottom=early + on_time, label=\"Delayed\", color=\"red\")\n",
    "\n",
    "# Labels and formatting\n",
    "plt.xticks(x, years)\n",
    "plt.ylabel(\"Number of Flights\")\n",
    "plt.title(\"Flight Status by Year\")\n",
    "plt.legend(title=\"Flight Status\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8a5456-6f33-4b29-bde5-f9a7a859606d",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "Similar to the first bar chart, we also present a stacked bar chart depicting the overall flight count per year between 2018 and 2022. The scale of this plot is in the millions of flights and 2019 appears to have a much higher overall flight count than the other years. Surprisingly, 2020 had the fewest amount of delayed departures by far even though it had a similar amount of overall flights. This could very well have something to do with the emergence of COVID during the beginning of that year, but it would be interesting to look closer at flight trends during that time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789371f0-c9c8-41e5-af2d-81077ffaaa9f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Which months had the most delayed flights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a11f36a-d220-4de2-bffd-3eb1e4d8d252",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "month_delay = filtered_df.select([\"Month\", \"DepDelay\"]).groupBy(\"Month\")\\\n",
    "        .agg(count(F.when(col(\"DepDelay\") > 0, 1)).alias(\"DelayCount\"), \n",
    "             count(F.when(col(\"DepDelay\") < 0, 1)).alias(\"EarlyCount\"),\n",
    "            count(\"*\").alias(\"TotalCount\")).orderBy(col(\"Month\"))\n",
    "pandas_month_delay = month_delay.toPandas()\n",
    "\n",
    "pmdf = pandas_month_delay.copy()\n",
    "pmdf[\"OnTimeCount\"] = pmdf[\"TotalCount\"] - pmdf[\"DelayCount\"] - pmdf[\"EarlyCount\"]\n",
    "month_counts = pmdf\n",
    "\n",
    "# Bar positions\n",
    "months = month_counts[\"Month\"]\n",
    "x = np.arange(len(months))\n",
    "\n",
    "# Heights\n",
    "early = month_counts[\"EarlyCount\"]\n",
    "on_time = month_counts[\"OnTimeCount\"]\n",
    "delayed = month_counts[\"DelayCount\"]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x, early, label=\"Early\", color=\"green\")\n",
    "plt.bar(x, on_time, bottom=early, label=\"On Time\", color=\"gray\")\n",
    "plt.bar(x, delayed, bottom=early + on_time, label=\"Delayed\", color=\"red\")\n",
    "\n",
    "# Labels and formatting\n",
    "plt.xticks(x, months)\n",
    "plt.ylabel(\"Number of Flights\")\n",
    "plt.title(\"Flight Status by Month\")\n",
    "plt.legend(title=\"Flight Status\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6ed815-f3a6-4501-894b-112627c95bc2",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "Continuing with the bar charts, the above chart depicts departure status as proportions of the total number of flights per month starting with January at 1. Intuitively, one might expect there to be more delayed flights during the winter months December-March. However, this graph depicts that there is really no discernible difference in delayed departures during that time, with the largest proportion of delayed flights actually coming in June. Keep in mind this is only looking at departures, so other statuses could have different outcomes, but it is interesting to note."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4ea3b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Which routes had the most delays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4272a44c-1062-4c86-9576-ac7e3551cc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_delay = filtered_df.select([\"Year\", \"DepDelay\"]).groupBy(\"Year\")\\\n",
    "        .agg(count(F.when(col(\"DepDelay\") > 0, 1)).alias(\"DelayCount\"), \n",
    "             count(F.when(col(\"DepDelay\") < 0, 1)).alias(\"EarlyCount\"),\n",
    "            count(\"*\").alias(\"TotalCount\")).orderBy(col(\"TotalCount\").desc())\n",
    "pandas_year_delay = count_delay.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2d5a7e-528f-4f36-8740-1d3c8600f2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep_2 = [\"Operating_Airline\", \"Origin\", \"Dest\", \"ArrDelayMinutes\", \"DepDelayMinutes\", \"Distance\", \"OriginCityName\", \"DestCityName\"]\n",
    "df2 = filtered_df.select(cols_to_keep_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed5ef3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by origin and city, then calculating the total average delay between the cities\n",
    "route_delays = df2.groupBy(\"OriginCityName\", \"DestCityName\") \\\n",
    "    .agg(\n",
    "        (F.avg(\"DepDelayMinutes\") + F.avg(\"ArrDelayMinutes\")).alias(\"AvgTotalDelay\")\n",
    "    ) \\\n",
    "    .orderBy(F.col(\"AvgTotalDelay\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac38afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pandas\n",
    "route_delays_pd = route_delays.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cd5e53-8fdb-48ce-8d14-6406a21ce58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "route_delays_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cd4d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining origin and dest for visual purposes \n",
    "route_delays_pd['Route'] = route_delays_pd['OriginCityName'] + ' to ' + route_delays_pd['DestCityName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bc7146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(x='AvgTotalDelay', y='Route', data=route_delays_pd)\n",
    "plt.title('Top 10 Most Delayed Flight Routes')\n",
    "plt.xlabel('Average Total Delay in Minutes')\n",
    "plt.ylabel('Route (Origin to Destination)')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ab4a04-a76d-4d2f-8f79-c55edc4c17aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Discussion\n",
    "This bar plot shows the top 10 most delayed flight routes ranked by the average total delay, which is the combined sum of both departure and arrival delays. The x-axis represents the total average delay in minutes, while the y-axis displays the origin and destination cities. We note that the route with the most significant delay, Bend/Redmond, OR to Medford, OR, occurs within the same state, with an average total delay of around 2200 minutes or 36 hours which is significantly higher than the other routes. Additionally, most of the other delayed flights seem to occur when the flights are approximately halfway across the country. Further analysis could explore how the amount of delayed flights on each route correlates with the average total delay."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425647c9",
   "metadata": {},
   "source": [
    "### Which airlines experience the most delays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bde894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining departure delay and arrival delay to one column\n",
    "df2 = df2.withColumn('TotalDelay', F.col('DepDelayMinutes') + F.col('ArrDelayMinutes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856c9c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming airline codes to their respective names\n",
    "airline_mapping = {\n",
    "    'AX': 'Trans States Airlines',\n",
    "    'C5': 'Commutair/Champlain Enterprises Inc.',\n",
    "    'G7': 'GoJet Airlines/United Express',\n",
    "    'ZW': 'Air Wisconsin Airlines Corp',\n",
    "    'EV': 'ExpressJet Airlines inc.',\n",
    "    'B6': 'JetBlue Airways',\n",
    "    'YV': 'Mesa Airlines Inc.',\n",
    "    'OO': 'Skywest Airlines Inc',\n",
    "    'F9': 'Frontier Airlines Inc',\n",
    "    'G4': 'Allegiant Air'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e21f13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting only delays that are over 0\n",
    "delayed_flights = df2.filter(df2['TotalDelay'] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0990ab41-5067-4225-975c-c903ddf19505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delayed_flights.printSchema()\n",
    "# random change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9704ab78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by airline, then calculating the total average delay\n",
    "total_delay = delayed_flights.groupBy(\"Operating_Airline\").agg(\n",
    "    F.avg(\"TotalDelay\").alias(\"TotalDelayMinutes\")\n",
    ").orderBy(F.col(\"TotalDelayMinutes\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b878004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pandas\n",
    "total_delay_pd = total_delay.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a2164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_delay_pd['Operating_Airline_Name'] = total_delay_pd['Operating_Airline'].map(airline_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d634d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_delay_pd.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b00a6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=\"TotalDelayMinutes\", y=\"Operating_Airline\", data=total_delay_pd)\n",
    "plt.title('Top 10 Airlines with the Most Delay in Minutes')\n",
    "plt.xlabel('Average Total Delay in Minutes')\n",
    "plt.ylabel('Airline')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cabfc8-4de4-443e-bac0-8580f478ede2",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "This bar plot illustrates the top 10 airlines with the highest average total delay, measured in minutes. The x-axis represents the total average delay, while the y-axis lists the airlines. It’s clear from the plot that certain airlines, such as Trans States Airlines and Commutair, experience somewhat higher delays than others with their total delays averaging over 100 minutes or a little over 1.5 hours. Further analysis could focus on the specific factors contributing to delays within these airlines, such as location, weather, or operational challenges. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a26667",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Do flights with a longer distance have longer departure delays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf50580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pandas\n",
    "distance_delays_sample_pd = df2.select(\"Distance\", \"DepDelayMinutes\").sample(withReplacement=False, fraction=0.3, seed=42).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8484f124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numeric to prevent error\n",
    "distance_delays_sample_pd['Distance'] = pd.to_numeric(distance_delays_sample_pd['Distance'], errors='coerce')\n",
    "distance_delays_sample_pd['DepDelayMinutes'] = pd.to_numeric(distance_delays_sample_pd['DepDelayMinutes'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e3064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing any NaNs and 0 values\n",
    "distance_delays_sample_pd = distance_delays_sample_pd.dropna(subset=['Distance', 'DepDelayMinutes'])\n",
    "distance_delays_sample_pd = distance_delays_sample_pd[(distance_delays_sample_pd['Distance'] > 0) & (distance_delays_sample_pd['DepDelayMinutes'] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b30dd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x='Distance', y='DepDelayMinutes', data= distance_delays_sample_pd, alpha=0.6)\n",
    "plt.title('Flight Distance vs Departure Delay')\n",
    "plt.xlabel('Flight Distance in Miles')\n",
    "plt.ylabel('Departure Delay in Minutes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96cc4ea-ea94-41c6-954c-03499416a6ac",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "This scatter plot compares flight distance and departure delay, with flight distance being on the x-axis and departure delay on the y-axis. It is only a sample of the whole dataset, and shows that there’s no clear correlation between the distance and delay. However, we can somewhat see that there is a slightly negative correlation where shorter flights have longer delays and longer flights have shorter delays. Further analysis could explore how specific factors contribute to delays, especially for shorter flights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9c5d09-ad6a-468f-851b-b45f143e261a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Checking delayed and cancelled flights Group By by operating airlines and time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c992ae14-976c-44a0-bed8-1001fa18301c",
   "metadata": {},
   "source": [
    "The following columns were manually chosen after reading column description on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ecfd8d-6ef2-4f25-8d2d-8907059bc2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_check = ['Marketing_Airline_Network', 'Operating_Airline', 'Origin', 'Dest', 'DepDel15', 'DepDelay', 'ArrDel15', 'ArrDelay',\\\n",
    "                 'Cancelled', 'CarrierDelay', 'WeatherDelay', 'NASDelay', 'SecurityDelay', 'LateAircraftDelay',\\\n",
    "                 'Distance', 'Year', 'Month', 'DayofMonth', 'FlightDate', 'Flight_Number_Operating_Airline']\n",
    "print(f'col_to_check = {len(cols_to_check)}')\n",
    "# Check to see if interested columns is in columns_with_few_nulls\n",
    "removed_col = []\n",
    "my_cols = []\n",
    "for c in cols_to_check:\n",
    "    if c in filtered_df.columns:\n",
    "        my_cols.append(c)\n",
    "    else:\n",
    "        removed_col.append(c)\n",
    "\n",
    "print('Chosen columns:')\n",
    "print(my_cols)\n",
    "print()\n",
    "print('Rejected columns:')\n",
    "print(removed_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277c08e3-227b-485a-935f-83b889b504e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out a subset of columns to do data analysis\n",
    "Nam_df = filtered_df.select(my_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf8401c-ac80-43e5-a73a-f4c97b1e3890",
   "metadata": {},
   "source": [
    "Cancelled flights have nulls values in other columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76698145-d6d0-4ac0-b36d-ab7aa82e4752",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_check = ['DepDel15', 'DepDelay', 'ArrDel15', 'ArrDelay']\n",
    "print('There are entries where Cancelled column will have value of 1 (canceled) while other columns might be null')\n",
    "for c in cols_to_check:\n",
    "    num_to_display = Nam_df.where(isnull(col(c)) & (col('Cancelled') == 1)).count()\n",
    "    print(f'Number of entries where {c} column is null but Cancelled is 1: {num_to_display}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3ab495-d2f2-490d-aa05-ef7cf4f2885d",
   "metadata": {},
   "source": [
    "Since I intend to do data exploration with cancelled flights, it is not a good idea for me to do dropna() on the dataset as I will lose data for those flights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f074bb8f-64f3-43fe-9f5a-63e85e6868a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of flights that are delayed by more 15 minutes and cancelled flights\n",
    "num_delayed = Nam_df.where(col('DepDel15') == 1).count()\n",
    "num_cancelled = Nam_df.where(col('Cancelled') == 1).count()\n",
    "print(f'Number of flights that were delayed by more than 15 minutes: {num_delayed}')\n",
    "print(f'Number of flights that were cancelled: {num_cancelled}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bd7e5d-894a-4f18-a440-acefbb3d8785",
   "metadata": {},
   "source": [
    "#### Group by operating airline data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb0bf2-2622-4ed9-a0e3-7b7260c2c614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate by operating airline and turn into Pandas dataframe\n",
    "airline_flights = Nam_df.groupBy(col('Operating_Airline')).agg(F.sum(col('DepDel15')).alias('DelayedFlights'),\n",
    "                                                               F.sum(col('Cancelled')).alias('CancelledFlights'),\n",
    "                                                               F.count('*').alias('TotalFlights'))\n",
    "airline_flights_pd = airline_flights.toPandas()\n",
    "airline_flights_pd['DelayedPercentage'] = airline_flights_pd['DelayedFlights'] / airline_flights_pd['TotalFlights']\n",
    "airline_flights_pd['CancelledPercentage'] = airline_flights_pd['CancelledFlights'] / airline_flights_pd['TotalFlights']\n",
    "airline_flights_pd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d1961e-4871-4b2c-bc6c-45738307cd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph number of delayed flights by airlines\n",
    "airline_flights_pd = airline_flights_pd.sort_values(by = 'DelayedFlights', axis = 0, ascending = True)\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "percentage_barplot = sns.barplot(data = airline_flights_pd, x = 'Operating_Airline', y = 'DelayedFlights')\n",
    "plt.title('Number of delayed flights by Operating Airline')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a017b96b-7e89-4928-bd7d-cb5ab53eadd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph delay flight percentage by airline\n",
    "airline_flights_pd = airline_flights_pd.sort_values(by = 'DelayedPercentage', axis = 0, ascending = True)\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "percentage_barplot = sns.barplot(data = airline_flights_pd, x = 'Operating_Airline', y = 'DelayedPercentage')\n",
    "plt.title('Percentage of delayed flights by Operating Airline')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc87dcf-acea-4248-991f-4ac13f32ff8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph number of cancelled flights by airlines\n",
    "airline_flights_pd = airline_flights_pd.sort_values(by = 'CancelledFlights', axis = 0, ascending = True)\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.barplot(data = airline_flights_pd, x = 'Operating_Airline', y = 'CancelledFlights')\n",
    "plt.title('Number of cancelled flights by Operating Airline')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b9acda-ca0a-4a91-9157-a26481115591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph peracentage of cancelled flights by airlines\n",
    "airline_flights_pd = airline_flights_pd.sort_values(by = 'CancelledPercentage', axis = 0, ascending = True)\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.barplot(data = airline_flights_pd, x = 'Operating_Airline', y = 'CancelledPercentage')\n",
    "plt.title('Percentage of cancelled flights by Operating Airline')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21105ab1-d942-49fb-9a8c-fea16cd91449",
   "metadata": {},
   "source": [
    "#### Group by time data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713a8ee0-a6b3-4b6c-a223-e381b543721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Year and Month and turn into Pandas df\n",
    "time_agg = Nam_df.groupBy('Year', 'Month').agg(F.sum(col('DepDel15')).alias('DelayedFlights'),\n",
    "                                              F.sum(col('Cancelled')).alias('CancelledFlights'),\n",
    "                                              F.count('*').alias('TotalFlights'))\n",
    "time_agg_pd = time_agg.toPandas()\n",
    "time_agg_pd['DelayedPercentage'] = time_agg_pd['DelayedFlights'] / time_agg_pd['TotalFlights']\n",
    "time_agg_pd['CancelledPercentage'] = time_agg_pd['CancelledFlights'] / time_agg_pd['TotalFlights']\n",
    "time_agg_pd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793de5b8-9350-4663-b46e-87cc74471a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph number of flights overtime\n",
    "colors = ['red', 'blue', 'green', 'purple', 'gold']\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "barplot = sns.lineplot(data = time_agg_pd, x = 'Month', y = 'TotalFlights', hue = 'Year', palette = colors)\n",
    "plt.title('Number of monthly flights over the year')\n",
    "plt.ylabel('')\n",
    "plt.legend(loc = 'upper right', bbox_to_anchor = (1.15, 1), title = 'Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e991c337-a0a3-4453-86b9-ff0023d9138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph percentage of delayed flights over time\n",
    "colors = ['red', 'blue', 'green', 'purple', 'gold']\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "barplot = sns.lineplot(data = time_agg_pd, x = 'Month', y = 'DelayedPercentage', hue = 'Year', palette = colors)\n",
    "plt.title('Percentage of delayed monthly flights over the year')\n",
    "plt.ylabel('')\n",
    "plt.legend(loc = 'upper right', bbox_to_anchor = (1.15, 1), title = 'Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563e8562-992e-41eb-ad62-7dcda34cd125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph percentage of cancelled flights over time\n",
    "colors = ['red', 'blue', 'green', 'purple', 'gold']\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "barplot = sns.lineplot(data = time_agg_pd, x = 'Month', y = 'CancelledPercentage', hue = 'Year', palette = colors)\n",
    "plt.title('Percentage of cancelled monthly flights over the year')\n",
    "plt.ylabel('')\n",
    "plt.legend(loc = 'upper right', bbox_to_anchor = (1.15, 1), title = 'Year')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0a6eee-1b8a-4c02-adf2-24f6b931ef7c",
   "metadata": {},
   "source": [
    "#### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e85723b-87bb-4c69-b23b-fe5463d19723",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**<u>Data cleaning</u>**\n",
    "\n",
    "Thanks to the team's work, we can learn that the majority of columns in our dataset contains a large number of NaN. We are able to quickly filter out those columns and focus on the others.\n",
    "My analysis focused on delayed and cancelled flights. I learned that entries of cancelled flights will have nulls in other columns making a simple dropna() not a viable data cleaning method.\n",
    "If we are to work with cancelled flights, we have to find away to fill in the nulls of other columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50186cea-70e9-497f-8c18-2f8f67213c5a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**<u>Aggregated by operating airlines</u>**\n",
    "\n",
    "Despite how miserable air travelling is, flights are seldomly late. About 10 - 25% of flights are at least 15 minutes late to depart. From the graph, I would say that an average of about 15% of flights operated by any airlines are late to depart.\n",
    "\n",
    "Airlines are keen to keep their flights operational cancelling less than 5% of their total scheduled flights. This makes sense as cancellation results in not only loss of revenue but also compensation of damages and potential loss of opportunities.\n",
    "\n",
    "Airline denoted by KS (Peninsula Airways) stood out to me. They do not serve a lot of flights. But their delayed and cancelled metrics are area of improvement to say the least."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0311efc1-0aaf-4e47-a987-127511ebc4dc",
   "metadata": {},
   "source": [
    "**<u>Aggregated by time</u>**\n",
    "\n",
    "Acknowledgement: I understand that the airline industry was heavy affected by COVID-19 and that the industry is recovering to pre-pandemic numbers.\n",
    "\n",
    "Overall, it seems that February tends to be a slow month for the airline industry. The summer months are busy. The holiday months of September - December are only slight less busy than the summer months. People are more likely to travel in the second half of the year.\n",
    "\n",
    "The data for 2018 flights stood out to me. The year started with fewer flights than 2021 and 2022 (post COVID-19 years) but then suddenly gained 300,000 flights for the summer months. This indicates to me that there is a potential socio/economical/geopolitical event happening and/or issue with data collection.\n",
    "\n",
    "Delayed flights are likely to happen during peak of the traveling seasons contributing to the misery of air travelling. February is an interesting month as customers are not travelling but flights pick up an increase in chance of being delayed. My guess is that flight crews and ground crews are burned out from the holiday season and their performance is decreased.\n",
    "\n",
    "As for cancelled flights, there is a gigantic mountain that sits in the middle of the graph. Almost half of scheduled flights for April of 2020 are cancelled. I wonder what happened."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7c98f1-497e-4124-8493-540c33c10671",
   "metadata": {},
   "source": [
    "# Group Project Milestone 3: Data PreProcessing & First Model\n",
    "\n",
    "In this assignment you will need to:\n",
    "\n",
    "1. Finish major preprocessing, this includes scaling and/or transforming your data, imputing your data, encoding your data, feature expansion, Feature expansion (example is taking features and generating new features by transforming via polynomial, log multiplication of features).\n",
    "\n",
    "2. Train your first model\n",
    "\n",
    "3. Evaluate your model compare training vs test error\n",
    "\n",
    "4. Where does your model fit in the fitting graph.\n",
    "\n",
    "5. What are the next models you are thinking of and why?\n",
    "\n",
    "6. Update your README.md to include your new work and updates you have all added. Make sure to upload all code and notebooks. Provide links in your README.md\n",
    "\n",
    "7. Conclusion section: What is the conclusion of your 1st model? What can be done to possibly improve it?\n",
    "\n",
    "Note: For supervised learning, include example ground truth and predictions for train, validation, and test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce7e796-2f89-453c-969e-e445a6853859",
   "metadata": {},
   "source": [
    "## 1. PreProcessing Finalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2e9440-bd90-4797-b2bb-ddc82381ce04",
   "metadata": {},
   "source": [
    "### Removal of Redundant Features\n",
    "\n",
    "Since we have Year, Quarter, Month, DayofMonth, and DayofWeek columns, we found it unnecessary to keep the FlightDate column and therefore removed it. Additionally, we found that the columns pertaining to airline and flight codes/identifiers were not relevant for the random forest model that we built. Similarly, we decided to remove airport or city identifiers such as AirportSeqID and Origin/DestWac as we found that they were also irrelevant for analysis. Lastly, we decided to remove miscellaneous columns and columns we found to be highly related to each other such as ArrDelayMinutes and ArrDelay15 as we decided to focus on ArrivalDelayGroups and DepartureDelayGroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baba2493-3e1b-4da8-ab05-5c7b6c82b616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of redundant columns\n",
    "redundant_cols = [\n",
    "    'FlightDate',\n",
    "    'Marketing_Airline_Network',\n",
    "    'Operated_or_Branded_Code_Share_Partners',\n",
    "    'DOT_ID_Marketing_Airline',\n",
    "    'IATA_Code_Marketing_Airline',\n",
    "    'Flight_Number_Marketing_Airline',\n",
    "    'Originally_Scheduled_Code_Share_Airline',\n",
    "    'DOT_ID_Originally_Scheduled_Code_Share_Airline',\n",
    "    'IATA_Code_Originally_Scheduled_Code_Share_Airline',\n",
    "    'Flight_Num_Originally_Scheduled_Code_Share_Airline',\n",
    "    'Flight_Number_Operating_Airline',\n",
    "    'Tail_Number',\n",
    "    'OriginAirportSeqID',\n",
    "    'DestAirportSeqID',\n",
    "    'OriginCityMarketID',\n",
    "    'DestCityMarketID',\n",
    "    'OriginWac',\n",
    "    'DestWac',\n",
    "    'OriginStateFips',\n",
    "    'DestStateFips',\n",
    "    'DepDelay',\n",
    "    'DepartureDelayGroups',\n",
    "    'DepDelayMinutes',\n",
    "    'DepDel15',\n",
    "    'DepTimeBlk',\n",
    "    'ArrDelay',\n",
    "    'ArrDelayMinutes',\n",
    "    'ArrDel15',\n",
    "    'ArrTimeBlk',\n",
    "    'Duplicate',\n",
    "    '_c119'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd27273-4608-41d0-b1d7-6ca8914cad1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removal of redundant columns from original df\n",
    "filtered_df = filtered_df.drop(*redundant_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56784b5a-57a9-4821-846b-820176b005fb",
   "metadata": {},
   "source": [
    "### Handling Missing Data\n",
    "\n",
    "- Columns with >10% of null values are excluded from analysis\n",
    "- Columns that pass this criteria but still have null values range from 1-3% of null values\n",
    "- Most columns that remain in the dataset have no null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a699adc7-c690-486a-8036-2c4e76844386",
   "metadata": {},
   "source": [
    "#### Columns with null values present\n",
    "- Tail_Number: 99.08% non-null\n",
    "- DepTime: 97.39% non-null\n",
    "- DepDelay: 97.39% non-null\n",
    "- DepDelayMinutes: 97.39% non-null\n",
    "- DepDel15: 97.39% non-null\n",
    "- DepartureDelayGroups: 97.39% non-null\n",
    "- WheelsOff: 97.33% non-null\n",
    "- TaxiOut: 97.33% non-null\n",
    "- ArrTime: 97.31% non-null\n",
    "- WheelsOn: 97.28% non-null\n",
    "- TaxiIn: 97.28% non-null\n",
    "- ActualElapsedTime: 97.10% non-null\n",
    "- ArrDelay: 97.10% non-null\n",
    "- ArrDelayMinutes: 97.10% non-null\n",
    "- ArrDel15: 97.10% non-null\n",
    "- ArrivalDelayGroups: 97.10% non-null\n",
    "- AirTime: 97.08% non-null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdd0bd2-5487-43a3-812f-46a9b482647f",
   "metadata": {},
   "source": [
    "#### Missing values are likely due to cancelled flights. Check if this is true. How do we want to handle missing data where the flight was cancelled?\n",
    "\n",
    "- Check if NA values fall under columns where df['Cancelled'] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11adc1a3-01d0-456f-8a4b-06c2eba3e6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if any value in col is Null\n",
    "# use reduce lambda to cobine expressions, if any col in instance is Null -> True\n",
    "na_condition = reduce(lambda a,b: a|b, (col(c).isNull() for c in filtered_df.columns))\n",
    "\n",
    "filtered_df.filter(na_condition).select(\"Cancelled\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222848a4-e652-4851-b57d-321fa622af19",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "Cancelled flights (Cancelled = 1) will be removed from analysis as the predictive model will be used to predict flight delay durations. Cancelled flights are not applicable to this analysis\n",
    "\n",
    "Any remaining instances with NA values will undergo data importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fd0d1f-e92b-47a9-ad9b-ab96d52122db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove cancelled flights\n",
    "filtered_df_not_cancelled = filtered_df.filter(col(\"Cancelled\") != 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ba4447-6780-46b5-8c9d-a1c7ed6ab715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find cols with nulls\n",
    "cols_with_na = [c for c in filtered_df_not_cancelled.columns\n",
    "                if filtered_df_not_cancelled.filter(col(c).isNull()).limit(1).count() >0]\n",
    "\n",
    "dtypes_with_na = {field.name: field.dataType.simpleString()\n",
    "                  for field in filtered_df_not_cancelled.schema.fields\n",
    "                  if field.name in cols_with_na}\n",
    "\n",
    "for col_name, dtype in dtypes_with_na.items():\n",
    "    print(f\"{col_name}: {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c586f4b5-0aa5-47ac-807f-9003478c8480",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "Tail Numbers may occur several times throughout the dataset. Imputation will be done on this categorical variable as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dac1fa4-9ef9-4951-80cb-2342a8f5b5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of expressions counting nulls per column per row\n",
    "null_exprs = [when(col(c).isNull(), 1).otherwise(0) for c in filtered_df_not_cancelled.columns]\n",
    "num_nulls_expr = reduce(lambda a, b: a + b, null_exprs)\n",
    "\n",
    "# select single column in a new df\n",
    "na_counts = filtered_df_not_cancelled.select(num_nulls_expr.alias(\"num_nulls\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1865976-3213-43b1-929e-c9dfa4c13ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count rows by number of nulls\n",
    "na_distribution = na_counts.groupBy(\"num_nulls\").count().orderBy(\"num_nulls\")\n",
    "na_distribution.show()\n",
    "\n",
    "# calculate totals and percentages\n",
    "total_rows = filtered_df_not_cancelled.count()\n",
    "rows_with_na = na_counts.filter(col(\"num_nulls\") > 0).count()\n",
    "percentage_with_na = (rows_with_na / total_rows) * 100\n",
    "\n",
    "print(f\"Rows with at least one NA: {rows_with_na} / {total_rows} ({percentage_with_na:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f268ec-1ce3-4c73-a297-f14c7e8a98b9",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "Only 0.27% of data entries contain NA values in them. The majority of entries with at least one NA have 6 NA values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb02603b-1261-4cca-bb12-173b576efb2f",
   "metadata": {},
   "source": [
    "#### Missing Data Imputation on Numeric Columns\n",
    "\n",
    "This code is inspired by MissForest, which uses Random Forest Classifier to leverage all features in the dataset to predict what the NA value might be. \n",
    "\n",
    "MissForest is not directly usable in pyspark. This code is a workaround to impute missing data in a MissForest - inspired manner\n",
    "\n",
    "**As only 0.27% of entries have missing data, they are simply removed from analysis for now until an appropriate data imputation strategy is successfully implemented for numeric and categorical variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238cbd6f-93b0-486d-a32a-31ca6a4e10c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df_not_cancelled.na.drop()\n",
    "filtered_df = filtered_df.drop(\"Cancelled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d498df-16f6-4c4f-afa8-fc78e11dc8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "del filtered_df_not_cancelled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffc6d82-f367-4074-bc81-f8797c6d832c",
   "metadata": {},
   "source": [
    "### Feature Expansion Ideas\n",
    "\n",
    "- Combine Origin & Dest into one feature (Route)\n",
    "- Ratio features AirTime/Distance\n",
    "- Log transformation of skewed numeric features - recommended 'Distance' had a >3 fold increase in skew magnitude out of top 20 skewed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e836670b-3985-42a8-82ee-cf694cf6ccc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eed1055-97e9-467b-977a-6761f7065450",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.drop(\"is_popular_route\")\n",
    "filtered_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35168191-9127-4a9f-b7a7-8410a3c7b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# ─── Add \"route\" Column ─────────────────────────────────────────────\n",
    "# Format: \"Origin - Dest\", replacing nulls with \"Unknown\"\n",
    "filtered_df = filtered_df.withColumn(\n",
    "    \"route\",\n",
    "    F.concat_ws(\n",
    "        \" - \",\n",
    "        F.coalesce(F.col(\"Origin\"), F.lit(\"Unknown\")),\n",
    "        F.coalesce(F.col(\"Dest\"), F.lit(\"Unknown\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "# ─── Add Average Speed (mph) ─────────────────────────────────────────\n",
    "# Formula: (Distance / AirTime) * 60, rounded to nearest integer\n",
    "filtered_df = filtered_df.withColumn(\n",
    "    \"avg_speed_mph\",\n",
    "    F.when(\n",
    "        F.col(\"AirTime\").isNotNull() & (F.col(\"AirTime\") != 0),\n",
    "        F.round((F.col(\"Distance\") / F.col(\"AirTime\")) * 60)\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "# ─── Add num_flights Using Window Function ───────────────────────────\n",
    "# Counts how many times each route appears, without using join\n",
    "route_window = Window.partitionBy(\"route\")\n",
    "filtered_df = filtered_df.withColumn(\n",
    "    \"num_flights\",\n",
    "    F.count(\"*\").over(route_window)\n",
    ")\n",
    "\n",
    "# ─── Monthly Route Count (for EDA only, optional) ────────────────────\n",
    "monthly_counts = filtered_df.groupBy(\"route\", \"Month\").count()\n",
    "\n",
    "# ─── Add route_popularity Bucket ─────────────────────────────────────\n",
    "# 0 = low (<5000), 1 = med (5K–15K), 2 = high (>15K)\n",
    "filtered_df = filtered_df.withColumn(\n",
    "    \"route_popularity\",\n",
    "    F.when(F.col(\"num_flights\") < 5000, 0)\n",
    "     .when(F.col(\"num_flights\") < 15000, 1)\n",
    "     .otherwise(2)\n",
    ")\n",
    "\n",
    "# ─── Preview Random Sample of Final Feature Columns ──────────────────\n",
    "sample_cols = [\"route\", \"avg_speed_mph\", \"num_flights\", \"route_popularity\"]\n",
    "filtered_df.select(sample_cols) \\\n",
    "    .orderBy(F.rand(seed=42)) \\\n",
    "    .limit(20) \\\n",
    "    .show(truncate=False)\n",
    "\n",
    "# ─── DataFrame Shape Summary ─────────────────────────────────────────\n",
    "num_rows = filtered_df.count()\n",
    "num_cols = len(filtered_df.columns)\n",
    "print(f\"Shape of the filtered DataFrame: ({num_rows}, {num_cols})\")\n",
    "\n",
    "# ─── Optional: Unpersist if it was cached earlier ────────────────────\n",
    "filtered_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a231de2a-e83c-40f1-98ec-f7f8c9275062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://plotly.com/python/lines-on-maps/\n",
    "airport_cols = ['AirportID', 'Name', 'City', 'Country', 'IATA', 'ICAO',\n",
    "                'Latitude', 'Longitude', 'Altitude', 'Timezone', 'DST',\n",
    "                'Tz database time zone', 'Type', 'Source']\n",
    "\n",
    "airport_df = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat\",\n",
    "    header=None, names=airport_cols\n",
    ")\n",
    "\n",
    "airport_coords = airport_df[['IATA', 'Latitude', 'Longitude']].dropna()\n",
    "airport_coords = airport_coords[airport_coords['IATA'].str.len() == 3]  # Keep valid codes\n",
    "\n",
    "top_routes_pd = (\n",
    "    filtered_df.groupBy(\"route\", \"Origin\", \"Dest\")\n",
    "    .agg(F.count(\"*\").alias(\"num_flights\"))\n",
    "    .orderBy(F.desc(\"num_flights\"))\n",
    "    .limit(1000)\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "# Join lat/lon for Origin and Dest\n",
    "top_routes_pd = top_routes_pd.merge(airport_coords, left_on=\"Origin\", right_on=\"IATA\") \\\n",
    "                             .rename(columns={\"Latitude\": \"Origin_Lat\", \"Longitude\": \"Origin_Lon\"}) \\\n",
    "                             .drop(\"IATA\", axis=1)\n",
    "\n",
    "top_routes_pd = top_routes_pd.merge(airport_coords, left_on=\"Dest\", right_on=\"IATA\") \\\n",
    "                             .rename(columns={\"Latitude\": \"Dest_Lat\", \"Longitude\": \"Dest_Lon\"}) \\\n",
    "                             .drop(\"IATA\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2f801c-2374-4553-befd-fe254b02ad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "# ─── Add Airport Markers ─────────────────────────────────────────────\n",
    "# Collect unique airport locations from both Origin and Dest\n",
    "airport_df = pd.concat([\n",
    "    top_routes_pd[['Origin', 'Origin_Lat', 'Origin_Lon']].rename(\n",
    "        columns={\"Origin\": \"IATA\", \"Origin_Lat\": \"lat\", \"Origin_Lon\": \"lon\"}),\n",
    "    top_routes_pd[['Dest', 'Dest_Lat', 'Dest_Lon']].rename(\n",
    "        columns={\"Dest\": \"IATA\", \"Dest_Lat\": \"lat\", \"Dest_Lon\": \"lon\"})\n",
    "]).drop_duplicates(subset=[\"IATA\"])\n",
    "\n",
    "fig.add_trace(go.Scattergeo(\n",
    "    locationmode='USA-states',\n",
    "    lon=airport_df[\"lon\"],\n",
    "    lat=airport_df[\"lat\"],\n",
    "    hoverinfo='text',\n",
    "    text=airport_df[\"IATA\"],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=4,\n",
    "        color='blue',\n",
    "        line=dict(width=1, color='rgba(68, 68, 68, 0)')\n",
    "    ),\n",
    "    name=\"Airports\"\n",
    "))\n",
    "\n",
    "# ─── Add Flight Routes ───────────────────────────────────────────────\n",
    "for _, row in top_routes_pd.iterrows():\n",
    "    fig.add_trace(go.Scattergeo(\n",
    "        locationmode='USA-states',\n",
    "        lon=[row[\"Origin_Lon\"], row[\"Dest_Lon\"]],\n",
    "        lat=[row[\"Origin_Lat\"], row[\"Dest_Lat\"]],\n",
    "        mode='lines',\n",
    "        line=dict(width=1, color='red'),\n",
    "        opacity=row[\"num_flights\"] / top_routes_pd[\"num_flights\"].max(),\n",
    "        name=f\"{row['Origin']} → {row['Dest']}\"\n",
    "    ))\n",
    "\n",
    "# ─── Layout & Display ────────────────────────────────────────────────\n",
    "fig.update_layout(\n",
    "    title_text='Top 1000 Flight Routes (Interactive Map)',\n",
    "    showlegend=True,\n",
    "    geo=dict(\n",
    "        scope='north america',\n",
    "        projection_type='azimuthal equal area',\n",
    "        showland=True,\n",
    "        landcolor='rgb(243, 243, 243)',\n",
    "        countrycolor='rgb(204, 204, 204)',\n",
    "    ),\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c6f284-3139-496b-b082-dd57c502f5f9",
   "metadata": {},
   "source": [
    "### Indexing Categorical Variables - StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4241d05-df67-417b-b131-3acc87f7825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def concat_route(origin, destination): # Concat origin and dest alphabetically\n",
    "    if origin.upper() < destination.upper():\n",
    "        return (origin.upper() + '-' + destination.upper())\n",
    "    else:\n",
    "        return (destination.upper() + '-' + origin.upper())\n",
    "\n",
    "concat_route_udf = udf(concat_route, StringType())\n",
    "\n",
    "chosen_cols = [] # List of final columns\n",
    "final_df = filtered_df.select(chosen_cols)\n",
    "final_df = filtered_df.withColumn(\n",
    "    \"OneWayRoute\",\n",
    "    F.concat_ws(\n",
    "        \"-\",\n",
    "        F.coalesce(F.col(\"Origin\"), F.lit(\"Unknown\")),\n",
    "        F.coalesce(F.col(\"Dest\"), F.lit(\"Unknown\"))\n",
    "    )\n",
    ")\n",
    "final_df = final_df.withColumn('TwoWayRoute', concat_route_udf('Origin', 'Dest')) # Establish TwoWayRoute column, ignore 2-way\n",
    "str_cols = [column[0] for column in final_df.dtypes if column[1] == 'string'] # Get columns with String datatype\n",
    "str_idx_cols = [(column + 'Index') for column in str_cols]\n",
    "\n",
    "indexer = StringIndexer(inputCols = str_cols, outputCols = str_idx_cols).fit(final_df)\n",
    "final_df_indexed = indexer.transform(final_df)\n",
    "final_df_indexed = final_df_indexed.drop(*str_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce714da-726e-461b-8f2c-d34574b9ff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = final_df_indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cc6486-61c0-47b4-9a69-791ae3d2bc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all columns ending with 'Index'\n",
    "index_cols = [col for col in filtered_df.columns if col.endswith(\"Index\")]\n",
    "\n",
    "# For each such column, count distinct values\n",
    "for col_name in index_cols:\n",
    "    distinct_count = filtered_df.select(col_name).distinct().count()\n",
    "    print(f\"Column '{col_name}' has {distinct_count} distinct categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9762966-901c-444e-8c92-8b8765101bd5",
   "metadata": {},
   "source": [
    "#### Feature expansion discussion\n",
    "\n",
    "OneWayRoute & TwoWayRoute feature expansions have thousands of distinct categories which is difficult for random forest to handle. These features will be excluded from training the model. \n",
    "\n",
    "- Column 'Operating_AirlineIndex' has 28 distinct categories\n",
    "- Column 'IATA_Code_Operating_AirlineIndex' has 28 distinct categories\n",
    "- Column 'OriginIndex' has 388 distinct categories\n",
    "- Column 'OriginCityNameIndex' has 381 distinct categories\n",
    "- Column 'OriginStateIndex' has 53 distinct categories\n",
    "- Column 'OriginStateNameIndex' has 53 distinct categories\n",
    "- Column 'DestIndex' has 388 distinct categories\n",
    "- Column 'DestCityNameIndex' has 381 distinct categories\n",
    "- Column 'DestStateIndex' has 53 distinct categories\n",
    "- Column 'DestStateNameIndex' has 53 distinct categories\n",
    "- Column 'OneWayRouteIndex' has 8182 distinct categories\n",
    "- Column 'TwoWayRouteIndex' has 4135 distinct categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b6750c-dc68-4029-9dcd-b0007e7f1f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.drop(\"routeIndex\")\n",
    "filtered_df = filtered_df.drop(\"TwoWayRouteIndex\")\n",
    "filtered_df = filtered_df.drop(\"OneWayRouteIndex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f42431-eb5d-4bc9-bdfe-5c9c1ba40330",
   "metadata": {},
   "outputs": [],
   "source": [
    "del final_df_indexed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a9b1a7-04a2-4203-b9d0-3083f61cf9d7",
   "metadata": {},
   "source": [
    "## 2. Train First Model: Random Forest Classifier + Feature Selection\n",
    "\n",
    "- RF cannot handle missing values\n",
    "- Values must be numeric (categorical features must be indexed)\n",
    "- Define Label column (Classification df['ArrDelayGroups'] or Regression df['ArrDelay'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac4face-865f-4335-8ba9-45383062f492",
   "metadata": {},
   "source": [
    "### Reduce label column to 4 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481c41f9-16d1-44ce-b57d-0f808cec4b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "adg = filtered_df.select([\"ArrivalDelayGroups\"])\n",
    "filtered_df = filtered_df.withColumn(\n",
    "    \"ArrivalDelayGroups\", \n",
    "    when(adg[\"ArrivalDelayGroups\"] == -2, 0)\n",
    "    .when(adg[\"ArrivalDelayGroups\"] == -1, 1)\n",
    "    .when((adg[\"ArrivalDelayGroups\"] == 0) | (adg[\"ArrivalDelayGroups\"] == 1), 2)\n",
    "    .otherwise(3)\n",
    ")\n",
    "filtered_df.groupBy(\"ArrivalDelayGroups\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369517c4-5406-4df2-898b-dc1d08b4d456",
   "metadata": {},
   "source": [
    "### Split dataset into train, test, validation set (x & y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be67f52a-e11d-4a32-b5ef-2477cedcb62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa28b92b-f190-4d27-a3fd-438b5783abc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataframe into train, validation, and test data\n",
    "train_df, val_df, test_df = filtered_df.randomSplit([0.7, 0.15, 0.15], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d02226-b2b1-495d-b522-748297ae9b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define label & features\n",
    "label_col = \"ArrivalDelayGroups\"\n",
    "feature_cols = [col for col in filtered_df.columns if col != label_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e645d3-3f00-48f6-bb80-ca630d35cb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble features\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badea2e5-eda8-443b-b18d-44ab954888a8",
   "metadata": {},
   "source": [
    "### Generate Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e1c7a0-5e03-4726-bfe6-f6c410f3dd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"ArrivalDelayGroups\", featuresCol=\"features\", seed=42, maxBins=800, numTrees = 5, maxDepth = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e91a7dc-f41a-4c8b-9a73-7ed9c006aad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pipeline to chain multiple steps into single reusable object\n",
    "pipeline = Pipeline(stages=[assembler, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02005f77-63de-4da4-91c3-b98e1aaa78c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dff4897-93c9-4e67-b70a-4dae0a9b618f",
   "metadata": {},
   "source": [
    "### Predict on Test Set & Evaluate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814eef1f-f6f0-4c62-aa27-1f2fea3cbfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply trained pipeline model to testing data\n",
    "\n",
    "# pipeline will do the following: \n",
    "## index label col\n",
    "## assemble all numeric features into features vector\n",
    "# takes features and predicts label, outputs class probabilities and raw scores\n",
    "pred_train = model.transform(train_df)\n",
    "pred_val = model.transform(val_df)\n",
    "pred_test = model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31035602-bbd0-472a-8187-95b09e55aa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check output contents\n",
    "pred_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c0a9fc-089f-4251-9980-29776c1eb758",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train.select(\"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17b1841-db91-44f1-bab8-d5915afb84bf",
   "metadata": {},
   "source": [
    "### Assess Overfitting for Initial Model\n",
    "\n",
    "For this initial model, overfitting is assessed on how well the model is able to predict outcomes on the training dataset compared to the testing dataset. Higher accuracy % on the training dataset compared to testing dataset may indicate overfitting. For future iterations of model optimization, a validation dataset will be used for hyperparameter tuning to assess which parameters gives us the highest predictive accuracy on testing datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d7a928-2dde-42fe-b2ca-4e238548b077",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"ArrivalDelayGroups\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy_train = evaluator.evaluate(pred_train)\n",
    "accuracy_test = evaluator.evaluate(pred_test)\n",
    "\n",
    "print(f\"Train Accuracy: {accuracy_train:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb8cefa-37ae-4631-8557-66116fe94d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE FOR PLOTTING ACCURACY VALUES AS A LINE GRAPH\n",
    "\n",
    "# y-axis: accuracy %\n",
    "# x-axis: Initial model\n",
    "# Group 1: Training dataset (accuracy_train value)\n",
    "# Group 2: Testing dataset (accuracy_test value)\n",
    "pd_data = [['Train', accuracy_train * 100], ['Test', accuracy_test * 100]]\n",
    "pd_df = pd.DataFrame(data = pd_data, columns = ['Model', 'Accuracy'])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.barplot(data = pd_df, x = 'Model', y = 'Accuracy')\n",
    "ax.set_ylim(0, 100)\n",
    "ax.set_title('Random Forest Model Accuracy')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('%')\n",
    "\n",
    "## Use this to write the discussion of how the model performed accuracy wise as outputs are deleted when we commit to github\n",
    "# Group 1 data point - RF model gave training accuracy of X%\n",
    "# Group 2 data point - RF model gave training accuracy of Y%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377250b9-9ae6-40e0-a628-2cb3b3654e80",
   "metadata": {},
   "source": [
    "#### First Model Accuracy Discussion\n",
    "\n",
    "Our first model yielded accuracy scores of around 42% for both the train and test data. While these results are not great first impressiont, they do give us a viable path forward. If we reverse our labels and implement boosted random forest, we can build a model with signicantly better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12235af9-7139-4e80-bdf9-6fd954b7cb58",
   "metadata": {},
   "source": [
    "### Feature Importance Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123ce112-2348-4f6a-829c-635d93307748",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = model.stages[-1]  # assuming RF is the last stage\n",
    "importances = rf_model.featureImportances\n",
    "feature_names = assembler.getInputCols()\n",
    "\n",
    "importances_list = list(zip(feature_names, importances.toArray()))\n",
    "importances_sorted = sorted(importances_list, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Feature Importances from RF Gini Importance:\\n\")\n",
    "for feat, score in importances_sorted:\n",
    "    print(f\"{feat}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443e9875-f85f-4af0-b90d-9f5ae5929fac",
   "metadata": {},
   "source": [
    "#### Feature Importance Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a457d48b-7897-46b2-8e03-dba9fadd3079",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = len([x for x in importances_sorted if x[1] < 0.05])\n",
    "print(f'There are {num} (out of {len(importances_sorted)}) features having Gini indexes that are less than 0.05.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117c1743-8cbf-4153-a808-0281ac3112e5",
   "metadata": {},
   "source": [
    "As can be seen from the importance scores above, only a minority of our features have a nonzero score. The highest gini importance score was consistantly the TaxiOut feature with an MDI of around 0.5 followed by ArrTime, and then a steep dropoff for the other features. This indicates that the model relies very heavily on these 2 features. Additionally, a majority of our features had 0 importance, indicating that the model found these features irrelevant or redundant. Notably, the model did not use any features relating to destination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd2e5bc-da25-4d68-ace7-075db6ac7bb3",
   "metadata": {},
   "source": [
    "#### Conclusion \n",
    "\n",
    "**The Random Forest model identified 'TaxiOut', 'ArrTime', and 'WheelsOn' as the most influential features, while the majority of other features contributed little to the prediction task. This strong dependence on a few variables raises the possibility of feature leakage, where future information (e.g., actual arrival time) is inadvertently used to predict outcomes, which would not be available at prediction time. Although the model performed marginally better than random guessing, both the training and test accuracies (~42%) indicate that the model is currently underperforming. Fortunately, this is only an initial iteration. There are several avenues for improvement, including hyperparameter tuning (e.g., adjusting n_estimators or max_depth), removing or re-engineering unused features, and exploring alternative models such as logistic regression or gradient-boosted trees (e.g., XGBoost).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d057690e-aaca-4e37-b6d1-6ee4670ec947",
   "metadata": {},
   "source": [
    "# Milestone 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c279693-1d3f-4ea8-9eda-46ab5cc6909b",
   "metadata": {},
   "source": [
    "### Optimize Random Forest Classifier Using Validation Dataset\n",
    "\n",
    "- numTrees\n",
    "- maxDepth\n",
    "- minInstancesPerNode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893ab25f-d8f6-4334-bf72-c98fd53abc5a",
   "metadata": {},
   "source": [
    "##### Validate smaller subsets of the full dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b3a6a8-9c7f-41bf-8d29-9c7d9a235884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "label_col = \"ArrivalDelayGroups\"\n",
    "feature_cols = [col for col in filtered_df.columns if col != label_col]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"ArrivalDelayGroups\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269ae9dc-86b8-4725-b036-96950122d53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_subsets(list_of_subsets):\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    for subset in list_of_subsets:\n",
    "        start = time()\n",
    "        # Creating a subset of data\n",
    "        splits = filtered_df.randomSplit([subset, 1 - subset], seed = 42)\n",
    "        trial_df = splits[0]\n",
    "        print(f'At {subset *100}% of original data, there are {trial_df.count()} rows.')\n",
    "        # train, test, split\n",
    "        train_df, val_df, test_df = trial_df.randomSplit([0.7, 0.15, 0.15], seed=42)\n",
    "        # assemble features and pipeline\n",
    "        rf = RandomForestClassifier(labelCol=\"ArrivalDelayGroups\",\n",
    "                                featuresCol=\"features\",\n",
    "                                seed=42, maxBins=800,\n",
    "                                numTrees = 5, maxDepth = 3)\n",
    "        pipeline = Pipeline(stages=[assembler, rf])\n",
    "        # Train and get predictions\n",
    "        model = pipeline.fit(train_df)\n",
    "        pred_train = model.transform(train_df)\n",
    "        pred_val = model.transform(val_df)\n",
    "        pred_test = model.transform(test_df)\n",
    "        # Output accuracy\n",
    "        train_accuracies.append(evaluator.evaluate(pred_train))\n",
    "        test_accuracies.append(evaluator.evaluate(pred_test))\n",
    "        \n",
    "        # print(f\"Train Accuracy: {accuracy_train:.4f}\")\n",
    "        # print(f\"Test Accuracy: {accuracy_test:.4f}\")\n",
    "        print(f\"This round of training took {int(time() - start)} seconds.\")\n",
    "        print()\n",
    "    train_df = pd.DataFrame({'SubsetPercent': list_of_subsets,\n",
    "                'Type': ['Train' for _ in list_of_subsets],\n",
    "                'Score': train_accuracies})\n",
    "    test_df = pd.DataFrame({'SubsetPercent': list_of_subsets,\n",
    "                'Type': ['Test' for _ in list_of_subsets],\n",
    "                'Score': test_accuracies})\n",
    "    return pd.concat([train_df, test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a636299-b2a5-4567-ae4f-e169186a7bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the train and test accuracy as sample size decreases\n",
    "subset_percents = [0.1, 0.05, 0.01, 0.001, 0.0001]\n",
    "subsets_eval = validate_subsets(subset_percents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6b5f93-cd8c-4494-aff4-9ca8d14b54b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot subset validation\n",
    "subsets_eval['SubsetPercent'] = subsets_eval['SubsetPercent'] * 100\n",
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(data = subsets_eval,\n",
    "    x = 'SubsetPercent',\n",
    "    y = 'Score',\n",
    "    hue = 'Type')\n",
    "plt.ylim(0.35, 0.5)\n",
    "ax.invert_xaxis()\n",
    "plt.xlabel('Percent of Subset')\n",
    "plt.title('Train and Test Accuracies as Sample Size Decreases')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58603dd2-608b-4b58-901e-4c7e8b3a209c",
   "metadata": {},
   "source": [
    "As we take smaller subset of of the data, the train and test accuracies of the subset stay close to 42% as the original data does. \n",
    "This suggests that our data is stable.\n",
    "\n",
    "Surprisingly, the time it takes to train the smaller subsets is pretty much same as larger ones. We only save minimal amount of time\n",
    "validating our model with smaller subsets. However, the RAM usage should be improved.\n",
    "\n",
    "I will tune the parameter with 0.1% of original dataset. At this split, the accuracy remains true to the original but it will help with the memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889da05f-fef2-44b8-aa96-8c691569a0db",
   "metadata": {},
   "source": [
    "##### Taking a 0.1% subset of data to build models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b91e1d-602c-4aac-b26c-68420527d149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the full dataframe and persist the train, test splits\n",
    "percent = 0.001\n",
    "\n",
    "splits = filtered_df.randomSplit([percent, 1 - percent], seed = 42)\n",
    "smaller_df = splits[0]\n",
    "train_df, val_df, test_df = smaller_df.randomSplit([0.7, 0.15, 0.15], seed=42)\n",
    "\n",
    "train_df.persist()\n",
    "val_df.persist()\n",
    "test_df.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a28cb3-b2fd-426b-8971-173172e393f8",
   "metadata": {},
   "source": [
    "##### Tune numTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6287e1-d0f3-4fb1-906b-e7f287926d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to pick best model out of lists of hyperparameters\n",
    "def validate_model(numTrees_list, maxDepth_list):\n",
    "    best_model = None\n",
    "    best_accuracy = 0\n",
    "    best_params = None\n",
    "    param_grid = [(nt, md) for nt in numTrees_list for md in maxDepth_list]\n",
    "    accuracies_list = []\n",
    "    for numTrees, maxDepth in param_grid: # for each hyperparameter\n",
    "        start = time()\n",
    "        print(f\"Training model with numTrees={numTrees}, maxDepth={maxDepth}\")\n",
    "    \n",
    "        # generate random forest classifier\n",
    "        rf = RandomForestClassifier(labelCol=\"ArrivalDelayGroups\", featuresCol=\"features\", \n",
    "                                    numTrees=numTrees, maxDepth=maxDepth, \n",
    "                                    seed=42, maxBins = 800)\n",
    "        # pipeline = Pipeline(stages=[label_indexer, assembler, rf])\n",
    "        pipeline = Pipeline(stages=[assembler, rf])\n",
    "    \n",
    "        # train on training data set\n",
    "        model = pipeline.fit(train_df)\n",
    "    \n",
    "        # Get train and val accuracy\n",
    "        train_predictions = model.transform(train_df)\n",
    "        train_accuracy = evaluator.evaluate(train_predictions)\n",
    "        val_predictions = model.transform(val_df)\n",
    "        val_accuracy = evaluator.evaluate(val_predictions)\n",
    "    \n",
    "        # print accuracy on train and validation set\n",
    "        print(f'This round of training took {int(time() - start)} seconds.')\n",
    "        print()\n",
    "    \n",
    "        # if accuracy is better than the previous one, update variables\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_model = model\n",
    "            best_accuracy = val_accuracy\n",
    "            best_params = (numTrees, maxDepth)\n",
    "\n",
    "        # Append values to a Pandas df\n",
    "        accuracies_list.append([numTrees, maxDepth,\n",
    "                                train_accuracy, val_accuracy])\n",
    "    \n",
    "    # Process Pandas df and return values\n",
    "    accuracies_df = pd.DataFrame(data = accuracies_list,\n",
    "                                 columns = ['NumTrees', 'MaxDepth', 'TrainAccuracy', 'ValAccuracy'])\n",
    "    return best_model, best_params, melt_pd_cols(accuracies_df)\n",
    "\n",
    "# Function to melt Pandas columns\n",
    "def melt_pd_cols(df):\n",
    "    melted_df = pd.melt(frame = df,\n",
    "                        id_vars = ['NumTrees', 'MaxDepth'],\n",
    "                        value_vars = ['TrainAccuracy', 'ValAccuracy'],\n",
    "                        var_name = 'AccuracyType',\n",
    "                        value_name = 'Score')\n",
    "    return melted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa57218c-f573-47d0-b725-a4248d6a13a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumTree tuning\n",
    "numTrees_list = [10, 30, 40]\n",
    "maxDepth_list = [10]\n",
    "_, _, numTrees_acc_df = validate_model(numTrees_list, maxDepth_list)\n",
    "\n",
    "# Plot validation for numTrees\n",
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(data = numTrees_acc_df,\n",
    "    x = 'NumTrees',\n",
    "    y = 'Score',\n",
    "    hue = 'AccuracyType')\n",
    "plt.ylim(0.4, 0.8)\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.legend(title = 'Accuracy Type')\n",
    "plt.title('Train and Validation Accuracies as NumTrees Increases')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f978b81-d421-45e5-a1c5-65bb7e899012",
   "metadata": {},
   "source": [
    "Increase of numTrees does not improve validation accuracy significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1391a7-6da4-4ea8-8b0f-d13b02399efc",
   "metadata": {},
   "source": [
    "##### Tune maxDepth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47650bb8-640e-4d0e-b95f-8dee9e01345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MaxDepth tuning\n",
    "numTrees_list = [10]\n",
    "maxDepth_list = [10, 20, 30]\n",
    "_, _, maxDepth_acc_df = validate_model(numTrees_list, maxDepth_list)\n",
    "\n",
    "# Plot validation for MaxDepth\n",
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(data = maxDepth_acc_df,\n",
    "    x = 'MaxDepth',\n",
    "    y = 'Score',\n",
    "    hue = 'AccuracyType')\n",
    "plt.ylim(0.4, 1)\n",
    "plt.xlabel('Number of Depths')\n",
    "plt.legend(title = 'Accuracy Type')\n",
    "plt.title('Train and Validation Accuracies as MaxDepth Increases')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b8a2e9-5816-4e87-8549-67c584e27990",
   "metadata": {},
   "source": [
    "As maxDepth is increased, we observe overfitting. Let's try a combination between numTrees and maxDepth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89916d8-4c13-43e8-b07c-f16e888b6751",
   "metadata": {},
   "source": [
    "##### Combination of numTrees and maxDepth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c417bc47-58ae-4415-9353-c6a1ffc4c6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "numTrees_list = [30, 40]\n",
    "maxDepth_list = [10, 20]\n",
    "model, param, accuracy_df = validate_model(numTrees_list, maxDepth_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12681dc8-756c-4710-892b-7bf084c95671",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize = (15, 8))\n",
    "fig.suptitle('Best Model by Hyper Parameters')\n",
    "\n",
    "numTree_plot = sns.lineplot(data = accuracy_df,\n",
    "    x = 'NumTrees',\n",
    "    y = 'Score',\n",
    "    hue = 'AccuracyType',\n",
    "    ax = axes[0])\n",
    "axes[0].set_xlabel('Number of Trees')\n",
    "axes[0].set_title('NumTree')\n",
    "\n",
    "maxDepth_plot = sns.lineplot(data = accuracy_df,\n",
    "    x = 'MaxDepth',\n",
    "    y = 'Score',\n",
    "    hue = 'AccuracyType',\n",
    "    ax = axes[1])\n",
    "axes[1].set_xlabel('Number of Depths')\n",
    "axes[1].set_title('MaxDepth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fb521e-b8d3-4414-9434-af278d8b0c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_test = model.transform(test_df)\n",
    "best_val = model.transform(test_df)\n",
    "best_test_acc = evaluator.evaluate(best_test)\n",
    "best_val_acc = evaluator.evaluate(best_val)\n",
    "\n",
    "scores = [['Test', best_test_acc], ['Validation', best_val_acc]]\n",
    "scores = pd.DataFrame(scores, columns = ['Type', 'Score'])\n",
    "sns.barplot(data = scores, x = 'Type', y = 'Score')\n",
    "plt.title(f'Performance of Best Model with numTrees = {param[0]} and maxDepth = {param[1]}')\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd1c9b6-4ce2-494c-addc-6abe2caacd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7b16a7-7c57-425b-946e-e2495488f1bb",
   "metadata": {},
   "source": [
    "Despite improving the model by 10% by tuning numTrees and maxDepth, our best Random Forest model scores at about 52% which is on par with random guessing.\n",
    "\n",
    "However, we maintain the belief that there is potential in the data that can benefit from more feature engineering and a different machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e1529b-448f-44f7-8ef0-e166a46149ad",
   "metadata": {},
   "source": [
    "#### Attempt to build a Gradient Boosted Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd73c7c-77fc-459b-abbf-97c799a17fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBTClassifier wants boolean labels. This cell will throw error.\n",
    "\n",
    "# %%time\n",
    "# from pyspark.ml.classification import GBTClassifier\n",
    "# from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# gbt = GBTClassifier(labelCol= \"ArrivalDelayGroups\",\n",
    "#                     featuresCol=\"features\", \n",
    "#                     maxIter=10, maxDepth=5)\n",
    "# pipeline = Pipeline(stages=[assembler, gbt])\n",
    "# model = pipeline.fit(train_df)\n",
    "# val_prediction = model.transform(val_df)\n",
    "\n",
    "# evaluator = MulticlassClassificationEvaluator(labelCol=\"ArrivalDelayGroups\", \n",
    "#                                               predictionCol=\"prediction\", \n",
    "#                                               metricName=\"accuracy\")\n",
    "# accuracy = evaluator.evaluate(predictions)\n",
    "# print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2680cf8f-a4a3-436d-845c-fd60e3fbcdb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
