{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "263d72e9-c4b6-4e1e-9c8f-ee3c0e44b16e",
   "metadata": {},
   "source": [
    "# Group Project Milestone 2: Data Exploration & Initial PreProcessing\n",
    "\n",
    "In this assignment you will need to:\n",
    "\n",
    "1. Create a GitHub ID\n",
    "2. Create a GitHub Repository (Public or Private it is up to you. In the end it will have to be Public) and add your group members as collaborators\n",
    "3. Perform the data exploration step (i.e. evaluate your data, # of observations, details about your data distributions, scales, missing data, column descriptions) Note: For image data you can still describe your data by the number of classes, # of images, plot example classes of the image, size of images, are sizes uniform? Do they need to be cropped? normalized? etc.\n",
    "4. Plot your data. For tabular data, you will need to run scatters, for image data, you will need to plot your example classes.\n",
    "5. How will you preprocess your data? You should explain this in your README.md file and link your Jupyter notebook to it. All code and  Jupyter notebooks have be uploaded to your repo.\n",
    "6. You must also include in your Jupyter Notebook, a link for data download and environment setup requirements: \n",
    "\n",
    "\n",
    "!wget !unzip like functions as well as !pip install functions for non standard libraries not available in colab are required to be in the top section of your jupyter lab notebook. Or having the data on GitHub (you will need the academic license for GitHub to do this, larger datasets will require a link to external storage)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43e3dd4-fca0-4421-9ce5-d68e001a4cd8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## GitHub ID\n",
    "\n",
    "https://github.com/SmoothData-BigBrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88642e35-73bb-4077-acc6-184563785bc6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Dataset link\n",
    "\n",
    "https://www.kaggle.com/datasets/robikscube/flight-delay-dataset-20182022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d757d7-5d93-4e03-b350-a04fc12a5d31",
   "metadata": {},
   "source": [
    "## Setup for spark and data\n",
    "\n",
    "Perform the data exploration step (i.e. evaluate your data, # of observations, details about your data distributions, scales, missing data, column descriptions) Note: For image data you can still describe your data by the number of classes, # of images, plot example classes of the image, size of images, are sizes uniform? Do they need to be cropped? normalized? etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a03d6c-1a93-4b54-8ecf-596818ee30e4",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271da376-11a6-4fa6-935b-f45b210b763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install everything inside the 'requirements.txt' file before running this notebook\n",
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8cc60b-8fd9-4a6a-b979-c114ba13dd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import util\n",
    "\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import col, isnan, when, count, isnull, sum, concat_ws, coalesce, lit, avg, rand, round\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863f162f-b363-4c17-b226-356b1582e8e0",
   "metadata": {},
   "source": [
    "### Creat Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e41cd7-f31e-46cd-bcf1-138b597e7944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"Flight Data Analysis\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)\n",
    "# spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "### Mihirs machine spark setting\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Flight Data Analysis\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.executor.memory\", \"12g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", \"1000\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feb895f-738d-486f-8d67-4e7a1ff8251f",
   "metadata": {},
   "source": [
    "### Read in data files\n",
    "\n",
    "> **Note: Update the home_dir and download_path variables before running this cell block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c2de02-6753-4c6d-a5cf-d724eba5828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# home_dir = os.path.expanduser('~')\n",
    "# path_for_Nam = 'C:/GitGroupProject/GroupProject' # comment this later\n",
    "# download_path = os.path.join(path_for_Nam,'/data/') # comment this later\n",
    "\n",
    "# download_path = os.path.join('/workspaces/GroupProject/data/') # Uncomment this later\n",
    "\n",
    "home_dir = os.path.expanduser('~')\n",
    "local_download_path = os.path.join(home_dir, 'Desktop/GroupProject/data/')\n",
    "file_id = '1tch7xbFIgBtXKXa16E4QCpVKedUExfO3'  # My File ID for airlines.zip on GDrive \n",
    "util.check_and_fetch_data(file_id, local_download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf04e58-8767-4dd4-ae08-f790b0920c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_path = '~/Desktop/GroupProject/data/archive/raw'\n",
    "# path_for_Nam = 'C:/GitGroupProject/GroupProject'\n",
    "# home_dir = os.path.expanduser('~')\n",
    "# download_path = os.path.join(home_dir, 'Desktop/GroupProject/data/')\n",
    "\n",
    "# download_path = os.path.join(path_for_Nam, '/data/')\n",
    "Nam_local = 'C:/lecture-notebooks/GroupProject/data/archive/raw' # comment this later\n",
    "\n",
    "csv_files = glob.glob(f\"{Nam_local}/*.csv\") # comment this later\n",
    "\n",
    "### *****************\n",
    "# Do we want to keep this spark.read.csv or just use parquet files from the start? \n",
    "# This would avoid running this, then saving to parquet, and reading parquet files into df again\n",
    "### *****************\n",
    "# csv_files = \"file:///mnt/desktop/combined_file.csv\"#glob.glob(f\"{local_download_path}archive/raw/*.csv\") #Uncomment this later\n",
    "df = spark.read.csv(csv_files,\n",
    "                       sep = ',',\n",
    "                       inferSchema = True,\n",
    "                       header = True)\n",
    "\n",
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80859ac5-2131-4275-bd26-2f8bf88e1de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out combined single csv file - why? \n",
    "# df.coalesce(1).write.csv(\"combined_file_csv\", header=True) # Uncomment this later\n",
    "# df.write.mode(\"overwrite\").parquet(\"combined_files\")\n",
    "df = spark.read.parquet(\"combined_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbe9348-4801-4a17-b8a2-a6b3d27b2c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_des = spark.read.csv('flights_column_des.csv', sep = ',', inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52b9cd5-ec4f-4234-ab63-62fcb7d2475f",
   "metadata": {},
   "source": [
    "## Explore Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1dbdd3-0278-4743-b6eb-fd8f912fe390",
   "metadata": {},
   "source": [
    "### Get dataset shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc98c34-1dc1-45ee-921e-cb7c2534331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get df shape\n",
    "num_entries = df.count()\n",
    "num_cols = len(df.columns)\n",
    "print(f\"Shape of the DataFrame: ({num_entries}, {num_cols})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ead44d9-dd3c-44f0-b34b-bd7eee5f9400",
   "metadata": {},
   "source": [
    "### Explore null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892ad7f6-6fca-49b6-a39c-fda420eaf2c7",
   "metadata": {},
   "source": [
    "#### Computing non-null counts as percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60addd02-eed0-42b9-8aea-20ef522d7115",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_null_counts = df.select([count(col(c)).alias(c) for c in df.columns]).collect()[0].asDict()\n",
    "\n",
    "# Calculate non-null percentages\n",
    "non_null_percentages = {\n",
    "    col_name: (count_val / num_entries) * 100\n",
    "    for col_name, count_val in non_null_counts.items()\n",
    "}\n",
    "\n",
    "sorted_columns = sorted(non_null_percentages.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for col_name, pct in sorted_columns:\n",
    "    print(f\"{col_name}: {pct:.2f}% non-null\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2587bb8f-7c94-4206-abbf-6892a1946f81",
   "metadata": {},
   "source": [
    "#### Subset dataset\n",
    "removing columns with <90% null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6026b7-cf26-4ba0-aad3-ed786cca2cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_above_90 = [col_name for col_name, pct in non_null_percentages.items() if pct >= 90]\n",
    "filtered_df = df.select(columns_above_90)\n",
    "filtered_df.select(filtered_df.columns[:8]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c44dab-7784-4664-a50a-39a53641cda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip the extra spaces from col names \n",
    "for c in filtered_df.columns:\n",
    "    filtered_df = filtered_df.withColumnRenamed(c, c.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a120f4-61e3-4e38-ad52-0b21fb368318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filtered df shape\n",
    "filtered_num_rows = filtered_df.count()\n",
    "filtered_num_cols = len(filtered_df.columns)\n",
    "print(f\"Shape of the Filtered DataFrame removing cols w/ <90% non-null values: ({filtered_num_rows}, {filtered_num_cols})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fc3021-969e-4854-8e82-5b93325dcbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc94a931-7d70-460f-845f-ebf903d055a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the master df since we won't need it anymore at all\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e0fafa-5968-4614-8d88-875119f399ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filtered df to not have to redo code later\n",
    "#filtered_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(\"filtered_df_temp\")\n",
    "\n",
    "# # read in already filtered_df saved previously\n",
    "# filtered_df = spark.read.csv('part-00000-b248588c-b561-414a-ba2c-bc77825e455a-c000.csv', sep = ',', inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ef213e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Discussion on null values\n",
    "Dataset consists of columns with >90% non-null values and then it drops down to 0-17% non-null. Dataset to be used for further exploration will only include columns with >90% non-null values for more robust analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a60fe8-5968-46dc-8b89-6d30f7f97ef6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Remaining Column Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc302c6-1c8d-474a-8b3a-eedbb2cff3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all cols in filtered_df\n",
    "filtered_cols = filtered_df.columns \n",
    "\n",
    "# remove any white space\n",
    "filtered_cols = [str(c).strip() for c in filtered_cols]\n",
    "\n",
    "# subset column description dataframe for only columns in filtered dataset\n",
    "filtered_col_des = col_des.filter(col('column').isin(filtered_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e0eb34-70f1-4c87-a7d1-575fab89b0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check df was filtered correctly, length & row count should match\n",
    "f_col_len = filtered_col_des.count()\n",
    "f_col_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9b65cc-65c0-458f-8669-f33cc3f960c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View all column descriptions in filtered dataframe\n",
    "# Full data col description is in \"../data/README.md\"\n",
    "filtered_col_des.show(n=f_col_len, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc67605-0fba-46ec-b337-358d30e82339",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Dataset Statistics & Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8010c2b-e810-4c3d-bd64-6653b9f5d63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data type for each column\n",
    "for name, dtype in filtered_df.dtypes:\n",
    "    print(f\"{name}: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139371c8-8fcb-4638-a3ed-56cbf475727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_string_cols = [col_name for col_name, dtype in filtered_df.dtypes if dtype != 'string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10722ce-b5c3-4735-a49d-86032ece9115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset column description dataframe for only non-string\n",
    "non_string_col_des = filtered_col_des.filter(col('column').isin(non_string_cols))\n",
    "non_string_col_des.show(n=non_string_col_des.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac44c139-4bdb-4771-9a9b-1e0be2ea4635",
   "metadata": {},
   "source": [
    "### Discussion on skewed data distributions\n",
    "\n",
    "When taking a look at the columns with the most amount of skew in the data distribution, columns that are ID inidicators or Flight numbers do not make sense to further investigations of data distributions. Although these are numerical values, they represent categorical variables as opposed to continuous. \n",
    "\n",
    "Columns with 'ID','Number', 'Origin', 'Dest' in the column name will be removed from statistical analysis to remove these categorical variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094db735-e065-47b8-b634-2a7b570891af",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_col_des = non_string_col_des.filter(\n",
    "    ~non_string_col_des['column'].rlike('Dest|Origin|ID|Number|FlightDate')\n",
    ")\n",
    "cont_col_des.show(n=cont_col_des.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f447eb1-f02c-4630-b549-e17ba5c22874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get statistics for all continuous variables\n",
    "cont_cols = [row['column'] for row in cont_col_des.select('column').collect()]\n",
    "\n",
    "describe_df = filtered_df.select(cont_cols).describe()\n",
    "\n",
    "# compute Q1, Median, Q3 for each column\n",
    "stats = {\n",
    "    \"25%\": {},\n",
    "    \"50%\": {},\n",
    "    \"75%\": {}\n",
    "}\n",
    "\n",
    "for col_name in cont_cols:\n",
    "    q1, median, q3 = filtered_df.approxQuantile(col_name, [0.25, 0.5, 0.75], 0.01)\n",
    "    stats[\"25%\"][col_name] = str(q1)\n",
    "    stats[\"50%\"][col_name] = str(median)\n",
    "    stats[\"75%\"][col_name] = str(q3)\n",
    "\n",
    "# convert new rows to df rows\n",
    "new_rows = [Row(summary=stat_name, **cols) for stat_name, cols in stats.items()]\n",
    "quartile_df = spark.createDataFrame(new_rows)\n",
    "\n",
    "# append the new rows to describe_df\n",
    "full_summary_df = describe_df.unionByName(quartile_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa81d22-37f7-4b68-80d6-d04e4bac3cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the new rows to describe_df\n",
    "full_summary_df = describe_df.unionByName(quartile_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407891b4-f3e8-40a5-a9fa-570a30abc424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert and save as parquet\n",
    "# full_summary_df.to_parquet(\"full_summary_df\", index=False)\n",
    "full_summary_df.write.mode(\"overwrite\").parquet(\"full_summary_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910780ae-036c-4734-8d7c-9b09bef8c032",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_summary_df = spark.read.parquet(\"full_summary_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d044b40-76f0-4546-92f9-d5d923193c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_summary_df.select(full_summary_df.columns[:6]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce22dad0-0fda-4167-a939-1e555d30da1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view df columns\n",
    "full_summary_df.select(full_summary_df.columns[11:17]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc35b0f-0d14-42fb-aca4-a1b71fa2c667",
   "metadata": {},
   "source": [
    "### Explore skewed data\n",
    "\n",
    "mean > median, data is right-skewed (longer tail on the right)\n",
    "median < mean, data is left-skewed (longer tail on the left)\n",
    "\n",
    "This code is to find top 20 features with largest skews. These features will then be plotted in histograms\n",
    "\n",
    "The purpose of doing this is to understand if there are any outliers in the dataset that may be worth removing from the dataset prior to applying ML methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b3acce-c99b-40b9-95a0-bde8890a30d0",
   "metadata": {},
   "source": [
    "### Explore data distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56419c4f-626b-4dc2-893f-9040acd940a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mean and median rows as dicts\n",
    "mean_row = full_summary_df.filter(col(\"summary\") == \"mean\").collect()[0].asDict()\n",
    "median_row = full_summary_df.filter(col(\"summary\") == \"50%\").collect()[0].asDict()\n",
    "\n",
    "# skip the 'summary' key\n",
    "cols = [col for col in mean_row.keys() if col != \"summary\"]\n",
    "\n",
    "# build rows of (column, absolute_diff, skew direction)\n",
    "result_rows = []\n",
    "for c in cols: # for each col\n",
    "    mean_val = float(mean_row[c]) # get mean\n",
    "    median_val = float(median_row[c]) # get median\n",
    "    diff = __builtins__.abs(mean_val - median_val) # get abs difference\n",
    "    skew = \"right\" if mean_val > median_val else \"left\" if mean_val < median_val else \"none\" # get skew direction\n",
    "    result_rows.append(Row(column=c, absolute_diff=diff, skew=skew)) # aggregate\n",
    "\n",
    "# create df\n",
    "diff_df = spark.createDataFrame(result_rows)\n",
    "\n",
    "# get top 20\n",
    "top_skewed = diff_df.orderBy(col(\"absolute_diff\").desc()).limit(20)\n",
    "\n",
    "top_skewed.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086edcdb-8a88-49f5-aedc-694077773bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all cols in filtered_df\n",
    "skewed_cols = [row['column'] for row in top_skewed.select('column').collect()]\n",
    "\n",
    "# remove any white space\n",
    "skewed_cols = [str(c).strip() for c in skewed_cols]\n",
    "\n",
    "# subset column description dataframe for only columns in filtered dataset\n",
    "skewed_col_des = col_des.filter(col('column').isin(skewed_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7774476a-fa3d-4d39-b880-c42e17a85f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed_col_des.show(n=skewed_col_des.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cd615b-04cb-4267-bd3f-f12200da0d43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list of columns from 'top_skewed'\n",
    "columns_to_plot = [row['column'] for row in top_skewed.collect()]\n",
    "\n",
    "# filter the columns that exist in filtered_df\n",
    "valid_columns = [col for col in columns_to_plot if col in filtered_df.columns]\n",
    "\n",
    "# plot histograms for each column\n",
    "n_cols = 4  # 4 histograms per row\n",
    "n_rows = (len(valid_columns) + n_cols - 1) // n_cols  # calculate num rows needed\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 5))\n",
    "\n",
    "# flatten axes for easier indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# loop through cols and plot\n",
    "for i, column in enumerate(valid_columns):\n",
    "    hist = filtered_df.select(column).rdd.flatMap(lambda x: x).histogram(20)  # 20 bins\n",
    "\n",
    "    bin_edges, bin_counts = hist\n",
    "\n",
    "    # plot the histogram using the bin edges and counts\n",
    "    axes[i].bar(bin_edges[:-1], bin_counts, width=(bin_edges[1] - bin_edges[0]), edgecolor='black')\n",
    "\n",
    "    # set axes & title\n",
    "    axes[i].set_title(f\"Histogram of {column}\")\n",
    "    axes[i].set_xlabel('Value')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "# turn off any unused subplots\n",
    "for i in range(len(valid_columns), len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "# need to update to add labels for axis \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5265a98b-d34a-4bf0-be25-150481e74fd4",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "The distance column, majority of flights in this dataset have a distance <1000 miles. With a few outliers ranging from 3000-5000 miles. \n",
    "\n",
    "Wheels On & Wheels Off time and CRSDepTime & DepTime columns have a few outliers at 0:00-4:00am, majority of times are listed between 5:00 & 23:59\n",
    "\n",
    "The majority of TaxiOut and TaxiIn times are around 0 (or <50minutes). However, there are outliers sitting at ~1300 & 300 minutes respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043f7534-aaa3-4a11-b303-4a94c75bf79f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Questions to analyze data with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09a6c97-5cb6-4897-9f92-865985101252",
   "metadata": {},
   "source": [
    "### Which Origin Cities had the most delayed flights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4affc1df-26b5-4def-a19b-d8740631b79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_delay = filtered_df.select([\"Origin\", \"DepDelay\"]).groupBy(\"Origin\")\\\n",
    "        .agg(count(F.when(col(\"DepDelay\") > 0, 1)).alias(\"DelayCount\"), \n",
    "             count(F.when(col(\"DepDelay\") < 0, 1)).alias(\"EarlyCount\"),\n",
    "            count(\"*\").alias(\"TotalCount\")).orderBy(col(\"TotalCount\").desc())\n",
    "pandas_delay = count_delay.toPandas()# Assuming pdf has these columns: OriginCity, DelayedFlights, EarlyFlights, OnTimeFlights\n",
    "\n",
    "pdf = pandas_delay.copy()\n",
    "pdf[\"OnTimeCount\"] = pdf[\"TotalCount\"] - pdf[\"DelayCount\"] - pdf[\"EarlyCount\"]\n",
    "top_20 = pdf.head(20)\n",
    "\n",
    "# Bar positions\n",
    "cities = top_20[\"Origin\"]\n",
    "x = np.arange(len(cities))\n",
    "\n",
    "# Heights\n",
    "early = top_20[\"EarlyCount\"]\n",
    "on_time = top_20[\"OnTimeCount\"]\n",
    "delayed = top_20[\"DelayCount\"]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x, early, label=\"Early\", color=\"green\")\n",
    "plt.bar(x, on_time, bottom=early, label=\"On Time\", color=\"gray\")\n",
    "plt.bar(x, delayed, bottom=early + on_time, label=\"Delayed\", color=\"red\")\n",
    "\n",
    "# Labels and formatting\n",
    "plt.xticks(x, cities, rotation=45)\n",
    "plt.ylabel(\"Number of Flights\")\n",
    "plt.title(\"Flight Status by Origin City (Top 20)\")\n",
    "plt.legend(title=\"Flight Status\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c41dc78-d0bc-4d48-ac2e-9629dac7ff1c",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "The plot above shows a stacked bar chart of the origin cities with the 20 highest total flight counts. The bars are stacked according to early departures (green), on time departures (grey), and delayed departures (red) and are organized in descending order starting at the left. From this plot, the overall trend suggests that the majority of flights are early and only a small proportion of flights are actually on time. The origin city with the seemingly largest proportion of delayed departures is Denver and, speaking as someone from Colorado, I can personally attest to this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f3f67a-f47e-48dd-90d9-b25cf3beebf6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Which years had the most delayed flights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0cb107-82c8-4ef5-a286-b71f118439fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_delay = filtered_df.select([\"Year\", \"DepDelay\"]).groupBy(\"Year\")\\\n",
    "        .agg(count(F.when(col(\"DepDelay\") > 0, 1)).alias(\"DelayCount\"), \n",
    "             count(F.when(col(\"DepDelay\") < 0, 1)).alias(\"EarlyCount\"),\n",
    "            count(\"*\").alias(\"TotalCount\")).orderBy(col(\"Year\"))\n",
    "pandas_year_delay = year_delay.toPandas()\n",
    "\n",
    "pydf = pandas_year_delay.copy()\n",
    "pydf[\"OnTimeCount\"] = pydf[\"TotalCount\"] - pydf[\"DelayCount\"] - pydf[\"EarlyCount\"]\n",
    "year_counts = pydf\n",
    "\n",
    "# Bar positions\n",
    "years = year_counts[\"Year\"]\n",
    "x = np.arange(len(years))\n",
    "\n",
    "# Heights\n",
    "early = year_counts[\"EarlyCount\"]\n",
    "on_time = year_counts[\"OnTimeCount\"]\n",
    "delayed = year_counts[\"DelayCount\"]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x, early, label=\"Early\", color=\"green\")\n",
    "plt.bar(x, on_time, bottom=early, label=\"On Time\", color=\"gray\")\n",
    "plt.bar(x, delayed, bottom=early + on_time, label=\"Delayed\", color=\"red\")\n",
    "\n",
    "# Labels and formatting\n",
    "plt.xticks(x, years)\n",
    "plt.ylabel(\"Number of Flights\")\n",
    "plt.title(\"Flight Status by Year\")\n",
    "plt.legend(title=\"Flight Status\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8a5456-6f33-4b29-bde5-f9a7a859606d",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "Similar to the first bar chart, we also present a stacked bar chart depicting the overall flight count per year between 2018 and 2022. The scale of this plot is in the millions of flights and 2019 appears to have a much higher overall flight count than the other years. Surprisingly, 2020 had the fewest amount of delayed departures by far even though it had a similar amount of overall flights. This could very well have something to do with the emergence of COVID during the beginning of that year, but it would be interesting to look closer at flight trends during that time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789371f0-c9c8-41e5-af2d-81077ffaaa9f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Which months had the most delayed flights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a11f36a-d220-4de2-bffd-3eb1e4d8d252",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "month_delay = filtered_df.select([\"Month\", \"DepDelay\"]).groupBy(\"Month\")\\\n",
    "        .agg(count(F.when(col(\"DepDelay\") > 0, 1)).alias(\"DelayCount\"), \n",
    "             count(F.when(col(\"DepDelay\") < 0, 1)).alias(\"EarlyCount\"),\n",
    "            count(\"*\").alias(\"TotalCount\")).orderBy(col(\"Month\"))\n",
    "pandas_month_delay = month_delay.toPandas()\n",
    "\n",
    "pmdf = pandas_month_delay.copy()\n",
    "pmdf[\"OnTimeCount\"] = pmdf[\"TotalCount\"] - pmdf[\"DelayCount\"] - pmdf[\"EarlyCount\"]\n",
    "month_counts = pmdf\n",
    "\n",
    "# Bar positions\n",
    "months = month_counts[\"Month\"]\n",
    "x = np.arange(len(months))\n",
    "\n",
    "# Heights\n",
    "early = month_counts[\"EarlyCount\"]\n",
    "on_time = month_counts[\"OnTimeCount\"]\n",
    "delayed = month_counts[\"DelayCount\"]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x, early, label=\"Early\", color=\"green\")\n",
    "plt.bar(x, on_time, bottom=early, label=\"On Time\", color=\"gray\")\n",
    "plt.bar(x, delayed, bottom=early + on_time, label=\"Delayed\", color=\"red\")\n",
    "\n",
    "# Labels and formatting\n",
    "plt.xticks(x, months)\n",
    "plt.ylabel(\"Number of Flights\")\n",
    "plt.title(\"Flight Status by Month\")\n",
    "plt.legend(title=\"Flight Status\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6ed815-f3a6-4501-894b-112627c95bc2",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "Continuing with the bar charts, the above chart depicts departure status as proportions of the total number of flights per month starting with January at 1. Intuitively, one might expect there to be more delayed flights during the winter months December-March. However, this graph depicts that there is really no discernible difference in delayed departures during that time, with the largest proportion of delayed flights actually coming in June. Keep in mind this is only looking at departures, so other statuses could have different outcomes, but it is interesting to note."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4ea3b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Which routes had the most delays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4272a44c-1062-4c86-9576-ac7e3551cc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_delay = filtered_df.select([\"Year\", \"DepDelay\"]).groupBy(\"Year\")\\\n",
    "        .agg(count(F.when(col(\"DepDelay\") > 0, 1)).alias(\"DelayCount\"), \n",
    "             count(F.when(col(\"DepDelay\") < 0, 1)).alias(\"EarlyCount\"),\n",
    "            count(\"*\").alias(\"TotalCount\")).orderBy(col(\"TotalCount\").desc())\n",
    "pandas_year_delay = count_delay.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2d5a7e-528f-4f36-8740-1d3c8600f2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep_2 = [\"Operating_Airline\", \"Origin\", \"Dest\", \"ArrDelayMinutes\", \"DepDelayMinutes\", \"Distance\", \"OriginCityName\", \"DestCityName\"]\n",
    "df2 = filtered_df.select(cols_to_keep_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed5ef3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by origin and city, then calculating the total average delay between the cities\n",
    "route_delays = df2.groupBy(\"OriginCityName\", \"DestCityName\") \\\n",
    "    .agg(\n",
    "        (F.avg(\"DepDelayMinutes\") + F.avg(\"ArrDelayMinutes\")).alias(\"AvgTotalDelay\")\n",
    "    ) \\\n",
    "    .orderBy(F.col(\"AvgTotalDelay\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac38afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pandas\n",
    "route_delays_pd = route_delays.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cd5e53-8fdb-48ce-8d14-6406a21ce58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "route_delays_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cd4d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining origin and dest for visual purposes \n",
    "route_delays_pd['Route'] = route_delays_pd['OriginCityName'] + ' to ' + route_delays_pd['DestCityName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bc7146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(x='AvgTotalDelay', y='Route', data=route_delays_pd)\n",
    "plt.title('Top 10 Most Delayed Flight Routes')\n",
    "plt.xlabel('Average Total Delay in Minutes')\n",
    "plt.ylabel('Route (Origin to Destination)')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ab4a04-a76d-4d2f-8f79-c55edc4c17aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Discussion\n",
    "This bar plot shows the top 10 most delayed flight routes ranked by the average total delay, which is the combined sum of both departure and arrival delays. The x-axis represents the total average delay in minutes, while the y-axis displays the origin and destination cities. We note that the route with the most significant delay, Bend/Redmond, OR to Medford, OR, occurs within the same state, with an average total delay of around 2200 minutes or 36 hours which is significantly higher than the other routes. Additionally, most of the other delayed flights seem to occur when the flights are approximately halfway across the country. Further analysis could explore how the amount of delayed flights on each route correlates with the average total delay."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425647c9",
   "metadata": {},
   "source": [
    "### Which airlines experience the most delays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bde894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining departure delay and arrival delay to one column\n",
    "df2 = df2.withColumn('TotalDelay', F.col('DepDelayMinutes') + F.col('ArrDelayMinutes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856c9c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming airline codes to their respective names\n",
    "airline_mapping = {\n",
    "    'AX': 'Trans States Airlines',\n",
    "    'C5': 'Commutair/Champlain Enterprises Inc.',\n",
    "    'G7': 'GoJet Airlines/United Express',\n",
    "    'ZW': 'Air Wisconsin Airlines Corp',\n",
    "    'EV': 'ExpressJet Airlines inc.',\n",
    "    'B6': 'JetBlue Airways',\n",
    "    'YV': 'Mesa Airlines Inc.',\n",
    "    'OO': 'Skywest Airlines Inc',\n",
    "    'F9': 'Frontier Airlines Inc',\n",
    "    'G4': 'Allegiant Air'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e21f13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting only delays that are over 0\n",
    "delayed_flights = df2.filter(df2['TotalDelay'] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0990ab41-5067-4225-975c-c903ddf19505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delayed_flights.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9704ab78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by airline, then calculating the total average delay\n",
    "total_delay = delayed_flights.groupBy(\"Operating_Airline\").agg(\n",
    "    F.avg(\"TotalDelay\").alias(\"TotalDelayMinutes\")\n",
    ").orderBy(F.col(\"TotalDelayMinutes\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b878004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pandas\n",
    "total_delay_pd = total_delay.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a2164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_delay_pd['Operating_Airline_Name'] = total_delay_pd['Operating_Airline'].map(airline_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d634d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_delay_pd.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b00a6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=\"TotalDelayMinutes\", y=\"Operating_Airline\", data=total_delay_pd)\n",
    "plt.title('Top 10 Airlines with the Most Delay in Minutes')\n",
    "plt.xlabel('Average Total Delay in Minutes')\n",
    "plt.ylabel('Airline')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cabfc8-4de4-443e-bac0-8580f478ede2",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "This bar plot illustrates the top 10 airlines with the highest average total delay, measured in minutes. The x-axis represents the total average delay, while the y-axis lists the airlines. It’s clear from the plot that certain airlines, such as Trans States Airlines and Commutair, experience somewhat higher delays than others with their total delays averaging over 100 minutes or a little over 1.5 hours. Further analysis could focus on the specific factors contributing to delays within these airlines, such as location, weather, or operational challenges. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a26667",
   "metadata": {},
   "source": [
    "### Do flights with a longer distance have longer departure delays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf50580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pandas\n",
    "distance_delays_sample_pd = df2.select(\"Distance\", \"DepDelayMinutes\").sample(withReplacement=False, fraction=0.3, seed=42).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8484f124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numeric to prevent error\n",
    "distance_delays_sample_pd['Distance'] = pd.to_numeric(distance_delays_sample_pd['Distance'], errors='coerce')\n",
    "distance_delays_sample_pd['DepDelayMinutes'] = pd.to_numeric(distance_delays_sample_pd['DepDelayMinutes'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e3064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing any NaNs and 0 values\n",
    "distance_delays_sample_pd = distance_delays_sample_pd.dropna(subset=['Distance', 'DepDelayMinutes'])\n",
    "distance_delays_sample_pd = distance_delays_sample_pd[(distance_delays_sample_pd['Distance'] > 0) & (distance_delays_sample_pd['DepDelayMinutes'] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b30dd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x='Distance', y='DepDelayMinutes', data= distance_delays_sample_pd, alpha=0.6)\n",
    "plt.title('Flight Distance vs Departure Delay')\n",
    "plt.xlabel('Flight Distance in Miles')\n",
    "plt.ylabel('Departure Delay in Minutes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96cc4ea-ea94-41c6-954c-03499416a6ac",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "This scatter plot compares flight distance and departure delay, with flight distance being on the x-axis and departure delay on the y-axis. It is only a sample of the whole dataset, and shows that there’s no clear correlation between the distance and delay. However, we can somewhat see that there is a slightly negative correlation where shorter flights have longer delays and longer flights have shorter delays. Further analysis could explore how specific factors contribute to delays, especially for shorter flights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9c5d09-ad6a-468f-851b-b45f143e261a",
   "metadata": {},
   "source": [
    "### Checking delayed and cancelled flights Group By by operating airlines and time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c992ae14-976c-44a0-bed8-1001fa18301c",
   "metadata": {},
   "source": [
    "The following columns were manually chosen after reading column description on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ecfd8d-6ef2-4f25-8d2d-8907059bc2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_check = ['Marketing_Airline_Network', 'Operating_Airline', 'Origin', 'Dest', 'DepDel15', 'DepDelay', 'ArrDel15', 'ArrDelay',\\\n",
    "                 'Cancelled', 'CarrierDelay', 'WeatherDelay', 'NASDelay', 'SecurityDelay', 'LateAircraftDelay',\\\n",
    "                 'Distance', 'Year', 'Month', 'DayofMonth', 'FlightDate', 'Flight_Number_Operating_Airline']\n",
    "print(f'col_to_check = {len(cols_to_check)}')\n",
    "# Check to see if interested columns is in columns_with_few_nulls\n",
    "removed_col = []\n",
    "my_cols = []\n",
    "for c in cols_to_check:\n",
    "    if c in filtered_df.columns:\n",
    "        my_cols.append(c)\n",
    "    else:\n",
    "        removed_col.append(c)\n",
    "\n",
    "print('Chosen columns:')\n",
    "print(my_cols)\n",
    "print()\n",
    "print('Rejected columns:')\n",
    "print(removed_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277c08e3-227b-485a-935f-83b889b504e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out a subset of columns to do data analysis\n",
    "Nam_df = filtered_df.select(my_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf8401c-ac80-43e5-a73a-f4c97b1e3890",
   "metadata": {},
   "source": [
    "Cancelled flights have nulls values in other columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76698145-d6d0-4ac0-b36d-ab7aa82e4752",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_check = ['DepDel15', 'DepDelay', 'ArrDel15', 'ArrDelay']\n",
    "print('There are entries where Cancelled column will have value of 1 (canceled) while other columns might be null')\n",
    "for c in cols_to_check:\n",
    "    num_to_display = Nam_df.where(isnull(col(c)) & (col('Cancelled') == 1)).count()\n",
    "    print(f'Number of entries where {c} column is null but Cancelled is 1: {num_to_display}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3ab495-d2f2-490d-aa05-ef7cf4f2885d",
   "metadata": {},
   "source": [
    "Since I intend to do data exploration with cancelled flights, it is not a good idea for me to do dropna() on the dataset as I will lose data for those flights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f074bb8f-64f3-43fe-9f5a-63e85e6868a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of flights that are delayed by more 15 minutes and cancelled flights\n",
    "num_delayed = Nam_df.where(col('DepDel15') == 1).count()\n",
    "num_cancelled = Nam_df.where(col('Cancelled') == 1).count()\n",
    "print(f'Number of flights that were delayed by more than 15 minutes: {num_delayed}')\n",
    "print(f'Number of flights that were cancelled: {num_cancelled}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bd7e5d-894a-4f18-a440-acefbb3d8785",
   "metadata": {},
   "source": [
    "#### Group by operating airline data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb0bf2-2622-4ed9-a0e3-7b7260c2c614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate by operating airline and turn into Pandas dataframe\n",
    "airline_flights = Nam_df.groupBy(col('Operating_Airline')).agg(F.sum(col('DepDel15')).alias('DelayedFlights'),\n",
    "                                                               F.sum(col('Cancelled')).alias('CancelledFlights'),\n",
    "                                                               F.count('*').alias('TotalFlights'))\n",
    "airline_flights_pd = airline_flights.toPandas()\n",
    "airline_flights_pd['DelayedPercentage'] = airline_flights_pd['DelayedFlights'] / airline_flights_pd['TotalFlights']\n",
    "airline_flights_pd['CancelledPercentage'] = airline_flights_pd['CancelledFlights'] / airline_flights_pd['TotalFlights']\n",
    "airline_flights_pd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d1961e-4871-4b2c-bc6c-45738307cd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph number of delayed flights by airlines\n",
    "airline_flights_pd = airline_flights_pd.sort_values(by = 'DelayedFlights', axis = 0, ascending = True)\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "percentage_barplot = sns.barplot(data = airline_flights_pd, x = 'Operating_Airline', y = 'DelayedFlights')\n",
    "plt.title('Number of delayed flights by Operating Airline')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a017b96b-7e89-4928-bd7d-cb5ab53eadd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph delay flight percentage by airline\n",
    "airline_flights_pd = airline_flights_pd.sort_values(by = 'DelayedPercentage', axis = 0, ascending = True)\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "percentage_barplot = sns.barplot(data = airline_flights_pd, x = 'Operating_Airline', y = 'DelayedPercentage')\n",
    "plt.title('Percentage of delayed flights by Operating Airline')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc87dcf-acea-4248-991f-4ac13f32ff8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph number of cancelled flights by airlines\n",
    "airline_flights_pd = airline_flights_pd.sort_values(by = 'CancelledFlights', axis = 0, ascending = True)\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.barplot(data = airline_flights_pd, x = 'Operating_Airline', y = 'CancelledFlights')\n",
    "plt.title('Number of cancelled flights by Operating Airline')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b9acda-ca0a-4a91-9157-a26481115591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph peracentage of cancelled flights by airlines\n",
    "airline_flights_pd = airline_flights_pd.sort_values(by = 'CancelledPercentage', axis = 0, ascending = True)\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.barplot(data = airline_flights_pd, x = 'Operating_Airline', y = 'CancelledPercentage')\n",
    "plt.title('Percentage of cancelled flights by Operating Airline')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21105ab1-d942-49fb-9a8c-fea16cd91449",
   "metadata": {},
   "source": [
    "#### Group by time data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713a8ee0-a6b3-4b6c-a223-e381b543721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Year and Month and turn into Pandas df\n",
    "time_agg = Nam_df.groupBy('Year', 'Month').agg(F.sum(col('DepDel15')).alias('DelayedFlights'),\n",
    "                                              F.sum(col('Cancelled')).alias('CancelledFlights'),\n",
    "                                              F.count('*').alias('TotalFlights'))\n",
    "time_agg_pd = time_agg.toPandas()\n",
    "time_agg_pd['DelayedPercentage'] = time_agg_pd['DelayedFlights'] / time_agg_pd['TotalFlights']\n",
    "time_agg_pd['CancelledPercentage'] = time_agg_pd['CancelledFlights'] / time_agg_pd['TotalFlights']\n",
    "time_agg_pd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793de5b8-9350-4663-b46e-87cc74471a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph number of flights overtime\n",
    "colors = ['red', 'blue', 'green', 'purple', 'gold']\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "barplot = sns.lineplot(data = time_agg_pd, x = 'Month', y = 'TotalFlights', hue = 'Year', palette = colors)\n",
    "plt.title('Number of monthly flights over the year')\n",
    "plt.ylabel('')\n",
    "plt.legend(loc = 'upper right', bbox_to_anchor = (1.15, 1), title = 'Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e991c337-a0a3-4453-86b9-ff0023d9138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph percentage of delayed flights over time\n",
    "colors = ['red', 'blue', 'green', 'purple', 'gold']\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "barplot = sns.lineplot(data = time_agg_pd, x = 'Month', y = 'DelayedPercentage', hue = 'Year', palette = colors)\n",
    "plt.title('Percentage of delayed monthly flights over the year')\n",
    "plt.ylabel('')\n",
    "plt.legend(loc = 'upper right', bbox_to_anchor = (1.15, 1), title = 'Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563e8562-992e-41eb-ad62-7dcda34cd125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph percentage of cancelled flights over time\n",
    "colors = ['red', 'blue', 'green', 'purple', 'gold']\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "barplot = sns.lineplot(data = time_agg_pd, x = 'Month', y = 'CancelledPercentage', hue = 'Year', palette = colors)\n",
    "plt.title('Percentage of cancelled monthly flights over the year')\n",
    "plt.ylabel('')\n",
    "plt.legend(loc = 'upper right', bbox_to_anchor = (1.15, 1), title = 'Year')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0a6eee-1b8a-4c02-adf2-24f6b931ef7c",
   "metadata": {},
   "source": [
    "#### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e85723b-87bb-4c69-b23b-fe5463d19723",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**<u>Data cleaning</u>**\n",
    "\n",
    "Thanks to the team's work, we can learn that the majority of columns in our dataset contains a large number of NaN. We are able to quickly filter out those columns and focus on the others.\n",
    "My analysis focused on delayed and cancelled flights. I learned that entries of cancelled flights will have nulls in other columns making a simple dropna() not a viable data cleaning method.\n",
    "If we are to work with cancelled flights, we have to find away to fill in the nulls of other columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50186cea-70e9-497f-8c18-2f8f67213c5a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**<u>Aggregated by operating airlines</u>**\n",
    "\n",
    "Despite how miserable air travelling is, flights are seldomly late. About 10 - 25% of flights are at least 15 minutes late to depart. From the graph, I would say that an average of about 15% of flights operated by any airlines are late to depart.\n",
    "\n",
    "Airlines are keen to keep their flights operational cancelling less than 5% of their total scheduled flights. This makes sense as cancellation results in not only loss of revenue but also compensation of damages and potential loss of opportunities.\n",
    "\n",
    "Airline denoted by KS (Peninsula Airways) stood out to me. They do not serve a lot of flights. But their delayed and cancelled metrics are area of improvement to say the least."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0311efc1-0aaf-4e47-a987-127511ebc4dc",
   "metadata": {},
   "source": [
    "**<u>Aggregated by time</u>**\n",
    "\n",
    "Acknowledgement: I understand that the airline industry was heavy affected by COVID-19 and that the industry is recovering to pre-pandemic numbers.\n",
    "\n",
    "Overall, it seems that February tends to be a slow month for the airline industry. The summer months are busy. The holiday months of September - December are only slight less busy than the summer months. People are more likely to travel in the second half of the year.\n",
    "\n",
    "The data for 2018 flights stood out to me. The year started with fewer flights than 2021 and 2022 (post COVID-19 years) but then suddenly gained 300,000 flights for the summer months. This indicates to me that there is a potential socio/economical/geopolitical event happening and/or issue with data collection.\n",
    "\n",
    "Delayed flights are likely to happen during peak of the traveling seasons contributing to the misery of air travelling. February is an interesting month as customers are not travelling but flights pick up an increase in chance of being delayed. My guess is that flight crews and ground crews are burned out from the holiday season and their performance is decreased.\n",
    "\n",
    "As for cancelled flights, there is a gigantic mountain that sits in the middle of the graph. Almost half of scheduled flights for April of 2020 are cancelled. I wonder what happened."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7c98f1-497e-4124-8493-540c33c10671",
   "metadata": {},
   "source": [
    "# Group Project Milestone 3: Data PreProcessing & First Model\n",
    "\n",
    "In this assignment you will need to:\n",
    "\n",
    "1. Finish major preprocessing, this includes scaling and/or transforming your data, imputing your data, encoding your data, feature expansion, Feature expansion (example is taking features and generating new features by transforming via polynomial, log multiplication of features).\n",
    "\n",
    "2. Train your first model\n",
    "\n",
    "3. Evaluate your model compare training vs test error\n",
    "\n",
    "4. Where does your model fit in the fitting graph.\n",
    "\n",
    "5. What are the next models you are thinking of and why?\n",
    "\n",
    "6. Update your README.md to include your new work and updates you have all added. Make sure to upload all code and notebooks. Provide links in your README.md\n",
    "\n",
    "7. Conclusion section: What is the conclusion of your 1st model? What can be done to possibly improve it?\n",
    "\n",
    "Note: For supervised learning, include example ground truth and predictions for train, validation, and test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce7e796-2f89-453c-969e-e445a6853859",
   "metadata": {},
   "source": [
    "## 1. PreProcessing Finalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56784b5a-57a9-4821-846b-820176b005fb",
   "metadata": {},
   "source": [
    "### Handling Missing Data\n",
    "\n",
    "- Columns with >10% of null values are excluded from analysis\n",
    "- Columns that pass this criteria but still have null values range from 1-3% of null values\n",
    "- Most columns that remain in the dataset have no null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a699adc7-c690-486a-8036-2c4e76844386",
   "metadata": {},
   "source": [
    "#### Columns with null values present\n",
    "- Tail_Number: 99.08% non-null\n",
    "- DepTime: 97.39% non-null\n",
    "- DepDelay: 97.39% non-null\n",
    "- DepDelayMinutes: 97.39% non-null\n",
    "- DepDel15: 97.39% non-null\n",
    "- DepartureDelayGroups: 97.39% non-null\n",
    "- WheelsOff: 97.33% non-null\n",
    "- TaxiOut: 97.33% non-null\n",
    "- ArrTime: 97.31% non-null\n",
    "- WheelsOn: 97.28% non-null\n",
    "- TaxiIn: 97.28% non-null\n",
    "- ActualElapsedTime: 97.10% non-null\n",
    "- ArrDelay: 97.10% non-null\n",
    "- ArrDelayMinutes: 97.10% non-null\n",
    "- ArrDel15: 97.10% non-null\n",
    "- ArrivalDelayGroups: 97.10% non-null\n",
    "- AirTime: 97.08% non-null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdd0bd2-5487-43a3-812f-46a9b482647f",
   "metadata": {},
   "source": [
    "#### Missing values are likely due to cancelled flights. Check if this is true. How do we want to handle missing data where the flight was cancelled?\n",
    "\n",
    "- Check if NA values fall under columns where df['Cancelled'] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11adc1a3-01d0-456f-8a4b-06c2eba3e6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if any value in col is Null\n",
    "# use reduce lambda to cobine expressions, if any col in instance is Null -> True\n",
    "na_condition = reduce(lambda a,b: a|b, (col(c).isNull() for c in filtered_df.columns))\n",
    "\n",
    "filtered_df.filter(na_condition).select(\"Cancelled\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222848a4-e652-4851-b57d-321fa622af19",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "Cancelled flights (Cancelled = 1) will be removed from analysis as the predictive model will be used to predict flight delay durations. Cancelled flights are not applicable to this analysis\n",
    "\n",
    "Any remaining instances with NA values will undergo data importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fd0d1f-e92b-47a9-ad9b-ab96d52122db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove cancelled flights\n",
    "filtered_df_not_cancelled = filtered_df.filter(col(\"Cancelled\") != 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ba4447-6780-46b5-8c9d-a1c7ed6ab715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find cols with nulls\n",
    "cols_with_na = [c for c in filtered_df_not_cancelled.columns\n",
    "                if filtered_df_not_cancelled.filter(col(c).isNull()).limit(1).count() >0]\n",
    "\n",
    "dtypes_with_na = {field.name: field.dataType.simpleString()\n",
    "                  for field in filtered_df_not_cancelled.schema.fields\n",
    "                  if field.name in cols_with_na}\n",
    "\n",
    "for col_name, dtype in dtypes_with_na.items():\n",
    "    print(f\"{col_name}: {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c586f4b5-0aa5-47ac-807f-9003478c8480",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "Tail Numbers may occur several times throughout the dataset. Imputation will be done on this categorical variable as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938845ac-7f05-4b16-ba01-56152fd41370",
   "metadata": {},
   "outputs": [],
   "source": [
    "del filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dac1fa4-9ef9-4951-80cb-2342a8f5b5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of expressions counting nulls per column per row\n",
    "null_exprs = [when(col(c).isNull(), 1).otherwise(0) for c in filtered_df_not_cancelled.columns]\n",
    "num_nulls_expr = reduce(lambda a, b: a + b, null_exprs)\n",
    "\n",
    "# select single column in a new df\n",
    "na_counts = filtered_df_not_cancelled.select(num_nulls_expr.alias(\"num_nulls\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1865976-3213-43b1-929e-c9dfa4c13ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count rows by number of nulls\n",
    "na_distribution = na_counts.groupBy(\"num_nulls\").count().orderBy(\"num_nulls\")\n",
    "na_distribution.show()\n",
    "\n",
    "# calculate totals and percentages\n",
    "total_rows = filtered_df_not_cancelled.count()\n",
    "rows_with_na = na_counts.filter(col(\"num_nulls\") > 0).count()\n",
    "percentage_with_na = (rows_with_na / total_rows) * 100\n",
    "\n",
    "print(f\"Rows with at least one NA: {rows_with_na} / {total_rows} ({percentage_with_na:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f268ec-1ce3-4c73-a297-f14c7e8a98b9",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "Only 0.27% of data entries contain NA values in them. The majority of entries with at least one NA have 6 NA values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb02603b-1261-4cca-bb12-173b576efb2f",
   "metadata": {},
   "source": [
    "#### Missing Data Imputation on Numeric Columns\n",
    "\n",
    "This code is inspired by MissForest, which uses Random Forest Classifier to leverage all features in the dataset to predict what the NA value might be. \n",
    "\n",
    "MissForest is not directly usable in pyspark. This code is a workaround to impute missing data in a MissForest - inspired manner\n",
    "\n",
    "**As only 0.27% of entries have missing data, they are simply removed from analysis for now until an appropriate data imputation strategy is successfully implemented for numeric and categorical variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2666b3a6-4410-4b93-b3fc-0d459c671719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import VectorAssembler\n",
    "# from pyspark.ml.regression import RandomForestRegressor\n",
    "# from pyspark.sql import DataFrame\n",
    "\n",
    "# numeric_cols = [f.name for f in filtered_df.schema.fields if f.dataType.simpleString() in ['int', 'double']]\n",
    "# cols_with_nulls = [c for c in numeric_cols if filtered_df.filter(col(c).isNull()).count() > 0]\n",
    "\n",
    "# df_imputed = filtered_df_not_cancelled\n",
    "\n",
    "# for target in cols_with_nulls:\n",
    "#     print(f\"Imputing column: {target}\")\n",
    "\n",
    "#     # Exclude columns with nulls (except the target)\n",
    "#     valid_features = [c for c in numeric_cols if c != target and df_imputed.filter(col(c).isNull()).count() == 0]\n",
    "\n",
    "#     if not valid_features:\n",
    "#         print(f\"Skipping {target} due to lack of valid predictors.\")\n",
    "#         continue\n",
    "\n",
    "#     assembler = VectorAssembler(inputCols=valid_features, outputCol=\"features\")\n",
    "\n",
    "#     df_vector = assembler.transform(df_imputed)\n",
    "\n",
    "#     known = df_vector.filter(col(target).isNotNull())\n",
    "#     unknown = df_vector.filter(col(target).isNull())\n",
    "\n",
    "#     if known.count() == 0 or unknown.count() == 0:\n",
    "#         continue\n",
    "\n",
    "#     rf = RandomForestRegressor(featuresCol=\"features\", labelCol=target)\n",
    "#     model = rf.fit(known)\n",
    "\n",
    "#     predicted = model.transform(unknown).select(\"id\", col(\"prediction\").alias(f\"{target}\"))\n",
    "\n",
    "#     # Join predictions and fill in missing values\n",
    "#     df_imputed = df_imputed.join(predicted, on=\"id\", how=\"left\") \\\n",
    "#         .withColumn(target, \n",
    "#                     col(target).when(col(target).isNotNull(), col(target))\n",
    "#                                .otherwise(col(f\"{target}_imputed\"))) \\\n",
    "#         .drop(f\"{target}_imputed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238cbd6f-93b0-486d-a32a-31ca6a4e10c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df_not_cancelled.na.drop()\n",
    "filtered_df = filtered_df.drop(\"Cancelled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2342dd52-4c26-485c-a262-61735f730cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "del filtered_df_not_cancelled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2e9440-bd90-4797-b2bb-ddc82381ce04",
   "metadata": {},
   "source": [
    "### Removal of Redundant Features\n",
    "\n",
    "- FlightDate is already parsed out in Year, Quarter, Month, DayofMonth, DayofWeek columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75181bb5-cce0-44aa-805e-f14a121f2410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04774059-95a0-40ba-b363-1ad3957bc4a3",
   "metadata": {},
   "source": [
    "### Indexing Categorical Variables - StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b34aa57-8b4f-44e8-a7c0-b345a4a95ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3772bc56-8455-49ab-8459-05a6474cbf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_route(origin, destination): # Concat origin and dest alphabetically\n",
    "    if origin.upper() < destination.upper():\n",
    "        return (origin.upper() + '-' + destination.upper())\n",
    "    else:\n",
    "        return (destination.upper() + '-' + origin.upper())\n",
    "\n",
    "concat_route_udf = udf(concat_route, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eadb36b-58e3-4ef5-894d-20d02dbc998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chosen_cols = [] # List of final columns\n",
    "# final_df = filtered_df.select(chosen_cols)\n",
    "two_way_df = filtered_df.withColumn('TwoWayRoute', concat_route_udf('Origin', 'Dest')) # Establish Route column, ignore 2-way\n",
    "str_cols = [column[0] for column in two_way_df.dtypes if column[1] == 'string'] # Get columns with String datatype\n",
    "str_idx_cols = [(column + 'Index') for column in str_cols]\n",
    "\n",
    "indexer = StringIndexer(inputCols = str_cols, outputCols = str_idx_cols).fit(two_way_df)\n",
    "filtered_df_indexed = indexer.transform(two_way_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffc6d82-f367-4074-bc81-f8797c6d832c",
   "metadata": {},
   "source": [
    "### Feature Expansion Ideas\n",
    "\n",
    "- Combine Origin & Dest into one feature (Route)\n",
    "- Ratio features AirTime/Distance\n",
    "- Log transformation of skewed numeric features - recommended 'Distance' had a >3 fold increase in skew magnitude out of top 20 skewed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e836670b-3985-42a8-82ee-cf694cf6ccc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eed1055-97e9-467b-977a-6761f7065450",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filtered_df = filtered_df.drop(\"is_popular_route\")\n",
    "filtered_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35168191-9127-4a9f-b7a7-8410a3c7b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# ─── Add \"route\" Column ─────────────────────────────────────────────\n",
    "# Format: \"Origin - Dest\", replacing nulls with \"Unknown\"\n",
    "filtered_df = filtered_df.withColumn(\n",
    "    \"route\",\n",
    "    F.concat_ws(\n",
    "        \" - \",\n",
    "        F.coalesce(F.col(\"Origin\"), F.lit(\"Unknown\")),\n",
    "        F.coalesce(F.col(\"Dest\"), F.lit(\"Unknown\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "# ─── Add Average Speed (mph) ─────────────────────────────────────────\n",
    "# Formula: (Distance / AirTime) * 60, rounded to nearest integer\n",
    "filtered_df = filtered_df.withColumn(\n",
    "    \"avg_speed_mph\",\n",
    "    F.when(\n",
    "        F.col(\"AirTime\").isNotNull() & (F.col(\"AirTime\") != 0),\n",
    "        F.round((F.col(\"Distance\") / F.col(\"AirTime\")) * 60)\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "# ─── Add num_flights Using Window Function ───────────────────────────\n",
    "# Counts how many times each route appears, without using join\n",
    "route_window = Window.partitionBy(\"route\")\n",
    "filtered_df = filtered_df.withColumn(\n",
    "    \"num_flights\",\n",
    "    F.count(\"*\").over(route_window)\n",
    ")\n",
    "\n",
    "# ─── Monthly Route Count (for EDA only, optional) ────────────────────\n",
    "monthly_counts = filtered_df.groupBy(\"route\", \"Month\").count()\n",
    "\n",
    "# ─── Add route_popularity Bucket ─────────────────────────────────────\n",
    "# 0 = low (<5000), 1 = med (5K–15K), 2 = high (>15K)\n",
    "filtered_df = filtered_df.withColumn(\n",
    "    \"route_popularity\",\n",
    "    F.when(F.col(\"num_flights\") < 5000, 0)\n",
    "     .when(F.col(\"num_flights\") < 15000, 1)\n",
    "     .otherwise(2)\n",
    ")\n",
    "\n",
    "# ─── Preview Random Sample of Final Feature Columns ──────────────────\n",
    "sample_cols = [\"route\", \"avg_speed_mph\", \"num_flights\", \"route_popularity\"]\n",
    "filtered_df.select(sample_cols) \\\n",
    "    .orderBy(F.rand(seed=42)) \\\n",
    "    .limit(20) \\\n",
    "    .show(truncate=False)\n",
    "\n",
    "# ─── DataFrame Shape Summary ─────────────────────────────────────────\n",
    "num_rows = filtered_df.count()\n",
    "num_cols = len(filtered_df.columns)\n",
    "print(f\"Shape of the filtered DataFrame: ({num_rows}, {num_cols})\")\n",
    "\n",
    "# ─── Optional: Unpersist if it was cached earlier ────────────────────\n",
    "filtered_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a231de2a-e83c-40f1-98ec-f7f8c9275062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://plotly.com/python/lines-on-maps/\n",
    "airport_cols = ['AirportID', 'Name', 'City', 'Country', 'IATA', 'ICAO',\n",
    "                'Latitude', 'Longitude', 'Altitude', 'Timezone', 'DST',\n",
    "                'Tz database time zone', 'Type', 'Source']\n",
    "\n",
    "airport_df = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat\",\n",
    "    header=None, names=airport_cols\n",
    ")\n",
    "\n",
    "airport_coords = airport_df[['IATA', 'Latitude', 'Longitude']].dropna()\n",
    "airport_coords = airport_coords[airport_coords['IATA'].str.len() == 3]  # Keep valid codes\n",
    "\n",
    "top_routes_pd = (\n",
    "    filtered_df.groupBy(\"route\", \"Origin\", \"Dest\")\n",
    "    .agg(F.count(\"*\").alias(\"num_flights\"))\n",
    "    .orderBy(F.desc(\"num_flights\"))\n",
    "    .limit(1000)\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "# Join lat/lon for Origin and Dest\n",
    "top_routes_pd = top_routes_pd.merge(airport_coords, left_on=\"Origin\", right_on=\"IATA\") \\\n",
    "                             .rename(columns={\"Latitude\": \"Origin_Lat\", \"Longitude\": \"Origin_Lon\"}) \\\n",
    "                             .drop(\"IATA\", axis=1)\n",
    "\n",
    "top_routes_pd = top_routes_pd.merge(airport_coords, left_on=\"Dest\", right_on=\"IATA\") \\\n",
    "                             .rename(columns={\"Latitude\": \"Dest_Lat\", \"Longitude\": \"Dest_Lon\"}) \\\n",
    "                             .drop(\"IATA\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2f801c-2374-4553-befd-fe254b02ad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "# ─── Add Airport Markers ─────────────────────────────────────────────\n",
    "# Collect unique airport locations from both Origin and Dest\n",
    "airport_df = pd.concat([\n",
    "    top_routes_pd[['Origin', 'Origin_Lat', 'Origin_Lon']].rename(\n",
    "        columns={\"Origin\": \"IATA\", \"Origin_Lat\": \"lat\", \"Origin_Lon\": \"lon\"}),\n",
    "    top_routes_pd[['Dest', 'Dest_Lat', 'Dest_Lon']].rename(\n",
    "        columns={\"Dest\": \"IATA\", \"Dest_Lat\": \"lat\", \"Dest_Lon\": \"lon\"})\n",
    "]).drop_duplicates(subset=[\"IATA\"])\n",
    "\n",
    "fig.add_trace(go.Scattergeo(\n",
    "    locationmode='USA-states',\n",
    "    lon=airport_df[\"lon\"],\n",
    "    lat=airport_df[\"lat\"],\n",
    "    hoverinfo='text',\n",
    "    text=airport_df[\"IATA\"],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=4,\n",
    "        color='blue',\n",
    "        line=dict(width=1, color='rgba(68, 68, 68, 0)')\n",
    "    ),\n",
    "    name=\"Airports\"\n",
    "))\n",
    "\n",
    "# ─── Add Flight Routes ───────────────────────────────────────────────\n",
    "for _, row in top_routes_pd.iterrows():\n",
    "    fig.add_trace(go.Scattergeo(\n",
    "        locationmode='USA-states',\n",
    "        lon=[row[\"Origin_Lon\"], row[\"Dest_Lon\"]],\n",
    "        lat=[row[\"Origin_Lat\"], row[\"Dest_Lat\"]],\n",
    "        mode='lines',\n",
    "        line=dict(width=1, color='red'),\n",
    "        opacity=row[\"num_flights\"] / top_routes_pd[\"num_flights\"].max(),\n",
    "        name=f\"{row['Origin']} → {row['Dest']}\"\n",
    "    ))\n",
    "\n",
    "# ─── Layout & Display ────────────────────────────────────────────────\n",
    "fig.update_layout(\n",
    "    title_text='Top 1000 Flight Routes (Interactive Map)',\n",
    "    showlegend=True,\n",
    "    geo=dict(\n",
    "        scope='north america',\n",
    "        projection_type='azimuthal equal area',\n",
    "        showland=True,\n",
    "        landcolor='rgb(243, 243, 243)',\n",
    "        countrycolor='rgb(204, 204, 204)',\n",
    "    ),\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d175e6-30f4-476c-90c7-92c94afca2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go\n",
    "# import plotly.io as pio\n",
    "\n",
    "# pio.renderers.default = 'notebook'\n",
    "\n",
    "# fig = go.Figure()\n",
    "\n",
    "# for _, row in top_routes_pd.iterrows():\n",
    "#     fig.add_trace(go.Scattergeo(\n",
    "#         locationmode='ISO-3',\n",
    "#         lon=[row[\"Origin_Lon\"], row[\"Dest_Lon\"]],\n",
    "#         lat=[row[\"Origin_Lat\"], row[\"Dest_Lat\"]],\n",
    "#         mode='lines',\n",
    "#         line=dict(width=min(0.01 + row['num_flights'] / top_routes_pd['num_flights'].max(), 1), color='blue'),\n",
    "#         opacity=min(0.01 + row['num_flights'] / top_routes_pd['num_flights'].max(), 1),\n",
    "#         hoverinfo='text',\n",
    "#         text=f\"{row['Origin']} → {row['Dest']} ({row['num_flights']} flights)\",\n",
    "#         name=f\"{row['Origin']} → {row['Dest']}\"  # ✅ this makes it show up in legend\n",
    "#     ))\n",
    "\n",
    "# fig.update_geos(\n",
    "#     scope=\"north america\",  # ✅ focus only on the USA\n",
    "#     showland=True,\n",
    "#     landcolor=\"rgb(243, 243, 243)\",\n",
    "#     countrycolor=\"rgb(204, 204, 204)\"\n",
    "# )\n",
    "\n",
    "# fig.update_layout(\n",
    "#     title_text=\"Top 1000 Most Frequent Flight Routes\",\n",
    "#     showlegend=True,  # ✅ Ensure legend is visible\n",
    "#     height=500,\n",
    "#     margin={\"r\":0,\"t\":30,\"l\":0,\"b\":0}\n",
    "# )\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a9b1a7-04a2-4203-b9d0-3083f61cf9d7",
   "metadata": {},
   "source": [
    "## 2. Train First Model: Random Forest Classifier + Feature Selection\n",
    "\n",
    "- RF cannot handle missing values\n",
    "- Values must be numeric (categorical features must be indexed)\n",
    "- Define Label column (Classification df['ArrDelayGroups'] or Regression df['ArrDelay'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369517c4-5406-4df2-898b-dc1d08b4d456",
   "metadata": {},
   "source": [
    "### Split dataset into train, test, validation set (x & y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa28b92b-f190-4d27-a3fd-438b5783abc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c279693-1d3f-4ea8-9eda-46ab5cc6909b",
   "metadata": {},
   "source": [
    "### Optimize Random Forest Classifier Using Validation Dataset\n",
    "\n",
    "- numTrees\n",
    "- maxDepth\n",
    "- minInstancesPerNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac0b92f-913e-4f00-ba2d-64964bb7fda2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cde5a81-40a3-4589-a547-b2f5ae049f2d",
   "metadata": {},
   "source": [
    "### Train RF Classifier & Apply to Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8fe409-cdb2-4d0a-905f-88eb10d4466a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
